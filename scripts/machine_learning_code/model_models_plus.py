import os
from pathlib import Path
from typing import Tuple, List

import torch
import torch.nn as nn
import torch.nn.functional as F
import yaml
import json5


# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
current_dir = Path(__file__).resolve().parent
# æ‹¼æ¥ label_mapping.json çš„å®Œæ•´è·¯å¾„
json_path = current_dir / "ion_attributes_dict.jsonc"
# è¯»å– JSON æ–‡ä»¶
with open(json_path, "r", encoding="utf-8") as f:
    ion_attr_dict = json5.load(f)
# ä¾‹å¦‚æŒ‰ä»¥ä¸‹é¡ºåºå›ºå®šæ’åˆ—
ion_order_plus = ["Ca2+", "Na+",  "Ni2+", "Cr3+", "Cu2+","Fe3+","no_ion"]
ion_attr_list_plus = [ion_attr_dict[ion] for ion in ion_order_plus]



json_path_1117 = current_dir / "ion_attributes_dict_1117.jsonc"
# è¯»å– JSON æ–‡ä»¶
with open(json_path_1117, "r", encoding="utf-8") as f:
    ion_attr_dict_1117 = json5.load(f)
ion_attr_list_plus_1117 = [ion_attr_dict_1117[ion] for ion in ion_order_plus]




# ä¾‹å¦‚æŒ‰ä»¥ä¸‹é¡ºåºå›ºå®šæ’åˆ—
ion_order = ["Ca2+", "Na+",  "Ni2+", "Cr3+", "Cu2+","Fe3+"]
ion_attr_list = [ion_attr_dict[ion] for ion in ion_order]
ion_attr_list_1117 = [ion_attr_dict_1117[ion] for ion in ion_order]


def load_ion_rule_prototypes(cfg_path: str):
    """
    ä» ion_rule_prototypes.yaml è¯»å–å„ç¦»å­çš„ rule prototype å‘é‡ r_kã€‚
    è¿”å›:
        rule_proto: (6, D_r) çš„ tensor
        feat_mean:  (1, D_r) å‡å€¼ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        feat_std:   (1, D_r) æ ‡å‡†å·®ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        ion_order:  list[str]ï¼Œç¦»å­é¡ºåº
    """
    cfg_path = str(cfg_path)
    if not os.path.exists(cfg_path):
        print(f"[WARN] ion_rule_prototypes.yaml not found at {cfg_path}, "
              f"rule-based prior will be disabled.")
        # å ä½ï¼š6 ä¸ªç±»ï¼Œæ¯ç±» 1 ç»´ 0
        rule_proto = torch.zeros(6, 1, dtype=torch.float32)
        feat_mean = rule_proto.mean(dim=0, keepdim=True)
        feat_std = rule_proto.std(dim=0, keepdim=True) + 1e-6
        ion_order = ["Ca2+", "Na+", "Ni2+", "Cr3+", "Cu2+", "Fe3+"]
        return rule_proto, feat_mean, feat_std, ion_order

    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    ion_order = cfg.get("ion_order", ["Ca2+", "Na+", "Ni2+", "Cr3+", "Cu2+", "Fe3+"])
    features_dict = cfg["features"]

    rule_mat = []
    for name in ion_order:
        if name not in features_dict:
            raise ValueError(f"ion_rule_prototypes: ion '{name}' not found in features section")
        vec = features_dict[name]
        rule_mat.append(vec)

    rule_proto = torch.tensor(rule_mat, dtype=torch.float32)  # (6, D_r)
    feat_mean = rule_proto.mean(dim=0, keepdim=True)
    feat_std = rule_proto.std(dim=0, keepdim=True) + 1e-6

    return rule_proto, feat_mean, feat_std, ion_order

class TransformerEncoderLayerWithReturnAttn(nn.Module):
    def __init__(self, d_model, nhead, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)

    def forward(self, src):
        attn_output, attn_weights = self.self_attn(src, src, src, need_weights=True, average_attn_weights=False)
        src = self.norm1(src + self.dropout1(attn_output))
        return src, attn_weights

class MyTransformerWithAttn(nn.Module):
    def __init__(self, d_model, nhead, num_layers, dropout=0.1):
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerEncoderLayerWithReturnAttn(d_model, nhead, dropout)
            for _ in range(num_layers)
        ])

    def forward(self, src):
        attn_all_layers = []
        for layer in self.layers:
            src, attn_weights = layer(src)
            attn_all_layers.append(attn_weights)
        return src, attn_all_layers
class TransformerCrossAttnLayerWithReturnAttn(nn.Module):
    def __init__(self, d_model, nhead, dropout=0.1):
        super().__init__()
        self.cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key_value):  # query: (B, T1, D), key_value: (B, T2, D)
        attn_output, attn_weights = self.cross_attn(
            query=query,
            key=key_value,
            value=key_value,
            need_weights=True,
            average_attn_weights=False
        )
        query = self.norm(query + self.dropout(attn_output))  # residual + norm
        return query, attn_weights
class MyCrossAttnTransformer(nn.Module):
    def __init__(self, d_model, nhead, num_layers, dropout=0.1):
        super().__init__()
        self.layers = nn.ModuleList([
            TransformerCrossAttnLayerWithReturnAttn(d_model, nhead, dropout)
            for _ in range(num_layers)
        ])

    def forward(self, query, key_value):
        attn_all_layers = []
        for layer in self.layers:
            query, attn_weights = layer(query, key_value)
            attn_all_layers.append(attn_weights)
        return query, attn_all_layers
    


class AdjustableMLP(nn.Module):
    def __init__(self,
                 input_dim,
                 hidden_dims,
                 output_dim,
                 num_layers=None,
                 dropout=None,
                 use_bn=False):
        """
        å¯è°ƒèŠ‚çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å—ï¼ˆå‘åå…¼å®¹ç‰ˆï¼‰

        å…¼å®¹ä¸¤ç§å†™æ³•ï¼š
        1) æ–°ï¼šAdjustableMLP(input_dim, hidden_dims, output_dim, num_layers, dropout=0.1)
        2) æ—§ï¼šAdjustableMLP(input_dim, hidden_dims, output_dim, use_bn=True, dropout=0.1)

        å‚æ•°:
            input_dim (int): è¾“å…¥ç»´åº¦
            hidden_dims (list[int]): éšè—å±‚ç»´åº¦åˆ—è¡¨ï¼Œä¾‹å¦‚ [1024, 512, 256]
            output_dim (int): è¾“å‡ºç»´åº¦
            num_layers (int|None): ä½¿ç”¨çš„éšè—å±‚å±‚æ•°ï¼›None æ—¶é»˜è®¤ä½¿ç”¨ len(hidden_dims)
            dropout (float|None): æ¯å±‚åçš„ dropout æ¦‚ç‡
            use_bn (bool): æ˜¯å¦åœ¨æ¯ä¸ªéšè—å±‚ååŠ  BatchNorm1dï¼ˆé»˜è®¤ Falseï¼‰
        """
        super().__init__()

        # ---- å…¼å®¹æ—§ç­¾åï¼šå¦‚æœç¬¬4ä¸ªä½ç½®ä¼ çš„æ˜¯ boolï¼ˆä»¥å‰å½“ use_bn ç”¨ï¼‰----
        if num_layers is not None and not isinstance(num_layers, int):
            # æ—§ç‰ˆæŠŠç¬¬4ä¸ªå‚æ•°å½“ä½œ use_bn ä¼ è¿›æ¥äº†
            use_bn = bool(num_layers)
            num_layers = None

        hidden_dims = list(hidden_dims) if hidden_dims is not None else []
        # æ¨æ–­å±‚æ•°
        if num_layers is None:
            num_layers = len(hidden_dims)

        # å®é™…ä½¿ç”¨çš„å±‚æ•°ï¼šå– num_layers ä¸ len(hidden_dims) çš„æœ€å°å€¼ï¼Œé¿å…è¶Šç•Œ
        use_layers = max(0, min(num_layers, len(hidden_dims)))

        layers = []
        in_dim = input_dim

        for li in range(use_layers):
            out_dim = hidden_dims[li]
            layers.append(nn.Linear(in_dim, out_dim))
            if use_bn:
                layers.append(nn.BatchNorm1d(out_dim))
            layers.append(nn.ReLU(inplace=True))
            if dropout is not None and dropout > 0:
                layers.append(nn.Dropout(dropout))
            in_dim = out_dim

        # æœ€åä¸€å±‚åˆ°è¾“å‡º
        layers.append(nn.Linear(in_dim, output_dim))
        self.mlp = nn.Sequential(*layers)

    def forward(self, x):
        return self.mlp(x)


# å®šä¹‰è®¡ç®—é¢„æµ‹ç”µå‹çš„å‡½æ•°
def calculate_predicted_voltage(theta_ca, phi_ca, theta_an, phi_an, psi, temperature, flow, current,
                                sigma_mem, alpha_ca, alpha_an, i_0ca, i_0an,
                                
                                
                                ):

    R = 8.314
    F = 96485
    T = temperature + 273.15
    i = current * 4 # 4æ˜¯ç”µè§£æ§½è†œé¢ç§¯

    #------------------ä¸‹é¢è®¡ç®—V_ocv-----------------
    E_rev = 1.229 - 0.9 * 10 ** -3 * (T - 298)
    P_H2 = torch.tensor(1.0, dtype=torch.float32)
    P_O2 = torch.tensor(1.0, dtype=torch.float32)
    P_H2O = torch.tensor(1.0, dtype=torch.float32)
    # V_ocv=1.229-0.9*10**-3*(T-298)+R*T/2/F*torch.log(alpha_H2*sqrt(alpha_O2)/alpha_H2O)
    V_ocv = E_rev + R * T / 2 / F * torch.log(P_H2*torch.sqrt(P_O2)/P_H2O)  # ä¿æŒ tensor è¿ç®—

    epsilon = 1e-9  # è®¾ç½®æœ€å°å€¼ï¼Œå¯è°ƒå°å€¼,

    # ä¿®æ”¹ safe_arcsinhï¼š
    def safe_arcsinh(x):
        return torch.log(x + torch.sqrt(x ** 2 + 1) + epsilon)
    #--------------------- ä¸‹é¢è®¡ç®—V_act
    V_act = R * T / (theta_ca * alpha_ca * F) * safe_arcsinh(current / (2 * i_0ca * (1 - phi_ca) + epsilon)) + \
            R * T / (theta_an * alpha_an * F) * safe_arcsinh(current / (2 * i_0an * (1 - phi_an)+ epsilon))

    # ---------------------ä¸‹é¢è®¡ç®—V_diff
    V_diff = 0
    # ---------------------ä¸‹é¢è®¡ç®—V_ohm
    t_an = 0.012 # é˜³æé’›æ¿åšåº¦m
    t_ca1 = 0.012 # é˜´æé’›æ¿åšåº¦m
    t_ca2 = 0.002 # é˜´æé“œæ¿åšåº¦m
    t_mem = 135 * 10 ** -6 # è´¨å­äº¤æ¢è†œåšåº¦m
    
    sigma_an = 2.38 * 10 ** 6
    sigma_ca1 = 2.38 * 10 ** 6
    sigma_ca2 = 5.96 * 10 ** 7



    V_ohm = i  * (t_an / sigma_an + t_ca1 / sigma_ca1 + t_ca2 / sigma_ca2 + t_mem / sigma_mem*100 / (1 - psi+epsilon))


    predicted_voltage = (V_ocv + V_act + V_diff + V_ohm).squeeze()

    return predicted_voltage,V_ocv,V_act,V_diff,V_ohm  
# å®šä¹‰è®¡ç®—é¢„æµ‹ç”µå‹çš„å‡½æ•°
def calculate_predicted_voltage_plus(theta_ca, phi_ca, theta_an, phi_an, psi, temperature, flow, current,
                                sigma_mem, alpha_ca, alpha_an, i_0ca, i_0an,
                                electrolyzer_parameters                                 
                                
                                ):

    R = 8.314
    F = 96485
    T = temperature + 273.15

    #------------------ä¸‹é¢è®¡ç®—V_ocv-----------------
    E_rev = 1.229 - 0.9 * 10 ** -3 * (T - 298)
    P_H2 = torch.tensor(1.0, dtype=torch.float32)
    P_O2 = torch.tensor(1.0, dtype=torch.float32)
    P_H2O = torch.tensor(1.0, dtype=torch.float32)
    # V_ocv=1.229-0.9*10**-3*(T-298)+R*T/2/F*torch.log(alpha_H2*sqrt(alpha_O2)/alpha_H2O)
    V_ocv = E_rev + R * T / 2 / F * torch.log(P_H2*torch.sqrt(P_O2)/P_H2O)  # ä¿æŒ tensor è¿ç®—

    epsilon = 1e-9  # è®¾ç½®æœ€å°å€¼ï¼Œå¯è°ƒå°å€¼,

    # ä¿®æ”¹ safe_arcsinhï¼š
    def safe_arcsinh(x):
        return torch.log(x + torch.sqrt(x ** 2 + 1) + epsilon)
    #--------------------- ä¸‹é¢è®¡ç®—V_act
    V_act = R * T / (theta_ca * alpha_ca * F) * safe_arcsinh(current / (2 * i_0ca * (1 - phi_ca) + epsilon)) + \
            R * T / (theta_an * alpha_an * F) * safe_arcsinh(current / (2 * i_0an * (1 - phi_an)+ epsilon))#æ³¨æ„ï¼šè¯¥å…¬å¼é‡Œçš„currentæ˜¯ç”µæµå¯†åº¦A/cm^2

    # ---------------------ä¸‹é¢è®¡ç®—V_diff
    V_diff = 0
    # ---------------------ä¸‹é¢è®¡ç®—V_ohm
    # t_an = 0.012 # é˜³æé’›æ¿åšåº¦m
    # t_ca1 = 0.012 # é˜´æé’›æ¿åšåº¦m
    # t_ca2 = 0.002 # é˜´æé“œæ¿åšåº¦m
    # t_mem = 135 * 10 ** -6 # è´¨å­äº¤æ¢è†œåšåº¦m
    
    # sigma_an = 2.38 * 10 ** 6#é˜³æé’›æ¿å¯¼ç”µç‡S/m
    # sigma_ca1 = 2.38 * 10 ** 6#é˜´æé’›æ¿å¯¼ç”µç‡S/m
    # sigma_ca2 = 5.96 * 10 ** 7#é˜´æé“œæ¿å¯¼ç”µç‡S/m
    # electrolyzer_parameters[7]æ˜¯ç”µè§£æ§½è†œæœ‰æ•ˆé¢ç§¯ cm^2

    # åˆ†åˆ«èµ‹å€¼
    t_an       = electrolyzer_parameters[0].item()
    t_ca1      = electrolyzer_parameters[1].item()
    t_ca2      = electrolyzer_parameters[2].item()
    t_mem      = electrolyzer_parameters[3].item()
    sigma_an   = electrolyzer_parameters[4].item()
    sigma_ca1  = electrolyzer_parameters[5].item()
    sigma_ca2  = electrolyzer_parameters[6].item()
    i = current * electrolyzer_parameters[7].item() # 4æ˜¯ç”µè§£æ§½è†œé¢ç§¯

    V_ohm = i  * (t_an / sigma_an + t_ca1 / sigma_ca1 + t_ca2 / sigma_ca2 + t_mem / sigma_mem*100 / (1 - psi+epsilon))


    predicted_voltage = (V_ocv + V_act + V_diff + V_ohm).squeeze()

    return predicted_voltage,V_ocv,V_act,V_diff,V_ohm           




# å®šä¹‰å®Œæ•´æ¨¡å‹
class Model_three_system(nn.Module):#ä¸‰ç³»ç»Ÿæ¶æ„
    def __init__(self, volt_input_dim, volt_mlp_hidden_dims,mlp_output_dims, volt_mlp_num_layers,
                 impe_input_dim,impe_mlp_hidden_dims,  impe_mlp_num_layers,
                 transformer_d_model, nhead, transformer_num_layers,param_transformer_num_layers,
                 physic_mlp_hidden_dims, physic_mlp_num_layers,
                 ion_attr_embed_hidden_dims,ion_attr_embed_num_layers,
                 ion_encoder_num_layers,
                 ion_post_hidden_dims,ion_post_num_layers,
                 probMLP_input_dims,probMLP_hidden_dims,probMLP_num_layers,
                 param_mlp_hidden_dims,param_mlp_num_layers,
                 freq_encoder_hidden_dims,freq_encoder_num_layers,
                 time_encoder_hidden_dims,time_encoder_num_layers,
                 cross_transformer_num_layers,
                 param_embed_hidden_dims,param_embed_num_layers,
                 physic_embed_hidden_dims,physic_embed_num_layers,
                 cls_token_mlp_hidden_dims, cls_token_mlp_num_layers,
                 num_freq_points,num_time_points):
        
        
        super(Model_three_system, self).__init__()
        self.voltMLP = AdjustableMLP(volt_input_dim, volt_mlp_hidden_dims, mlp_output_dims, volt_mlp_num_layers)
        self.impeMLP = AdjustableMLP(impe_input_dim, impe_mlp_hidden_dims, mlp_output_dims, impe_mlp_num_layers)
        self.transformer = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=transformer_num_layers)
        self.physicMLP = AdjustableMLP(transformer_d_model, physic_mlp_hidden_dims, 5*transformer_d_model, physic_mlp_num_layers)
        


        self.probMLP = AdjustableMLP(input_dim=probMLP_input_dims, hidden_dims=probMLP_hidden_dims, output_dim=mlp_output_dims, num_layers=probMLP_num_layers)  # å°† physic_output æŠ•å½±åˆ°åµŒå…¥ç©ºé—´

        
        # å®šä¹‰ç‰©ç†æ¨¡å‹éƒ¨åˆ†
        self.param_transformer =  MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=param_transformer_num_layers )
        self.paramMLP = AdjustableMLP(transformer_d_model,param_mlp_hidden_dims, 5*transformer_d_model, param_mlp_num_layers)
        self.norm = nn.LayerNorm(transformer_d_model)
        self.ion_attr_embed = AdjustableMLP(input_dim=len(ion_attr_list[0]), hidden_dims=ion_attr_embed_hidden_dims, output_dim=transformer_d_model, num_layers=ion_attr_embed_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨
        self.ion_encoder = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=ion_encoder_num_layers)
        self.ion_postMLP = AdjustableMLP(input_dim=transformer_d_model, hidden_dims=ion_post_hidden_dims, output_dim=transformer_d_model, num_layers=ion_post_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨

        self.mlp_output_dims = mlp_output_dims
        self.my_cross_attn_transformer = MyCrossAttnTransformer(d_model=transformer_d_model, nhead=nhead, num_layers=cross_transformer_num_layers)


        self.param_compress = AdjustableMLP(transformer_d_model, param_embed_hidden_dims, 1, param_embed_num_layers)
        self.influence_compress = AdjustableMLP(transformer_d_model, physic_embed_hidden_dims, 1, physic_embed_num_layers)

        self.transformer_d_model = transformer_d_model
        # å¯¹ physic_embed æ·»åŠ æ®‹å·®å’Œ norm
        self.prob_norm = nn.LayerNorm(transformer_d_model)

        # å¯¹ ion_embeddings æ·»åŠ æ®‹å·®å’Œ norm
        self.ion_norm = nn.LayerNorm(transformer_d_model)

        # çœŸå®é¢‘ç‡åˆ—è¡¨ï¼ˆ64ä¸ªç‚¹ï¼‰
        if num_freq_points==64:
            freq_values_hz = torch.tensor([
                19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
                794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
                79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
                7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
                0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
                0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259, 0.01
            ], dtype=torch.float32)
        elif num_freq_points==63:
            freq_values_hz = torch.tensor([
            19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
            794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
            79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
            7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
            0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
            0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259
            ], dtype=torch.float32)
        else:
            print("num_freq_points:",num_freq_points)
        # å½’ä¸€åŒ–åˆ° å¯¹æ•°åæ ‡ç³»[0, 1]
        freq_values_log = torch.log10(freq_values_hz)  # log10å˜æ¢
        freq_values_norm = (freq_values_log - freq_values_log.min()) / (freq_values_log.max() - freq_values_log.min())
        self.register_buffer("freq_values_tensor", freq_values_norm)     

        # å®šä¹‰ frequency encoder MLP,å°†é¢‘ç‡æ•°æ®å˜ä¸ºå’ŒMLPåŒæ ·çš„å¤§å°ï¼Œè¿™æ ·å¯ä»¥å¯¹é˜»æŠ—æ•°æ®è¿›è¡Œé¢‘ç‡ç¼–ç 
        self.freq_encoder = AdjustableMLP(1, freq_encoder_hidden_dims, mlp_output_dims, freq_encoder_num_layers)#(64ï¼Œ1)->(64,32)
        # æ—¶é—´ç¼–ç å™¨
        self.time_embedding = AdjustableMLP(1, time_encoder_hidden_dims, mlp_output_dims, time_encoder_num_layers)  # nn.Embedding é»˜è®¤åªèƒ½å¤„ç†ç¦»æ•£ç´¢å¼•ï¼ˆå¦‚ 0ã€1ã€2ã€3ï¼‰ï¼Œå®ƒå¹¶ä¸ä¼šæ˜¾å¼æ•æ‰åˆ°è¿™äº›æ—¶é—´ç‚¹åœ¨ç‰©ç†ä¸Šæ˜¯â€œè¿ç»­ä¸”é—´éš”ä¸º2å°æ—¶â€çš„ã€‚é‚£ä¸ºä»€ä¹ˆè¿˜ä¼šç”¨ nn.Embedding è¡¨ç¤ºæ—¶é—´ï¼Ÿè¿™æ˜¯å› ä¸ºåœ¨å¾ˆå¤šåœºæ™¯ä¸‹ï¼ˆç‰¹åˆ«æ˜¯åœ¨ Transformer ç­‰ç»“æ„ä¸­ï¼‰ï¼Œæ—¶é—´ç‚¹åªä½œä¸ºä¸€ä¸ªâ€œä½ç½®æ ‡è¯†ç¬¦â€å­˜åœ¨ï¼Œä¾‹å¦‚ç¬¬å‡ ä¸ªæ—¶é—´æ­¥ï¼Œå®ƒæœ¬èº«çš„ç»å¯¹å«ä¹‰å¹¶ä¸é‚£ä¹ˆé‡è¦ã€‚

        # ç¯å¢ƒå‚æ•°å’Œæµ“åº¦çš„ MLPï¼Œç”¨äºç”Ÿæˆcls_token
        self.cls_token_mlp = AdjustableMLP(
            input_dim=4,  # [æ¸©åº¦, æµé‡, ç”µæµå¯†åº¦, æµ“åº¦]
            hidden_dims=cls_token_mlp_hidden_dims,
            output_dim=self.mlp_output_dims,  # ä¿æŒä¸volt_outputã€impe_outputç»´åº¦ä¸€è‡´ï¼ˆ32ï¼‰
            num_layers=cls_token_mlp_num_layers,
        )



        
        self.cross_attn = torch.nn.MultiheadAttention(embed_dim=mlp_output_dims, num_heads=nhead, batch_first=False)
        

        self.register_buffer("freq_mask", build_freq_mask(freq_values_hz))
        ion_attr_tensor = torch.tensor(ion_attr_list, dtype=torch.float32)
        self.register_buffer("ion_attr_tensor", ion_attr_tensor)


    def forward(self, volt_data, impe_data, env_params,concentrations):
        B, T, F, C = impe_data.shape  # (B,4,64,2)
        B2,T2,C2 = volt_data.shape
         # ==== æ·»åŠ æ–­è¨€æ£€æŸ¥ ====
        assert B == B2, f"Batch size mismatch: impe_data B={B}, volt_data B={B2}"
        assert T == T2, f"Time steps mismatch: impe_data T={T}, volt_data T={T2}"

        batch_size = B
        num_time_points = T
        num_freq_points = F
        device = volt_data.device

        # ========== ç”µå‹ç‰¹å¾ ==========
        volt_output = self.voltMLP(volt_data)  # (B,4,1)->(B,4,32)

        # ======== æ—¶é—´ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        time_base_input = torch.arange(T, device=device).float().view(T, 1)   # (T,1)
        time_encoded_base = self.time_embedding(time_base_input)              # (T,32)

        # æ‰©å±•åˆ° batch ç»´åº¦ (B,T,32)
        time_encoded = time_encoded_base.unsqueeze(0).expand(B, T, -1)

        # ç”µå‹ç‰¹å¾åŠ æ—¶é—´ç¼–ç 
        volt_output = volt_output + time_encoded

        # ========== é˜»æŠ—ç‰¹å¾ ==========
        impe_output = self.impeMLP(impe_data.view(B, T * F, -1))  # (B,256,32)
        impe_output = impe_output.view(B, T, F, -1)               # (B,4,64,32)

        # ======== é¢‘ç‡ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        freq_base_input = self.freq_values_tensor.view(F, 1)        # (64,1)
        freq_encoded_base = self.freq_encoder(freq_base_input)      # (64,32)
        freq_encoded = freq_encoded_base.unsqueeze(0).unsqueeze(0).expand(B, T, F, -1)

        # ======== æ—¶é—´ç¼–ç æ‰©å±•åˆ°é˜»æŠ—ç‰¹å¾ ========
        time_encoded_impe = time_encoded.unsqueeze(2).expand(B, T, F, -1)    # (B,4,64,32)

        # åŠ å…¥æ—¶é—´ç¼–ç +é¢‘ç‡ç¼–ç 
        impe_output = impe_output + time_encoded_impe + freq_encoded

        # reshapeå›åŸå§‹å½¢çŠ¶
        impe_output = impe_output.view(B, T * F, -1)   # (B,256,32)


        # æå–ç¯å¢ƒå‚æ•°
        temperature = env_params[:, 0].unsqueeze(1)      # (B,1)
        flow = env_params[:, 1].unsqueeze(1)             # (B,1)
        current = env_params[:, 2].unsqueeze(1)          # (B,1)

        # æå–æµ“åº¦ï¼ˆå‡è®¾ä½œä¸ºforwardå‚æ•°ä¼ å…¥ï¼Œå½¢çŠ¶ (B,1)ï¼‰
        # è®­ç»ƒé˜¶æ®µæœ‰å€¼ï¼Œæ¨ç†é˜¶æ®µå¯ä»¥ä¼ å…¥å‡å€¼æˆ–0å¡«å……
        conc = concentrations.unsqueeze(1) if concentrations.dim()==1 else concentrations  # (B,1)

        # æ‹¼æ¥ä¸º (B,4)
        cls_inputs = torch.cat([temperature, flow, current, conc], dim=1)

        # é€šè¿‡MLPç”Ÿæˆcls_tokenç‰¹å¾
        cls_token_feat = self.cls_token_mlp(cls_inputs)  # (B,32)
        cls_token_feat = cls_token_feat.unsqueeze(1)     # (B,1,32)

        # æ‹¼æ¥åˆ°volt_outputå’Œimpe_outputåé¢
        tokens = torch.cat([ volt_output, impe_output,cls_token_feat], dim=1)  # (B,1+4+256,32)
        transformer_output, attn_weights = self.transformer(tokens)  # (B, 261, 32)
 
        # å‡è®¾åªç”¨æœ€åä¸€å±‚çš„transformerçš„æ³¨æ„åŠ›
        attn_map = attn_weights[-1]  # shape: (B, num_heads, tgt_len, src_len)
        cls_attn = attn_map[:, :, -1, :]  # (B, num_heads, 261)
        cls_attn_mean = cls_attn.mean(dim=1)    # (B, 261)  â† å¹³å‡æ‰€æœ‰å¤´
        # å–æœ€åä¸€ä¸ªè¾“å‡º (blank input çš„è¾“å‡º)
        last_output = transformer_output[:, -1, :] # Shape: (batch_size, feature_dim)

        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹4ä¸ªtoken)
        freq_attn = cls_attn_mean[:, num_time_points:-1]  # (B, T*F)
        freq_attn = freq_attn.view(B, T, F)  # (B, T, F)


        # =============== ç”¨äºé¢„æµ‹äº”ä¸ªåˆå§‹çŠ¶æ€çš„ç‰©ç†å‚æ•° ===============
        # =============== 5ï¸âƒ£ ä½¿ç”¨ä¸»åˆ†æ”¯embeddingé¢„æµ‹åˆå§‹ç‰©ç†å‚æ•° ===============

        # 1ï¸âƒ£ å–ç”µå‹ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„embedding (B,32)
        volt_first_feat = volt_output[:, 0, :].unsqueeze(1)  # (B,1,32)

        # 2ï¸âƒ£ å–é˜»æŠ—ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„embedding (B,64,32)
        impe_first_feat = impe_output[:, 0:num_freq_points, :]  # (B,64,32)

        # 3ï¸âƒ£ æ‹¼æ¥ cls_token_featã€ç”µå‹å’Œé˜»æŠ—
        param_tokens = torch.cat([ cls_token_feat,volt_first_feat, impe_first_feat], dim=1)  # (B,66,32)

        # 4ï¸âƒ£ é€å…¥å‚æ•° Transformer
        param_encoded,param_attn_weights = self.param_transformer(param_tokens)  # (B,66,32)

        # Transformer
        param_attn_map = param_attn_weights[-1]  # (B, nhead, N_tokens, N_tokens)
        param_attn_weights = param_attn_map[:, :, 0, :]  # (B, nhead, N_tokens)
        param_attn_mean = param_attn_weights.mean(dim=1)  # (B, N_tokens)
        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹1ä¸ªtoken)
        freq_attn_param = param_attn_mean[:, 1:-1]  # (B, 1*F)
        freq_attn_param = freq_attn_param.view(B, F)  # (B, 1, F)
        
        first_cls_output = param_encoded[:, 0,:]  # (B, D)
        # print("first_cls_output.shape:",first_cls_output.shape)

        # 8. é€šè¿‡ paramMLP è¾“å‡º5ä¸ªåˆå§‹çŠ¶æ€å‚æ•°åŸå§‹ç‰©ç†é‡ï¼ˆä¸åš clampï¼Œä¸åš sigmoidï¼‰
        param_output = self.paramMLP(first_cls_output) # (B, 5*32)ï¼Œå…è®¸å…¶è‡ªç”±è¡¨è¾¾å¤§å¹…ç¯å¢ƒå˜åŒ–
        # print("param_output.shape:",param_output.shape)
        param_output = param_output.view(B, 5, self.transformer_d_model)       # (B, 5, 32)
        # æ¯ä¸ªå‚æ•°ç‹¬ç«‹å‹ç¼©ä¸ºæ ‡é‡
        param_values = self.param_compress(param_output) # (B, 5, 1)
        param_values = param_values.squeeze(-1)          # (B, 5)


        # ========== 2ï¸âƒ£ ç‰©ç†å½±å“å› å­éƒ¨åˆ† ==========
        raw_physic_output = self.physicMLP(last_output)  # (B, 160)
        raw_physic_output = raw_physic_output.view(B, 5, self.transformer_d_model)

        # æ¯ä¸ªå½±å“å› å­ç‹¬ç«‹å‹ç¼©
        influence_values = self.influence_compress(raw_physic_output)  # (B, 5, 1)
        influence_values = influence_values.squeeze(-1)   
        # print("influence_values.shape:",influence_values.shape) 



        sigmoid = torch.nn.Sigmoid()
        
        # # ------------------- è¡Œä¸ºå½±å“å› å­ï¼ˆphysic_outputï¼‰ -------------------
        theta_min, theta_max = 0.001, 0.999
        phi_min, phi_max     = 0.001, 0.999

        theta_ca = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 0:1])
        phi_ca   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 1:2])
        theta_an = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 2:3])
        phi_an   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 3:4])
        psi      = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 4:5])  # åŒæ ·é™åˆ¶


        # ------------------- å›ºæœ‰ç‰©æ€§å‚æ•°ï¼ˆparam_outputï¼‰ -------------------
        sigma_min, sigma_max = 0.01, 2  # S/cm
        sigma_mem = sigma_min + (sigma_max - sigma_min) * sigmoid(param_values[:, 0:1])

        alpha_min, alpha_max = 0.01, 2# S/cm
        alpha_ca = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 1:2])
        alpha_an = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 2:3])

        log_i0ca_min, log_i0ca_max = -9, 0# S/cm
        log_i0an_min, log_i0an_max = -9, 0# S/cm
        log_i0ca = log_i0ca_min + (log_i0ca_max - log_i0ca_min) * sigmoid(param_values[:, 3:4])
        log_i0an = log_i0an_min + (log_i0an_max - log_i0an_min) * sigmoid(param_values[:, 4:5])
        i_0ca = torch.pow(10, log_i0ca)# S/cm
        i_0an = torch.pow(10, log_i0an)# S/cm


        
        
        # ========== Cross Attention èåˆå½±å“å› å­å’Œç‰©æ€§å‚æ•° ==========
        # ========== 3ï¸âƒ£ Cross Attention äº¤äº’ ==========
        # Query: param_output, Key: raw_physic_output
        cross_output, cross_attn_weights = self.my_cross_attn_transformer(
            query=param_output,
            key_value=raw_physic_output
        )

        # 1ï¸âƒ£ æŠ•å½±
        # physic_embed = self.probMLP(param_output)  # (B, 5, 32)
        physic_embed_raw = self.probMLP(cross_output)  # (B, 5, 32)
        physic_embed = self.prob_norm(param_output + physic_embed_raw)  # âœ… æ®‹å·® + norm

        # 1ï¸âƒ£ è¾“å…¥ ion_attr_tensorï¼ˆé™æ€å±æ€§è¡¨ï¼‰ï¼Œå½¢çŠ¶ä¸º (6, 7)
        ion_attr_proj = self.ion_attr_embed(self.ion_attr_tensor)  # (6, 32)

        # 2ï¸âƒ£ æ·»åŠ  batch ç»´åº¦ï¼Œè¾“å…¥ Transformer
        ion_attr_proj = ion_attr_proj.unsqueeze(0)  # (1, 6, 32)

        # 3ï¸âƒ£ ç¼–ç ï¼Œè¿”å›ç¼–ç ç»“æœå’Œæ³¨æ„åŠ›æƒé‡
        ion_encoded, ion_attn_weights = self.ion_encoder(ion_attr_proj)  # (1, 6, 32), list of attn

        # 4ï¸âƒ£ å»æ‰ batch ç»´åº¦ï¼Œä½œä¸ºæ¯ä¸ªç¦»å­çš„åµŒå…¥å‘é‡
        ion_embeddings_raw = self.ion_postMLP(ion_encoded.squeeze(0))   # (6, 32)
        ion_embeddings = self.ion_norm(ion_encoded.squeeze(0) + ion_embeddings_raw)  # âœ… æ®‹å·® + norm

        # 5ï¸âƒ£ ç‚¹ç§¯ï¼šparam_output -> physic_embed (B, 5, 32)
        #         ion_embeddings -> (6, 32)
        raw_scores = torch.matmul(physic_embed, ion_embeddings.T)  # (B, 5, 6)

        # 6ï¸âƒ£ èšåˆ
        raw_prob_output = raw_scores.mean(dim=1)  # (B, 6)

        # 4ï¸âƒ£ Softmax
        prob_output = torch.softmax(raw_prob_output, dim=1)
        

    


        # ---------- ç”µå‹è®¡ç®—ï¼šåŸºäºé˜²NaNã€ç¨³å®šåçš„å‚æ•° ----------
        predicted_voltages = []
        for i in range(batch_size):
            pred_v, _, _, _, _ = calculate_predicted_voltage(
                theta_ca[i], phi_ca[i], theta_an[i], phi_an[i], psi[i],
                temperature[i], flow[i], current[i],
                sigma_mem[i], alpha_ca[i], alpha_an[i], i_0ca[i], i_0an[i],
                         )
            predicted_voltages.append(pred_v)

        # ç»„åˆ batch çš„ç”µå‹ç»“æœ
        predicted_voltage = torch.stack(predicted_voltages).unsqueeze(1)  # shape: (B, 1)

        wuxing = [sigma_mem,alpha_ca,alpha_an,i_0ca,i_0an]
        influence=[psi,theta_ca,theta_an,phi_ca,phi_an]

        return prob_output, predicted_voltage, wuxing,cls_attn_mean ,raw_prob_output,influence,param_attn_mean,freq_attn,freq_attn_param

def build_freq_mask(freq_values_hz: torch.Tensor, debug: bool = False) -> torch.Tensor:
    """
    æ ¹æ®é¢„å®šä¹‰é¢‘å¸¦æ„é€ ä¸€ä¸ª 0/1 æ©ç ï¼Œç”¨äºåç»­åšé¢‘å¸¦ç»Ÿè®¡æˆ–å¯è§†åŒ–ã€‚

    Args:
        freq_values_hz: é¢‘ç‡åˆ—è¡¨ï¼Œshape = (F,)
        debug: æ˜¯å¦æ‰“å°åŒ¹é…åˆ°çš„é¢‘ç‡ç´¢å¼•ï¼ˆé»˜è®¤ Falseï¼‰

    Returns:
        mask: shape = (F,) çš„ 0/1 tensor
    """
    freq_values_hz = freq_values_hz.to(torch.float32)
    mask = torch.zeros_like(freq_values_hz, dtype=torch.float32)

    # åˆ¤æ–­æ˜¯å¦è¦å»æ‰ 0.01 Hz
    last_band = [0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259, 0.01]
    if len(freq_values_hz) == 63:
        last_band = last_band[:-1]  # åˆ é™¤æœ€åä¸€ä¸ª 0.01 Hz

    target_bands = [
        [19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000],
        [794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0],
        [79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0],
        [7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0],
        [0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1],
        last_band,
    ]

    # ç”¨ä¸€ä¸ªå°çš„ç›¸å¯¹è¯¯å·®åšåŒ¹é…
    tol = 1e-4
    for band in target_bands:
        for f_target in band:
            diff = torch.abs(freq_values_hz - f_target)
            rel_err = diff / (torch.abs(freq_values_hz) + 1e-12)
            idx = torch.argmin(rel_err)
            if rel_err[idx] < tol:
                mask[idx] = 1.0

    if debug:
        print("ğŸ” freq_mask non-zero count:", int(mask.sum().item()))
        print("ğŸ” freq_mask indices:", torch.nonzero(mask).squeeze(-1).tolist())

    return mask





# å®šä¹‰å®Œæ•´æ¨¡å‹
class Model_three_system_one_encoder(nn.Module):#ä¸‰ç³»ç»Ÿæ¶æ„
    def __init__(self, volt_input_dim, volt_mlp_hidden_dims,mlp_output_dims, volt_mlp_num_layers,
                 impe_input_dim,impe_mlp_hidden_dims,  impe_mlp_num_layers,
                 transformer_d_model, nhead, transformer_num_layers,param_transformer_num_layers,
                 physic_mlp_hidden_dims, physic_mlp_num_layers,
                 ion_attr_embed_hidden_dims,ion_attr_embed_num_layers,
                 ion_encoder_num_layers,
                 ion_post_hidden_dims,ion_post_num_layers,
                 probMLP_input_dims,probMLP_hidden_dims,probMLP_num_layers,
                 param_mlp_hidden_dims,param_mlp_num_layers,
                 freq_encoder_hidden_dims,freq_encoder_num_layers,
                 time_encoder_hidden_dims,time_encoder_num_layers,
                 cross_transformer_num_layers,
                 param_embed_hidden_dims,param_embed_num_layers,
                 physic_embed_hidden_dims,physic_embed_num_layers,
                 cls_token_mlp_hidden_dims, cls_token_mlp_num_layers,
                 num_freq_points,num_time_points):
        
        
        super(Model_three_system_one_encoder, self).__init__()
        self.voltMLP = AdjustableMLP(volt_input_dim, volt_mlp_hidden_dims, mlp_output_dims, volt_mlp_num_layers)
        self.impeMLP = AdjustableMLP(impe_input_dim, impe_mlp_hidden_dims, mlp_output_dims, impe_mlp_num_layers)
        self.transformer = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=transformer_num_layers)
        self.physicMLP = AdjustableMLP(transformer_d_model, physic_mlp_hidden_dims, 5*transformer_d_model, physic_mlp_num_layers)
        


        self.probMLP = AdjustableMLP(input_dim=probMLP_input_dims, hidden_dims=probMLP_hidden_dims, output_dim=mlp_output_dims, num_layers=probMLP_num_layers)  # å°† physic_output æŠ•å½±åˆ°åµŒå…¥ç©ºé—´

        
        # å®šä¹‰ç‰©ç†æ¨¡å‹éƒ¨åˆ†
        self.paramMLP = AdjustableMLP(transformer_d_model,param_mlp_hidden_dims, 5*transformer_d_model, param_mlp_num_layers)
        self.norm = nn.LayerNorm(transformer_d_model)
        self.ion_attr_embed = AdjustableMLP(input_dim=len(ion_attr_list[0]), hidden_dims=ion_attr_embed_hidden_dims, output_dim=transformer_d_model, num_layers=ion_attr_embed_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨
        self.ion_encoder = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=ion_encoder_num_layers)
        self.ion_postMLP = AdjustableMLP(input_dim=transformer_d_model, hidden_dims=ion_post_hidden_dims, output_dim=transformer_d_model, num_layers=ion_post_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨

        self.mlp_output_dims = mlp_output_dims
        self.my_cross_attn_transformer = MyCrossAttnTransformer(d_model=transformer_d_model, nhead=nhead, num_layers=cross_transformer_num_layers)


        self.param_compress = AdjustableMLP(transformer_d_model, param_embed_hidden_dims, 1, param_embed_num_layers)
        self.influence_compress = AdjustableMLP(transformer_d_model, physic_embed_hidden_dims, 1, physic_embed_num_layers)

        self.transformer_d_model = transformer_d_model
        # å¯¹ physic_embed æ·»åŠ æ®‹å·®å’Œ norm
        self.prob_norm = nn.LayerNorm(transformer_d_model)

        # å¯¹ ion_embeddings æ·»åŠ æ®‹å·®å’Œ norm
        self.ion_norm = nn.LayerNorm(transformer_d_model)

        # çœŸå®é¢‘ç‡åˆ—è¡¨ï¼ˆ64ä¸ªç‚¹ï¼‰
        if num_freq_points==64:
            freq_values_hz = torch.tensor([
                19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
                794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
                79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
                7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
                0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
                0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259, 0.01
            ], dtype=torch.float32)
        elif num_freq_points==63:
            freq_values_hz = torch.tensor([
            19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
            794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
            79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
            7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
            0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
            0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259
            ], dtype=torch.float32)
        else:
            print("num_freq_points:",num_freq_points)
        # å½’ä¸€åŒ–åˆ° å¯¹æ•°åæ ‡ç³»[0, 1]
        freq_values_log = torch.log10(freq_values_hz)  # log10å˜æ¢
        freq_values_norm = (freq_values_log - freq_values_log.min()) / (freq_values_log.max() - freq_values_log.min())
        self.register_buffer("freq_values_tensor", freq_values_norm)     

        # å®šä¹‰ frequency encoder MLP,å°†é¢‘ç‡æ•°æ®å˜ä¸ºå’ŒMLPåŒæ ·çš„å¤§å°ï¼Œè¿™æ ·å¯ä»¥å¯¹é˜»æŠ—æ•°æ®è¿›è¡Œé¢‘ç‡ç¼–ç 
        self.freq_encoder = AdjustableMLP(1, freq_encoder_hidden_dims, mlp_output_dims, freq_encoder_num_layers)#(64ï¼Œ1)->(64,32)
        # æ—¶é—´ç¼–ç å™¨
        self.time_embedding = AdjustableMLP(1, time_encoder_hidden_dims, mlp_output_dims, time_encoder_num_layers)  # nn.Embedding é»˜è®¤åªèƒ½å¤„ç†ç¦»æ•£ç´¢å¼•ï¼ˆå¦‚ 0ã€1ã€2ã€3ï¼‰ï¼Œå®ƒå¹¶ä¸ä¼šæ˜¾å¼æ•æ‰åˆ°è¿™äº›æ—¶é—´ç‚¹åœ¨ç‰©ç†ä¸Šæ˜¯â€œè¿ç»­ä¸”é—´éš”ä¸º2å°æ—¶â€çš„ã€‚é‚£ä¸ºä»€ä¹ˆè¿˜ä¼šç”¨ nn.Embedding è¡¨ç¤ºæ—¶é—´ï¼Ÿè¿™æ˜¯å› ä¸ºåœ¨å¾ˆå¤šåœºæ™¯ä¸‹ï¼ˆç‰¹åˆ«æ˜¯åœ¨ Transformer ç­‰ç»“æ„ä¸­ï¼‰ï¼Œæ—¶é—´ç‚¹åªä½œä¸ºä¸€ä¸ªâ€œä½ç½®æ ‡è¯†ç¬¦â€å­˜åœ¨ï¼Œä¾‹å¦‚ç¬¬å‡ ä¸ªæ—¶é—´æ­¥ï¼Œå®ƒæœ¬èº«çš„ç»å¯¹å«ä¹‰å¹¶ä¸é‚£ä¹ˆé‡è¦ã€‚

        # ç¯å¢ƒå‚æ•°å’Œæµ“åº¦çš„ MLPï¼Œç”¨äºç”Ÿæˆcls_token
        self.cls_token_mlp = AdjustableMLP(
            input_dim=4,  # [æ¸©åº¦, æµé‡, ç”µæµå¯†åº¦, æµ“åº¦]
            hidden_dims=cls_token_mlp_hidden_dims,
            output_dim=self.mlp_output_dims,  # ä¿æŒä¸volt_outputã€impe_outputç»´åº¦ä¸€è‡´ï¼ˆ32ï¼‰
            num_layers=cls_token_mlp_num_layers,
        )



        
        self.cross_attn = torch.nn.MultiheadAttention(embed_dim=mlp_output_dims, num_heads=nhead, batch_first=False)
        

        self.register_buffer("freq_mask", build_freq_mask(freq_values_hz))
        ion_attr_tensor = torch.tensor(ion_attr_list, dtype=torch.float32)
        self.register_buffer("ion_attr_tensor", ion_attr_tensor)


    def forward(self, volt_data, impe_data, env_params,concentrations):
        B, T, F, C = impe_data.shape  # (B,4,64,2)
        B2,T2,C2 = volt_data.shape
         # ==== æ·»åŠ æ–­è¨€æ£€æŸ¥ ====
        assert B == B2, f"Batch size mismatch: impe_data B={B}, volt_data B={B2}"
        assert T == T2, f"Time steps mismatch: impe_data T={T}, volt_data T={T2}"

        batch_size = B
        num_time_points = T
        num_freq_points = F
        device = volt_data.device

        # ========== ç”µå‹ç‰¹å¾ ==========
        volt_output = self.voltMLP(volt_data)  # (B,4,1)->(B,4,32)

        # ======== æ—¶é—´ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        time_base_input = torch.arange(T, device=device).float().view(T, 1)   # (T,1)
        time_encoded_base = self.time_embedding(time_base_input)              # (T,32)

        # æ‰©å±•åˆ° batch ç»´åº¦ (B,T,32)
        time_encoded = time_encoded_base.unsqueeze(0).expand(B, T, -1)

        # ç”µå‹ç‰¹å¾åŠ æ—¶é—´ç¼–ç 
        volt_output = volt_output + time_encoded

        # ========== é˜»æŠ—ç‰¹å¾ ==========
        impe_output = self.impeMLP(impe_data.view(B, T * F, -1))  # (B,256,32)
        impe_output = impe_output.view(B, T, F, -1)               # (B,4,64,32)

        # ======== é¢‘ç‡ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        freq_base_input = self.freq_values_tensor.view(F, 1)        # (64,1)
        freq_encoded_base = self.freq_encoder(freq_base_input)      # (64,32)
        freq_encoded = freq_encoded_base.unsqueeze(0).unsqueeze(0).expand(B, T, F, -1)

        # ======== æ—¶é—´ç¼–ç æ‰©å±•åˆ°é˜»æŠ—ç‰¹å¾ ========
        time_encoded_impe = time_encoded.unsqueeze(2).expand(B, T, F, -1)    # (B,4,64,32)

        # åŠ å…¥æ—¶é—´ç¼–ç +é¢‘ç‡ç¼–ç 
        impe_output = impe_output + time_encoded_impe + freq_encoded

        # reshapeå›åŸå§‹å½¢çŠ¶
        impe_output = impe_output.view(B, T * F, -1)   # (B,256,32)


        # æå–ç¯å¢ƒå‚æ•°
        temperature = env_params[:, 0].unsqueeze(1)      # (B,1)
        flow = env_params[:, 1].unsqueeze(1)             # (B,1)
        current = env_params[:, 2].unsqueeze(1)          # (B,1)

        # æå–æµ“åº¦ï¼ˆå‡è®¾ä½œä¸ºforwardå‚æ•°ä¼ å…¥ï¼Œå½¢çŠ¶ (B,1)ï¼‰
        # è®­ç»ƒé˜¶æ®µæœ‰å€¼ï¼Œæ¨ç†é˜¶æ®µå¯ä»¥ä¼ å…¥å‡å€¼æˆ–0å¡«å……
        conc = concentrations.unsqueeze(1) if concentrations.dim()==1 else concentrations  # (B,1)

        # æ‹¼æ¥ä¸º (B,4)
        cls_inputs = torch.cat([temperature, flow, current, conc], dim=1)

        # é€šè¿‡MLPç”Ÿæˆcls_tokenç‰¹å¾
        cls_token_feat = self.cls_token_mlp(cls_inputs)  # (B,32)
        cls_token_feat = cls_token_feat.unsqueeze(1)     # (B,1,32)

        # æ‹¼æ¥åˆ°volt_outputå’Œimpe_outputåé¢
        tokens = torch.cat([cls_token_feat,volt_output, impe_output,cls_token_feat], dim=1)  # (B,1+4+256,32)
        transformer_output, attn_weights = self.transformer(tokens)  # (B, 261, 32)
 
        # å‡è®¾åªç”¨æœ€åä¸€å±‚çš„transformerçš„æ³¨æ„åŠ›
        attn_map = attn_weights[-1]  # shape: (B, num_heads, tgt_len, src_len)
        cls_attn = attn_map[:, :, -1, :]  # (B, num_heads, 261)
        cls_attn_mean = cls_attn.mean(dim=1)    # (B, 261)  â† å¹³å‡æ‰€æœ‰å¤´
        # å–æœ€åä¸€ä¸ªè¾“å‡º (blank input çš„è¾“å‡º)
        last_output = transformer_output[:, -1, :] # Shape: (batch_size, feature_dim)

        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹4ä¸ªtoken)
        freq_attn = cls_attn_mean[:, 1+num_time_points:-1]  # (B, T*F)
        freq_attn = freq_attn.view(B, T, F)  # (B, T, F)


        # Transformer
        param_attn_map = attn_weights[-1]  # (B, nhead, N_tokens, N_tokens)
        param_attn_weights = attn_map[:, :, 0, :]  # (B, nhead, N_tokens)
        param_attn_mean = param_attn_weights.mean(dim=1)  # (B, N_tokens)
        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹1ä¸ªtoken)
        freq_attn_param = param_attn_mean[:, 1:F+1]  # (B, 1*F)
        freq_attn_param = freq_attn_param.view(B, F)  # (B, 1, F)
        
        first_cls_output = transformer_output[:, 0,:]  # (B, D)
        # print("first_cls_output.shape:",first_cls_output.shape)

        # 8. é€šè¿‡ paramMLP è¾“å‡º5ä¸ªåˆå§‹çŠ¶æ€å‚æ•°åŸå§‹ç‰©ç†é‡ï¼ˆä¸åš clampï¼Œä¸åš sigmoidï¼‰
        param_output = self.paramMLP(first_cls_output) # (B, 5*32)ï¼Œå…è®¸å…¶è‡ªç”±è¡¨è¾¾å¤§å¹…ç¯å¢ƒå˜åŒ–
        # print("param_output.shape:",param_output.shape)
        param_output = param_output.view(B, 5, self.transformer_d_model)       # (B, 5, 32)
        # æ¯ä¸ªå‚æ•°ç‹¬ç«‹å‹ç¼©ä¸ºæ ‡é‡
        param_values = self.param_compress(param_output) # (B, 5, 1)
        param_values = param_values.squeeze(-1)          # (B, 5)


        # ========== 2ï¸âƒ£ ç‰©ç†å½±å“å› å­éƒ¨åˆ† ==========
        raw_physic_output = self.physicMLP(last_output)  # (B, 160)
        raw_physic_output = raw_physic_output.view(B, 5, self.transformer_d_model)

        # æ¯ä¸ªå½±å“å› å­ç‹¬ç«‹å‹ç¼©
        influence_values = self.influence_compress(raw_physic_output)  # (B, 5, 1)
        influence_values = influence_values.squeeze(-1)   
        # print("influence_values.shape:",influence_values.shape) 



        sigmoid = torch.nn.Sigmoid()
        
        # # ------------------- è¡Œä¸ºå½±å“å› å­ï¼ˆphysic_outputï¼‰ -------------------
        theta_min, theta_max = 0.001, 0.999
        phi_min, phi_max     = 0.001, 0.999

        theta_ca = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 0:1])
        phi_ca   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 1:2])
        theta_an = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 2:3])
        phi_an   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 3:4])
        psi      = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 4:5])  # åŒæ ·é™åˆ¶


        # ------------------- å›ºæœ‰ç‰©æ€§å‚æ•°ï¼ˆparam_outputï¼‰ -------------------
        sigma_min, sigma_max = 0.01, 2  # S/cm
        sigma_mem = sigma_min + (sigma_max - sigma_min) * sigmoid(param_values[:, 0:1])

        alpha_min, alpha_max = 0.01, 2
        alpha_ca = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 1:2])
        alpha_an = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 2:3])

        log_i0ca_min, log_i0ca_max = -9, 0
        log_i0an_min, log_i0an_max = -9, 0
        log_i0ca = log_i0ca_min + (log_i0ca_max - log_i0ca_min) * sigmoid(param_values[:, 3:4])
        log_i0an = log_i0an_min + (log_i0an_max - log_i0an_min) * sigmoid(param_values[:, 4:5])
        i_0ca = torch.pow(10, log_i0ca)
        i_0an = torch.pow(10, log_i0an)


        
        
        # ========== Cross Attention èåˆå½±å“å› å­å’Œç‰©æ€§å‚æ•° ==========
        # ========== 3ï¸âƒ£ Cross Attention äº¤äº’ ==========
        # Query: param_output, Key: raw_physic_output
        cross_output, cross_attn_weights = self.my_cross_attn_transformer(
            query=param_output,
            key_value=raw_physic_output
        )

        # 1ï¸âƒ£ æŠ•å½±
        # physic_embed = self.probMLP(param_output)  # (B, 5, 32)
        physic_embed_raw = self.probMLP(cross_output)  # (B, 5, 32)
        physic_embed = self.prob_norm(param_output + physic_embed_raw)  # âœ… æ®‹å·® + norm

        # 1ï¸âƒ£ è¾“å…¥ ion_attr_tensorï¼ˆé™æ€å±æ€§è¡¨ï¼‰ï¼Œå½¢çŠ¶ä¸º (6, 7)
        ion_attr_proj = self.ion_attr_embed(self.ion_attr_tensor)  # (6, 32)

        # 2ï¸âƒ£ æ·»åŠ  batch ç»´åº¦ï¼Œè¾“å…¥ Transformer
        ion_attr_proj = ion_attr_proj.unsqueeze(0)  # (1, 6, 32)

        # 3ï¸âƒ£ ç¼–ç ï¼Œè¿”å›ç¼–ç ç»“æœå’Œæ³¨æ„åŠ›æƒé‡
        ion_encoded, ion_attn_weights = self.ion_encoder(ion_attr_proj)  # (1, 6, 32), list of attn

        # 4ï¸âƒ£ å»æ‰ batch ç»´åº¦ï¼Œä½œä¸ºæ¯ä¸ªç¦»å­çš„åµŒå…¥å‘é‡
        ion_embeddings_raw = self.ion_postMLP(ion_encoded.squeeze(0))   # (6, 32)
        ion_embeddings = self.ion_norm(ion_encoded.squeeze(0) + ion_embeddings_raw)  # âœ… æ®‹å·® + norm

        # 5ï¸âƒ£ ç‚¹ç§¯ï¼šparam_output -> physic_embed (B, 5, 32)
        #         ion_embeddings -> (6, 32)
        raw_scores = torch.matmul(physic_embed, ion_embeddings.T)  # (B, 5, 6)

        # 6ï¸âƒ£ èšåˆ
        raw_prob_output = raw_scores.mean(dim=1)  # (B, 6)

        # 4ï¸âƒ£ Softmax
        prob_output = torch.softmax(raw_prob_output, dim=1)
        

    


        # ---------- ç”µå‹è®¡ç®—ï¼šåŸºäºé˜²NaNã€ç¨³å®šåçš„å‚æ•° ----------
        predicted_voltages = []
        for i in range(batch_size):
            pred_v, _, _, _, _ = calculate_predicted_voltage(
                theta_ca[i], phi_ca[i], theta_an[i], phi_an[i], psi[i],
                temperature[i], flow[i], current[i],
                sigma_mem[i], alpha_ca[i], alpha_an[i], i_0ca[i], i_0an[i]
            )
            predicted_voltages.append(pred_v)

        # ç»„åˆ batch çš„ç”µå‹ç»“æœ
        predicted_voltage = torch.stack(predicted_voltages).unsqueeze(1)  # shape: (B, 1)

        wuxing = [sigma_mem,alpha_ca,alpha_an,i_0ca,i_0an]
        influence=[psi,theta_ca,theta_an,phi_ca,phi_an]

        return prob_output, predicted_voltage, wuxing,cls_attn_mean ,raw_prob_output,influence,param_attn_mean,freq_attn,freq_attn_param


# å®šä¹‰å®Œæ•´æ¨¡å‹
class Model_three_system_plus(nn.Module):#ä¸‰ç³»ç»Ÿæ¶æ„
    def __init__(self, volt_input_dim, volt_mlp_hidden_dims,mlp_output_dims, volt_mlp_num_layers,
                 impe_input_dim,impe_mlp_hidden_dims,  impe_mlp_num_layers,
                 transformer_d_model, nhead, transformer_num_layers,param_transformer_num_layers,
                 physic_mlp_hidden_dims, physic_mlp_num_layers,
                 ion_attr_embed_hidden_dims,ion_attr_embed_num_layers,
                 ion_encoder_num_layers,
                 ion_post_hidden_dims,ion_post_num_layers,
                 probMLP_input_dims,probMLP_hidden_dims,probMLP_num_layers,
                 param_mlp_hidden_dims,param_mlp_num_layers,
                 freq_encoder_hidden_dims,freq_encoder_num_layers,
                 time_encoder_hidden_dims,time_encoder_num_layers,
                 cross_transformer_num_layers,
                 param_embed_hidden_dims,param_embed_num_layers,
                 physic_embed_hidden_dims,physic_embed_num_layers,
                 envMLP_input_dim, env_mlp_hidden_dims, env_mlp_num_layers,
                 ep_input_dim, ep_mlp_hidden_dims,  ep_mlp_num_layers,
                 conc_input_dim, conc_mlp_hidden_dims, conc_mlp_num_layers,
                 Z_encoder_num_layers ,
                 num_freq_points,num_time_points):
        
        
        super(Model_three_system_plus, self).__init__()
        self.voltMLP = AdjustableMLP(volt_input_dim, volt_mlp_hidden_dims, mlp_output_dims, volt_mlp_num_layers)
        self.impeMLP = AdjustableMLP(impe_input_dim, impe_mlp_hidden_dims, mlp_output_dims, impe_mlp_num_layers)
        self.transformer = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=transformer_num_layers)
        self.physicMLP = AdjustableMLP(transformer_d_model, physic_mlp_hidden_dims, 5*transformer_d_model, physic_mlp_num_layers)
        


        self.probMLP = AdjustableMLP(input_dim=probMLP_input_dims, hidden_dims=probMLP_hidden_dims, output_dim=mlp_output_dims, num_layers=probMLP_num_layers)  # å°† physic_output æŠ•å½±åˆ°åµŒå…¥ç©ºé—´

        
        # å®šä¹‰ç‰©ç†æ¨¡å‹éƒ¨åˆ†
        self.param_transformer =  MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=param_transformer_num_layers )
        self.paramMLP = AdjustableMLP(transformer_d_model,param_mlp_hidden_dims, 5*transformer_d_model, param_mlp_num_layers)
        self.norm = nn.LayerNorm(transformer_d_model)
        self.ion_attr_embed = AdjustableMLP(input_dim=len(ion_attr_list[0]), hidden_dims=ion_attr_embed_hidden_dims, output_dim=transformer_d_model, num_layers=ion_attr_embed_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨
        self.ion_encoder = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=ion_encoder_num_layers)
        self.ion_postMLP = AdjustableMLP(input_dim=transformer_d_model, hidden_dims=ion_post_hidden_dims, output_dim=transformer_d_model, num_layers=ion_post_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨

        self.mlp_output_dims = mlp_output_dims
        self.my_cross_attn_transformer = MyCrossAttnTransformer(d_model=transformer_d_model, nhead=nhead, num_layers=cross_transformer_num_layers)


        self.param_compress = AdjustableMLP(transformer_d_model, param_embed_hidden_dims, 1, param_embed_num_layers)
        self.influence_compress = AdjustableMLP(transformer_d_model, physic_embed_hidden_dims, 1, physic_embed_num_layers)

        self.transformer_d_model = transformer_d_model
        # å¯¹ physic_embed æ·»åŠ æ®‹å·®å’Œ norm
        self.prob_norm = nn.LayerNorm(transformer_d_model)

        # å¯¹ ion_embeddings æ·»åŠ æ®‹å·®å’Œ norm
        self.ion_norm = nn.LayerNorm(transformer_d_model)


        self.envMLP = AdjustableMLP(envMLP_input_dim, env_mlp_hidden_dims, mlp_output_dims, env_mlp_num_layers)
        self.epMLP = AdjustableMLP(ep_input_dim, ep_mlp_hidden_dims, mlp_output_dims, ep_mlp_num_layers)
        self.concMLP = AdjustableMLP(conc_input_dim, conc_mlp_hidden_dims, mlp_output_dims, conc_mlp_num_layers)
        self.Z_encoder =  MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=Z_encoder_num_layers )

        # 1ï¸âƒ£ å®šä¹‰ä¸€ä¸ªå¯å­¦ä¹ çš„ cls_token (åˆå§‹å€¼å¯ç”¨æ­£æ€åˆ†å¸ƒ)
        self.cls_token = nn.Parameter(torch.zeros(1, transformer_d_model))
        nn.init.trunc_normal_(self.cls_token, std=0.02)




        # çœŸå®é¢‘ç‡åˆ—è¡¨ï¼ˆ64ä¸ªç‚¹ï¼‰
        if num_freq_points==64:
            freq_values_hz = torch.tensor([
                19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
                794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
                79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
                7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
                0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
                0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259, 0.01
            ], dtype=torch.float32)
        elif num_freq_points==63:
            freq_values_hz = torch.tensor([
            19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
            794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
            79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
            7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
            0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
            0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259
            ], dtype=torch.float32)
        else:
            print("num_freq_points:",num_freq_points)
        # å½’ä¸€åŒ–åˆ° å¯¹æ•°åæ ‡ç³»[0, 1]
        freq_values_log = torch.log10(freq_values_hz)  # log10å˜æ¢
        freq_values_norm = (freq_values_log - freq_values_log.min()) / (freq_values_log.max() - freq_values_log.min())
        self.register_buffer("freq_values_tensor", freq_values_norm)     

        # å®šä¹‰ frequency encoder MLP,å°†é¢‘ç‡æ•°æ®å˜ä¸ºå’ŒMLPåŒæ ·çš„å¤§å°ï¼Œè¿™æ ·å¯ä»¥å¯¹é˜»æŠ—æ•°æ®è¿›è¡Œé¢‘ç‡ç¼–ç 
        self.freq_encoder = AdjustableMLP(1, freq_encoder_hidden_dims, mlp_output_dims, freq_encoder_num_layers)#(64ï¼Œ1)->(64,32)
        # æ—¶é—´ç¼–ç å™¨
        self.time_embedding = AdjustableMLP(1, time_encoder_hidden_dims, mlp_output_dims, time_encoder_num_layers)  # nn.Embedding é»˜è®¤åªèƒ½å¤„ç†ç¦»æ•£ç´¢å¼•ï¼ˆå¦‚ 0ã€1ã€2ã€3ï¼‰ï¼Œå®ƒå¹¶ä¸ä¼šæ˜¾å¼æ•æ‰åˆ°è¿™äº›æ—¶é—´ç‚¹åœ¨ç‰©ç†ä¸Šæ˜¯â€œè¿ç»­ä¸”é—´éš”ä¸º2å°æ—¶â€çš„ã€‚é‚£ä¸ºä»€ä¹ˆè¿˜ä¼šç”¨ nn.Embedding è¡¨ç¤ºæ—¶é—´ï¼Ÿè¿™æ˜¯å› ä¸ºåœ¨å¾ˆå¤šåœºæ™¯ä¸‹ï¼ˆç‰¹åˆ«æ˜¯åœ¨ Transformer ç­‰ç»“æ„ä¸­ï¼‰ï¼Œæ—¶é—´ç‚¹åªä½œä¸ºä¸€ä¸ªâ€œä½ç½®æ ‡è¯†ç¬¦â€å­˜åœ¨ï¼Œä¾‹å¦‚ç¬¬å‡ ä¸ªæ—¶é—´æ­¥ï¼Œå®ƒæœ¬èº«çš„ç»å¯¹å«ä¹‰å¹¶ä¸é‚£ä¹ˆé‡è¦ã€‚





        
        self.cross_attn = torch.nn.MultiheadAttention(embed_dim=mlp_output_dims, num_heads=nhead, batch_first=False)
        
        ion_attr_tensor = torch.tensor(ion_attr_list_plus, dtype=torch.float32)
        self.register_buffer("ion_attr_tensor", ion_attr_tensor)


    def forward(self, volt_data, impe_data, env_params,electrolyzer_parameters,concentration):
        B, T, F, C = impe_data.shape  # (B,4,64,2)
        B2,T2,C2 = volt_data.shape
         # ==== æ·»åŠ æ–­è¨€æ£€æŸ¥ ====
        assert B == B2, f"Batch size mismatch: impe_data B={B}, volt_data B={B2}"
        assert T == T2, f"Time steps mismatch: impe_data T={T}, volt_data T={T2}"

        batch_size = B
        num_time_points = T
        num_freq_points = F
        device = volt_data.device

        # ========== ç”µå‹ç‰¹å¾ ==========
        volt_output = self.voltMLP(volt_data)  # (B,4,1)->(B,4,32)

        # ======== æ—¶é—´ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        time_base_input = torch.arange(T, device=device).float().view(T, 1)   # (T,1)
        time_encoded_base = self.time_embedding(time_base_input)              # (T,32)

        # æ‰©å±•åˆ° batch ç»´åº¦ (B,T,32)
        time_encoded = time_encoded_base.unsqueeze(0).expand(B, T, -1)

        # ç”µå‹ç‰¹å¾åŠ æ—¶é—´ç¼–ç 
        volt_output = volt_output + time_encoded

        # ========== é˜»æŠ—ç‰¹å¾ ==========
        impe_output = self.impeMLP(impe_data.view(B, T * F, -1))  # (B,256,32)
        impe_output = impe_output.view(B, T, F, -1)               # (B,4,64,32)

        # ======== é¢‘ç‡ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        freq_base_input = self.freq_values_tensor.view(F, 1)        # (64,1)
        freq_encoded_base = self.freq_encoder(freq_base_input)      # (64,32)
        freq_encoded = freq_encoded_base.unsqueeze(0).unsqueeze(0).expand(B, T, F, -1)

        # ======== æ—¶é—´ç¼–ç æ‰©å±•åˆ°é˜»æŠ—ç‰¹å¾ ========
        time_encoded_impe = time_encoded.unsqueeze(2).expand(B, T, F, -1)    # (B,4,64,32)

        # åŠ å…¥æ—¶é—´ç¼–ç +é¢‘ç‡ç¼–ç 
        impe_output = impe_output + time_encoded_impe + freq_encoded

        # reshapeå›åŸå§‹å½¢çŠ¶
        impe_output = impe_output.view(B, T * F, -1)   # (B,256,32)


        # æå–ç¯å¢ƒå‚æ•°
        temperature = env_params[:, 0].unsqueeze(1)      # (B,1)
        flow = env_params[:, 1].unsqueeze(1)             # (B,1)
        current = env_params[:, 2].unsqueeze(1)          # (B,1)


        # (1) ç¼–ç å¾—åˆ°ä¸¤ä¸ª 32 ç»´ token
        env_coding = self.envMLP(env_params)                 # (B, 32)
        ep_coding  = self.epMLP(electrolyzer_parameters)     # (B, 32)
        conc_coding  = self.concMLP(concentration)

        # (2) æ‰©å±•å¯å­¦ä¹  cls_token åˆ° batch
        B = env_coding.size(0)
        cls_token = self.cls_token.expand(B, -1, -1)         # (B, 1, 32)

        # (3) åœ¨â€œåºåˆ—ç»´â€æ‹¼æ¥æˆ 3 ä¸ª token
        cls_inputs = torch.cat([
            env_coding.unsqueeze(1),                         # (B, 1, 32)
            ep_coding.unsqueeze(1),
            conc_coding.unsqueeze(1),                      # (B, 1, 32)
            cls_token                                        # (B, 1, 32)
        ], dim=1)                                            # => (B, 3, 32)

        # (4) é€šè¿‡ Transformer ç¼–ç å™¨
        enc_out,Z_attn = self.Z_encoder(cls_inputs)                 # å¯èƒ½è¿”å› Tensor æˆ– (Tensor, attn) / (Tensor, list_of_attn)

        # (5) å– CLS ä½ç½®ï¼ˆæœ€åä¸€ä¸ª tokenï¼‰
        cls_token_feat = enc_out[:, -1:, :]               # (B, 1,32)
        # print("cls_token_feat.shape",cls_token_feat.shape)

        # æ‹¼æ¥åˆ°volt_outputå’Œimpe_outputåé¢
        tokens = torch.cat([ volt_output, impe_output,cls_token_feat], dim=1)  # (B,1+4+256,32)
        transformer_output, attn_weights = self.transformer(tokens)  # (B, 261, 32)
 
        # å‡è®¾åªç”¨æœ€åä¸€å±‚çš„transformerçš„æ³¨æ„åŠ›
        attn_map = attn_weights[-1]  # shape: (B, num_heads, tgt_len, src_len)
        cls_attn = attn_map[:, :, -1, :]  # (B, num_heads, 261)
        cls_attn_mean = cls_attn.mean(dim=1)    # (B, 261)  â† å¹³å‡æ‰€æœ‰å¤´
        # å–æœ€åä¸€ä¸ªè¾“å‡º (blank input çš„è¾“å‡º)
        last_output = transformer_output[:, -1, :] # Shape: (batch_size, feature_dim)

        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹4ä¸ªtoken)
        freq_attn = cls_attn_mean[:, num_time_points:-1]  # (B, T*F)
        freq_attn = freq_attn.view(B, T, F)  # (B, T, F)


        # =============== ç”¨äºé¢„æµ‹äº”ä¸ªåˆå§‹çŠ¶æ€çš„ç‰©ç†å‚æ•° ===============
        # =============== 5ï¸âƒ£ ä½¿ç”¨ä¸»åˆ†æ”¯embeddingé¢„æµ‹åˆå§‹ç‰©ç†å‚æ•° ===============

        # 1ï¸âƒ£ å–ç”µå‹ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„embedding (B,32)
        volt_first_feat = volt_output[:, 0, :].unsqueeze(1)  # (B,1,32)

        # 2ï¸âƒ£ å–é˜»æŠ—ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„embedding (B,64,32)
        impe_first_feat = impe_output[:, 0:num_freq_points, :]  # (B,64,32)

        # 3ï¸âƒ£ æ‹¼æ¥ cls_token_featã€ç”µå‹å’Œé˜»æŠ—
        param_tokens = torch.cat([ cls_token_feat,volt_first_feat, impe_first_feat], dim=1)  # (B,66,32)

        # 4ï¸âƒ£ é€å…¥å‚æ•° Transformer
        param_encoded,param_attn_weights = self.param_transformer(param_tokens)  # (B,66,32)

        # Transformer
        param_attn_map = param_attn_weights[-1]  # (B, nhead, N_tokens, N_tokens)
        param_attn_weights = param_attn_map[:, :, 0, :]  # (B, nhead, N_tokens)
        param_attn_mean = param_attn_weights.mean(dim=1)  # (B, N_tokens)
        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹1ä¸ªtoken)
        freq_attn_param = param_attn_mean[:, 1:-1]  # (B, 1*F)
        freq_attn_param = freq_attn_param.view(B, F)  # (B, 1, F)
        
        first_cls_output = param_encoded[:, 0,:]  # (B, D)
        # print("first_cls_output.shape:",first_cls_output.shape)

        # 8. é€šè¿‡ paramMLP è¾“å‡º5ä¸ªåˆå§‹çŠ¶æ€å‚æ•°åŸå§‹ç‰©ç†é‡ï¼ˆä¸åš clampï¼Œä¸åš sigmoidï¼‰
        param_output = self.paramMLP(first_cls_output) # (B, 5*32)ï¼Œå…è®¸å…¶è‡ªç”±è¡¨è¾¾å¤§å¹…ç¯å¢ƒå˜åŒ–
        # print("param_output.shape:",param_output.shape)
        param_output = param_output.view(B, 5, self.transformer_d_model)       # (B, 5, 32)
        # æ¯ä¸ªå‚æ•°ç‹¬ç«‹å‹ç¼©ä¸ºæ ‡é‡
        param_values = self.param_compress(param_output) # (B, 5, 1)
        param_values = param_values.squeeze(-1)          # (B, 5)


        # ========== 2ï¸âƒ£ ç‰©ç†å½±å“å› å­éƒ¨åˆ† ==========
        raw_physic_output = self.physicMLP(last_output)  # (B, 160)
        raw_physic_output = raw_physic_output.view(B, 5, self.transformer_d_model)

        # æ¯ä¸ªå½±å“å› å­ç‹¬ç«‹å‹ç¼©
        influence_values = self.influence_compress(raw_physic_output)  # (B, 5, 1)
        influence_values = influence_values.squeeze(-1)   
        # print("influence_values.shape:",influence_values.shape) 



        sigmoid = torch.nn.Sigmoid()
        
        # # ------------------- è¡Œä¸ºå½±å“å› å­ï¼ˆphysic_outputï¼‰ -------------------
        theta_min, theta_max = 0.001, 0.999
        phi_min, phi_max     = 0.001, 0.999

        theta_ca = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 0:1])
        phi_ca   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 1:2])
        theta_an = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 2:3])
        phi_an   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 3:4])
        psi      = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 4:5])  # åŒæ ·é™åˆ¶


        # ------------------- å›ºæœ‰ç‰©æ€§å‚æ•°ï¼ˆparam_outputï¼‰ -------------------
        sigma_min, sigma_max = 0.01, 2  # S/cm
        sigma_mem = sigma_min + (sigma_max - sigma_min) * sigmoid(param_values[:, 0:1])

        alpha_min, alpha_max = 0.01, 2
        alpha_ca = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 1:2])
        alpha_an = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 2:3])

        log_i0ca_min, log_i0ca_max = -9, 0
        log_i0an_min, log_i0an_max = -9, 0
        log_i0ca = log_i0ca_min + (log_i0ca_max - log_i0ca_min) * sigmoid(param_values[:, 3:4])
        log_i0an = log_i0an_min + (log_i0an_max - log_i0an_min) * sigmoid(param_values[:, 4:5])
        i_0ca = torch.pow(10, log_i0ca)
        i_0an = torch.pow(10, log_i0an)


        
        
        # ========== Cross Attention èåˆå½±å“å› å­å’Œç‰©æ€§å‚æ•° ==========
        # ========== 3ï¸âƒ£ Cross Attention äº¤äº’ ==========
        # Query: param_output, Key: raw_physic_output
        cross_output, cross_attn_weights = self.my_cross_attn_transformer(
            query=param_output,
            key_value=raw_physic_output
        )

        # 1ï¸âƒ£ æŠ•å½±
        # physic_embed = self.probMLP(param_output)  # (B, 5, 32)
        physic_embed_raw = self.probMLP(cross_output)  # (B, 5, 32)
        physic_embed = self.prob_norm(param_output + physic_embed_raw)  # âœ… æ®‹å·® + norm

        # 1ï¸âƒ£ è¾“å…¥ ion_attr_tensorï¼ˆé™æ€å±æ€§è¡¨ï¼‰ï¼Œå½¢çŠ¶ä¸º (6, 7)
        ion_attr_proj = self.ion_attr_embed(self.ion_attr_tensor)  # (6, 32)

        # 2ï¸âƒ£ æ·»åŠ  batch ç»´åº¦ï¼Œè¾“å…¥ Transformer
        ion_attr_proj = ion_attr_proj.unsqueeze(0)  # (1, 6, 32)

        # 3ï¸âƒ£ ç¼–ç ï¼Œè¿”å›ç¼–ç ç»“æœå’Œæ³¨æ„åŠ›æƒé‡
        ion_encoded, ion_attn_weights = self.ion_encoder(ion_attr_proj)  # (1, 6, 32), list of attn

        # 4ï¸âƒ£ å»æ‰ batch ç»´åº¦ï¼Œä½œä¸ºæ¯ä¸ªç¦»å­çš„åµŒå…¥å‘é‡
        ion_embeddings_raw = self.ion_postMLP(ion_encoded.squeeze(0))   # (6, 32)
        ion_embeddings = self.ion_norm(ion_encoded.squeeze(0) + ion_embeddings_raw)  # âœ… æ®‹å·® + norm

        # 5ï¸âƒ£ ç‚¹ç§¯ï¼šparam_output -> physic_embed (B, 5, 32)
        #         ion_embeddings -> (6, 32)
        raw_scores = torch.matmul(physic_embed, ion_embeddings.T)  # (B, 5, 6)

        # 6ï¸âƒ£ èšåˆ
        raw_prob_output = raw_scores.mean(dim=1)  # (B, 6)

        # 4ï¸âƒ£ Softmax
        prob_output = torch.softmax(raw_prob_output, dim=1)
        

    


        # ---------- ç”µå‹è®¡ç®—ï¼šåŸºäºé˜²NaNã€ç¨³å®šåçš„å‚æ•° ----------
        predicted_voltages = []
        for i in range(batch_size):
            pred_v, _, _, _, _ = calculate_predicted_voltage_plus(
                theta_ca[i], phi_ca[i], theta_an[i], phi_an[i], psi[i],
                temperature[i], flow[i], current[i],
                sigma_mem[i], alpha_ca[i], alpha_an[i], i_0ca[i], i_0an[i],
                electrolyzer_parameters= electrolyzer_parameters[i]  
            )
            predicted_voltages.append(pred_v)

        # ç»„åˆ batch çš„ç”µå‹ç»“æœ
        predicted_voltage = torch.stack(predicted_voltages).unsqueeze(1)  # shape: (B, 1)

        wuxing = [sigma_mem,alpha_ca,alpha_an,i_0ca,i_0an]
        influence=[psi,theta_ca,theta_an,phi_ca,phi_an]

        return prob_output, predicted_voltage, wuxing,cls_attn_mean ,raw_prob_output,influence,param_attn_mean,freq_attn,freq_attn_param




# å®šä¹‰å®Œæ•´æ¨¡å‹
class Model_three_system_1004(nn.Module):#ä¸‰ç³»ç»Ÿæ¶æ„
    def __init__(self, volt_input_dim, volt_mlp_hidden_dims,mlp_output_dims, volt_mlp_num_layers,
                 impe_input_dim,impe_mlp_hidden_dims,  impe_mlp_num_layers,
                 transformer_d_model, nhead, transformer_num_layers,param_transformer_num_layers,
                 physic_mlp_hidden_dims, physic_mlp_num_layers,
                 ion_attr_embed_hidden_dims,ion_attr_embed_num_layers,
                 ion_encoder_num_layers,
                 ion_post_hidden_dims,ion_post_num_layers,
                 probMLP_input_dims,probMLP_hidden_dims,probMLP_num_layers,
                 param_mlp_hidden_dims,param_mlp_num_layers,
                 freq_encoder_hidden_dims,freq_encoder_num_layers,
                 time_encoder_hidden_dims,time_encoder_num_layers,
                 cross_transformer_num_layers,
                 param_embed_hidden_dims,param_embed_num_layers,
                 physic_embed_hidden_dims,physic_embed_num_layers,
                 envMLP_input_dim, env_mlp_hidden_dims, env_mlp_num_layers,
                 ep_input_dim, ep_mlp_hidden_dims,  ep_mlp_num_layers,
                 Z_encoder_num_layers ,
                 conc_input_dim, conc_mlp_hidden_dims, conc_mlp_num_layers,
                 num_freq_points,num_time_points):
        
        
        super(Model_three_system_1004, self).__init__()
        self.voltMLP = AdjustableMLP(volt_input_dim, volt_mlp_hidden_dims, mlp_output_dims, volt_mlp_num_layers)
        self.impeMLP = AdjustableMLP(impe_input_dim, impe_mlp_hidden_dims, mlp_output_dims, impe_mlp_num_layers)
        self.transformer = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=transformer_num_layers)
        self.physicMLP = AdjustableMLP(transformer_d_model, physic_mlp_hidden_dims, 5*transformer_d_model, physic_mlp_num_layers)
        


        self.probMLP = AdjustableMLP(input_dim=probMLP_input_dims, hidden_dims=probMLP_hidden_dims, output_dim=mlp_output_dims, num_layers=probMLP_num_layers)  # å°† physic_output æŠ•å½±åˆ°åµŒå…¥ç©ºé—´

        
        # å®šä¹‰ç‰©ç†æ¨¡å‹éƒ¨åˆ†
        self.param_transformer =  MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=param_transformer_num_layers )
        self.paramMLP = AdjustableMLP(transformer_d_model,param_mlp_hidden_dims, 5*transformer_d_model, param_mlp_num_layers)
        self.norm = nn.LayerNorm(transformer_d_model)
        self.ion_attr_embed = AdjustableMLP(input_dim=len(ion_attr_list[0]), hidden_dims=ion_attr_embed_hidden_dims, output_dim=transformer_d_model, num_layers=ion_attr_embed_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨
        self.ion_encoder = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=ion_encoder_num_layers)
        self.ion_postMLP = AdjustableMLP(input_dim=transformer_d_model, hidden_dims=ion_post_hidden_dims, output_dim=transformer_d_model, num_layers=ion_post_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨

        self.mlp_output_dims = mlp_output_dims
        self.my_cross_attn_transformer = MyCrossAttnTransformer(d_model=transformer_d_model, nhead=nhead, num_layers=cross_transformer_num_layers)


        self.param_compress = AdjustableMLP(transformer_d_model, param_embed_hidden_dims, 1, param_embed_num_layers)
        self.influence_compress = AdjustableMLP(transformer_d_model, physic_embed_hidden_dims, 1, physic_embed_num_layers)

        self.transformer_d_model = transformer_d_model
        # å¯¹ physic_embed æ·»åŠ æ®‹å·®å’Œ norm
        self.prob_norm = nn.LayerNorm(transformer_d_model)

        # å¯¹ ion_embeddings æ·»åŠ æ®‹å·®å’Œ norm
        self.ion_norm = nn.LayerNorm(transformer_d_model)


        self.envMLP = AdjustableMLP(envMLP_input_dim, env_mlp_hidden_dims, mlp_output_dims, env_mlp_num_layers)
        self.epMLP = AdjustableMLP(ep_input_dim, ep_mlp_hidden_dims, mlp_output_dims, ep_mlp_num_layers)

        self.Z_encoder =  MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=Z_encoder_num_layers )

        self.concMLP = AdjustableMLP(conc_input_dim, conc_mlp_hidden_dims, 1, conc_mlp_num_layers)

        # 1ï¸âƒ£ å®šä¹‰ä¸€ä¸ªå¯å­¦ä¹ çš„ cls_token (åˆå§‹å€¼å¯ç”¨æ­£æ€åˆ†å¸ƒ)
        self.cls_token = nn.Parameter(torch.zeros(1, transformer_d_model))
        nn.init.trunc_normal_(self.cls_token, std=0.02)




        # çœŸå®é¢‘ç‡åˆ—è¡¨ï¼ˆ64ä¸ªç‚¹ï¼‰
        if num_freq_points==64:
            freq_values_hz = torch.tensor([
                19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
                794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
                79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
                7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
                0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
                0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259, 0.01
            ], dtype=torch.float32)
        elif num_freq_points==63:
            freq_values_hz = torch.tensor([
            19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
            794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
            79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
            7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
            0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
            0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259
            ], dtype=torch.float32)
        else:
            print("num_freq_points:",num_freq_points)
        # å½’ä¸€åŒ–åˆ° å¯¹æ•°åæ ‡ç³»[0, 1]
        freq_values_log = torch.log10(freq_values_hz)  # log10å˜æ¢
        freq_values_norm = (freq_values_log - freq_values_log.min()) / (freq_values_log.max() - freq_values_log.min())
        self.register_buffer("freq_values_tensor", freq_values_norm)     

        # å®šä¹‰ frequency encoder MLP,å°†é¢‘ç‡æ•°æ®å˜ä¸ºå’ŒMLPåŒæ ·çš„å¤§å°ï¼Œè¿™æ ·å¯ä»¥å¯¹é˜»æŠ—æ•°æ®è¿›è¡Œé¢‘ç‡ç¼–ç 
        self.freq_encoder = AdjustableMLP(1, freq_encoder_hidden_dims, mlp_output_dims, freq_encoder_num_layers)#(64ï¼Œ1)->(64,32)
        # æ—¶é—´ç¼–ç å™¨
        self.time_embedding = AdjustableMLP(1, time_encoder_hidden_dims, mlp_output_dims, time_encoder_num_layers)  # nn.Embedding é»˜è®¤åªèƒ½å¤„ç†ç¦»æ•£ç´¢å¼•ï¼ˆå¦‚ 0ã€1ã€2ã€3ï¼‰ï¼Œå®ƒå¹¶ä¸ä¼šæ˜¾å¼æ•æ‰åˆ°è¿™äº›æ—¶é—´ç‚¹åœ¨ç‰©ç†ä¸Šæ˜¯â€œè¿ç»­ä¸”é—´éš”ä¸º2å°æ—¶â€çš„ã€‚é‚£ä¸ºä»€ä¹ˆè¿˜ä¼šç”¨ nn.Embedding è¡¨ç¤ºæ—¶é—´ï¼Ÿè¿™æ˜¯å› ä¸ºåœ¨å¾ˆå¤šåœºæ™¯ä¸‹ï¼ˆç‰¹åˆ«æ˜¯åœ¨ Transformer ç­‰ç»“æ„ä¸­ï¼‰ï¼Œæ—¶é—´ç‚¹åªä½œä¸ºä¸€ä¸ªâ€œä½ç½®æ ‡è¯†ç¬¦â€å­˜åœ¨ï¼Œä¾‹å¦‚ç¬¬å‡ ä¸ªæ—¶é—´æ­¥ï¼Œå®ƒæœ¬èº«çš„ç»å¯¹å«ä¹‰å¹¶ä¸é‚£ä¹ˆé‡è¦ã€‚




        
        ion_attr_tensor = torch.tensor(ion_attr_list_plus, dtype=torch.float32)
        self.register_buffer("ion_attr_tensor", ion_attr_tensor)


    def forward(self, volt_data, impe_data, env_params,electrolyzer_parameters,concentration):
        B, T, F, C = impe_data.shape  # (B,4,64,2)
        B2,T2,C2 = volt_data.shape
         # ==== æ·»åŠ æ–­è¨€æ£€æŸ¥ ====
        assert B == B2, f"Batch size mismatch: impe_data B={B}, volt_data B={B2}"
        assert T == T2, f"Time steps mismatch: impe_data T={T}, volt_data T={T2}"

        batch_size = B
        num_time_points = T
        num_freq_points = F
        device = volt_data.device

        # ========== ç”µå‹ç‰¹å¾ ==========
        volt_output = self.voltMLP(volt_data)  # (B,4,1)->(B,4,32)

        # ======== æ—¶é—´ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        time_base_input = torch.arange(T, device=device).float().view(T, 1)   # (T,1)
        time_encoded_base = self.time_embedding(time_base_input)              # (T,32)

        # æ‰©å±•åˆ° batch ç»´åº¦ (B,T,32)
        time_encoded = time_encoded_base.unsqueeze(0).expand(B, T, -1)

        # ç”µå‹ç‰¹å¾åŠ æ—¶é—´ç¼–ç 
        volt_output = volt_output + time_encoded

        # ========== é˜»æŠ—ç‰¹å¾ ==========
        impe_output = self.impeMLP(impe_data.view(B, T * F, -1))  # (B,256,32)
        impe_output = impe_output.view(B, T, F, -1)               # (B,4,64,32)

        # ======== é¢‘ç‡ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        freq_base_input = self.freq_values_tensor.view(F, 1)        # (64,1)
        freq_encoded_base = self.freq_encoder(freq_base_input)      # (64,32)
        freq_encoded = freq_encoded_base.unsqueeze(0).unsqueeze(0).expand(B, T, F, -1)

        # ======== æ—¶é—´ç¼–ç æ‰©å±•åˆ°é˜»æŠ—ç‰¹å¾ ========
        time_encoded_impe = time_encoded.unsqueeze(2).expand(B, T, F, -1)    # (B,4,64,32)

        # åŠ å…¥æ—¶é—´ç¼–ç +é¢‘ç‡ç¼–ç 
        impe_output = impe_output + time_encoded_impe + freq_encoded

        # reshapeå›åŸå§‹å½¢çŠ¶
        impe_output = impe_output.view(B, T * F, -1)   # (B,256,32)


        # æå–ç¯å¢ƒå‚æ•°
        temperature = env_params[:, 0].unsqueeze(1)      # (B,1)
        flow = env_params[:, 1].unsqueeze(1)             # (B,1)
        current = env_params[:, 2].unsqueeze(1)          # (B,1)


        # (1) ç¼–ç å¾—åˆ°ä¸¤ä¸ª 32 ç»´ token
        env_coding = self.envMLP(env_params)                 # (B, 32)
        ep_coding  = self.epMLP(electrolyzer_parameters)     # (B, 32)

        # (2) æ‰©å±•å¯å­¦ä¹  cls_token åˆ° batch
        B = env_coding.size(0)
        cls_token = self.cls_token.expand(B, -1, -1)         # (B, 1, 32)

        # (3) åœ¨â€œåºåˆ—ç»´â€æ‹¼æ¥æˆ 3 ä¸ª token
        cls_inputs = torch.cat([
            env_coding.unsqueeze(1),                         # (B, 1, 32)
            ep_coding.unsqueeze(1),
            cls_token                                        # (B, 1, 32)
        ], dim=1)                                            # => (B, 3, 32)

        # (4) é€šè¿‡ Transformer ç¼–ç å™¨
        enc_out,Z_attn = self.Z_encoder(cls_inputs)                 # å¯èƒ½è¿”å› Tensor æˆ– (Tensor, attn) / (Tensor, list_of_attn)

        # (5) å– CLS ä½ç½®ï¼ˆæœ€åä¸€ä¸ª tokenï¼‰
        cls_token_feat = enc_out[:, -1:, :]               # (B, 1,32)
        # print("cls_token_feat.shape",cls_token_feat.shape)

        # æ‹¼æ¥åˆ°volt_outputå’Œimpe_outputåé¢
        tokens = torch.cat([ volt_output, impe_output,cls_token_feat], dim=1)  # (B,1+4+256,32)
        transformer_output, attn_weights = self.transformer(tokens)  # (B, 261, 32)
 
        # å‡è®¾åªç”¨æœ€åä¸€å±‚çš„transformerçš„æ³¨æ„åŠ›
        attn_map = attn_weights[-1]  # shape: (B, num_heads, tgt_len, src_len)
        cls_attn = attn_map[:, :, -1, :]  # (B, num_heads, 261)
        cls_attn_mean = cls_attn.mean(dim=1)    # (B, 261)  â† å¹³å‡æ‰€æœ‰å¤´
        # å–æœ€åä¸€ä¸ªè¾“å‡º (blank input çš„è¾“å‡º)
        last_output = transformer_output[:, -1, :] # Shape: (batch_size, feature_dim)

        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹4ä¸ªtoken)
        freq_attn = cls_attn_mean[:, num_time_points:-1]  # (B, T*F)
        freq_attn = freq_attn.view(B, T, F)  # (B, T, F)


        # =============== ç”¨äºé¢„æµ‹äº”ä¸ªåˆå§‹çŠ¶æ€çš„ç‰©ç†å‚æ•° ===============
        # =============== 5ï¸âƒ£ ä½¿ç”¨ä¸»åˆ†æ”¯embeddingé¢„æµ‹åˆå§‹ç‰©ç†å‚æ•° ===============

        # 1ï¸âƒ£ å–ç”µå‹ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„embedding (B,32)
        volt_first_feat = volt_output[:, 0, :].unsqueeze(1)  # (B,1,32)

        # 2ï¸âƒ£ å–é˜»æŠ—ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„embedding (B,64,32)
        impe_first_feat = impe_output[:, 0:num_freq_points, :]  # (B,64,32)

        # 3ï¸âƒ£ æ‹¼æ¥ cls_token_featã€ç”µå‹å’Œé˜»æŠ—
        param_tokens = torch.cat([ cls_token_feat,volt_first_feat, impe_first_feat], dim=1)  # (B,66,32)

        # 4ï¸âƒ£ é€å…¥å‚æ•° Transformer
        param_encoded,param_attn_weights = self.param_transformer(param_tokens)  # (B,66,32)

        # Transformer
        param_attn_map = param_attn_weights[-1]  # (B, nhead, N_tokens, N_tokens)
        param_attn_weights = param_attn_map[:, :, 0, :]  # (B, nhead, N_tokens)
        param_attn_mean = param_attn_weights.mean(dim=1)  # (B, N_tokens)
        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹1ä¸ªtoken)
        freq_attn_param = param_attn_mean[:, 2:]  # (B, 1*F)
        freq_attn_param = freq_attn_param.view(B, F)  # (B, 1, F)
        
        first_cls_output = param_encoded[:, 0,:]  # (B, D)
        # print("first_cls_output.shape:",first_cls_output.shape)

        # 8. é€šè¿‡ paramMLP è¾“å‡º5ä¸ªåˆå§‹çŠ¶æ€å‚æ•°åŸå§‹ç‰©ç†é‡ï¼ˆä¸åš clampï¼Œä¸åš sigmoidï¼‰
        param_output = self.paramMLP(first_cls_output) # (B, 5*32)ï¼Œå…è®¸å…¶è‡ªç”±è¡¨è¾¾å¤§å¹…ç¯å¢ƒå˜åŒ–
        # print("param_output.shape:",param_output.shape)
        param_output = param_output.view(B, 5, self.transformer_d_model)       # (B, 5, 32)
        # æ¯ä¸ªå‚æ•°ç‹¬ç«‹å‹ç¼©ä¸ºæ ‡é‡
        param_values = self.param_compress(param_output) # (B, 5, 1)
        param_values = param_values.squeeze(-1)          # (B, 5)


        # ========== 2ï¸âƒ£ ç‰©ç†å½±å“å› å­éƒ¨åˆ† ==========
        raw_physic_output = self.physicMLP(last_output)  # (B, 160)
        raw_physic_output = raw_physic_output.view(B, 5, self.transformer_d_model)




        # æ¯ä¸ªå½±å“å› å­ç‹¬ç«‹å‹ç¼©
        influence_values = self.influence_compress(raw_physic_output)  # (B, 5, 1)
        influence_values = influence_values.squeeze(-1)   
        # print("influence_values.shape:",influence_values.shape) 



        sigmoid = torch.nn.Sigmoid()
        
        # # ------------------- è¡Œä¸ºå½±å“å› å­ï¼ˆphysic_outputï¼‰ -------------------
        theta_min, theta_max = 0.001, 0.999
        phi_min, phi_max     = 0.001, 0.999

        theta_ca = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 0:1])
        phi_ca   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 1:2])
        theta_an = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 2:3])
        phi_an   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 3:4])
        psi      = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 4:5])  # åŒæ ·é™åˆ¶


        # ------------------- å›ºæœ‰ç‰©æ€§å‚æ•°ï¼ˆparam_outputï¼‰ -------------------
        sigma_min, sigma_max = 0.01, 2  # S/cm
        sigma_mem = sigma_min + (sigma_max - sigma_min) * sigmoid(param_values[:, 0:1])

        alpha_min, alpha_max = 0.01, 2
        alpha_ca = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 1:2])
        alpha_an = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 2:3])

        log_i0ca_min, log_i0ca_max = -9, 0
        log_i0an_min, log_i0an_max = -9, 0
        log_i0ca = log_i0ca_min + (log_i0ca_max - log_i0ca_min) * sigmoid(param_values[:, 3:4])
        log_i0an = log_i0an_min + (log_i0an_max - log_i0an_min) * sigmoid(param_values[:, 4:5])
        i_0ca = torch.pow(10, log_i0ca)
        i_0an = torch.pow(10, log_i0an)


        
        
        # ========== Cross Attention èåˆå½±å“å› å­å’Œç‰©æ€§å‚æ•° ==========
        # ========== 3ï¸âƒ£ Cross Attention äº¤äº’ ==========
        # Query: param_output, Key: raw_physic_output
        cross_output, cross_attn_weights = self.my_cross_attn_transformer(
            query=param_output,
            key_value=raw_physic_output
        )

        #é¢„æµ‹æµ“åº¦
        # 1) å½¢çŠ¶è½¬æ¢ -> (B, 5*self.transformer_d_model)
        cross_output_pred = cross_output.flatten(start_dim=1)  # #(B, 5, self.transformer_d_model)->(B,5*self.transformer_d_model)

        # 2) é¢„æµ‹æµ“åº¦
        conc_pred = self.concMLP(cross_output_pred)

        # 1ï¸âƒ£ æŠ•å½±
        # physic_embed = self.probMLP(param_output)  # (B, 5, 32)
        physic_embed_raw = self.probMLP(cross_output)  # (B, 5, 32)
        physic_embed = self.prob_norm(param_output + physic_embed_raw)  # âœ… æ®‹å·® + norm

        # 1ï¸âƒ£ è¾“å…¥ ion_attr_tensorï¼ˆé™æ€å±æ€§è¡¨ï¼‰ï¼Œå½¢çŠ¶ä¸º (6, 7)
        ion_attr_proj = self.ion_attr_embed(self.ion_attr_tensor)  # (6, 32)

        # 2ï¸âƒ£ æ·»åŠ  batch ç»´åº¦ï¼Œè¾“å…¥ Transformer
        ion_attr_proj = ion_attr_proj.unsqueeze(0)  # (1, 6, 32)

        # 3ï¸âƒ£ ç¼–ç ï¼Œè¿”å›ç¼–ç ç»“æœå’Œæ³¨æ„åŠ›æƒé‡
        ion_encoded, ion_attn_weights = self.ion_encoder(ion_attr_proj)  # (1, 6, 32), list of attn

        # 4ï¸âƒ£ å»æ‰ batch ç»´åº¦ï¼Œä½œä¸ºæ¯ä¸ªç¦»å­çš„åµŒå…¥å‘é‡
        ion_embeddings_raw = self.ion_postMLP(ion_encoded.squeeze(0))   # (6, 32)
        ion_embeddings = self.ion_norm(ion_encoded.squeeze(0) + ion_embeddings_raw)  # âœ… æ®‹å·® + norm

        # 5ï¸âƒ£ ç‚¹ç§¯ï¼šparam_output -> physic_embed (B, 5, 32)
        #         ion_embeddings -> (6, 32)
        raw_scores = torch.matmul(physic_embed, ion_embeddings.T)  # (B, 5, 6)

        # 6ï¸âƒ£ èšåˆ
        raw_prob_output = raw_scores.mean(dim=1)  # (B, 6)

        # 4ï¸âƒ£ Softmax
        prob_output = torch.softmax(raw_prob_output, dim=1)
        

    


        # ---------- ç”µå‹è®¡ç®—ï¼šåŸºäºé˜²NaNã€ç¨³å®šåçš„å‚æ•° ----------
        predicted_voltages = []
        for i in range(batch_size):
            pred_v, _, _, _, _ = calculate_predicted_voltage_plus(
                theta_ca[i], phi_ca[i], theta_an[i], phi_an[i], psi[i],
                temperature[i], flow[i], current[i],
                sigma_mem[i], alpha_ca[i], alpha_an[i], i_0ca[i], i_0an[i],
                electrolyzer_parameters= electrolyzer_parameters[i]  
            )
            predicted_voltages.append(pred_v)

        # ç»„åˆ batch çš„ç”µå‹ç»“æœ
        predicted_voltage = torch.stack(predicted_voltages).unsqueeze(1)  # shape: (B, 1)

        wuxing = [sigma_mem,alpha_ca,alpha_an,i_0ca,i_0an]
        influence=[psi,theta_ca,theta_an,phi_ca,phi_an]

        return prob_output, predicted_voltage,conc_pred, wuxing,cls_attn_mean ,raw_prob_output,influence,param_attn_mean,freq_attn,freq_attn_param



# å®šä¹‰å®Œæ•´æ¨¡å‹
class Model_three_system_1117(nn.Module):#ä¸‰ç³»ç»Ÿæ¶æ„
    def __init__(self, volt_input_dim, volt_mlp_hidden_dims,mlp_output_dims, volt_mlp_num_layers,
                 impe_input_dim,impe_mlp_hidden_dims,  impe_mlp_num_layers,
                 transformer_d_model, nhead, transformer_num_layers,param_transformer_num_layers,
                 physic_mlp_hidden_dims, physic_mlp_num_layers,
                 ion_attr_embed_hidden_dims,ion_attr_embed_num_layers,
                 ion_encoder_num_layers,
                 ion_post_hidden_dims,ion_post_num_layers,
                 probMLP_input_dims,probMLP_hidden_dims,probMLP_num_layers,
                 param_mlp_hidden_dims,param_mlp_num_layers,
                 freq_encoder_hidden_dims,freq_encoder_num_layers,
                 time_encoder_hidden_dims,time_encoder_num_layers,
                 cross_transformer_num_layers,
                 param_embed_hidden_dims,param_embed_num_layers,
                 physic_embed_hidden_dims,physic_embed_num_layers,
                 envMLP_input_dim, env_mlp_hidden_dims, env_mlp_num_layers,
                 ep_input_dim, ep_mlp_hidden_dims,  ep_mlp_num_layers,
                 Z_encoder_num_layers ,
                 conc_input_dim, conc_mlp_hidden_dims, conc_mlp_num_layers,
                 num_freq_points,num_time_points):
        
        
        super(Model_three_system_1117, self).__init__()
        self.voltMLP = AdjustableMLP(volt_input_dim, volt_mlp_hidden_dims, mlp_output_dims, volt_mlp_num_layers)
        self.impeMLP = AdjustableMLP(impe_input_dim, impe_mlp_hidden_dims, mlp_output_dims, impe_mlp_num_layers)
        self.transformer = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=transformer_num_layers)
        self.physicMLP = AdjustableMLP(transformer_d_model, physic_mlp_hidden_dims, 5*transformer_d_model, physic_mlp_num_layers)
        


        self.probMLP = AdjustableMLP(input_dim=probMLP_input_dims, hidden_dims=probMLP_hidden_dims, output_dim=mlp_output_dims, num_layers=probMLP_num_layers)  # å°† physic_output æŠ•å½±åˆ°åµŒå…¥ç©ºé—´

        
        # å®šä¹‰ç‰©ç†æ¨¡å‹éƒ¨åˆ†
        self.param_transformer =  MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=param_transformer_num_layers )
        self.paramMLP = AdjustableMLP(transformer_d_model,param_mlp_hidden_dims, 5*transformer_d_model, param_mlp_num_layers)
        self.norm = nn.LayerNorm(transformer_d_model)
        self.ion_attr_embed = AdjustableMLP(input_dim=len(ion_attr_list_plus_1117[0]), hidden_dims=ion_attr_embed_hidden_dims, output_dim=transformer_d_model, num_layers=ion_attr_embed_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨
        self.ion_encoder = MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=ion_encoder_num_layers)
        self.ion_postMLP = AdjustableMLP(input_dim=transformer_d_model, hidden_dims=ion_post_hidden_dims, output_dim=transformer_d_model, num_layers=ion_post_num_layers)  # å¯å­¦ä¹ ç¦»å­åµŒå…¥å™¨

        self.mlp_output_dims = mlp_output_dims
        self.my_cross_attn_transformer = MyCrossAttnTransformer(d_model=transformer_d_model, nhead=nhead, num_layers=cross_transformer_num_layers)


        self.param_compress = AdjustableMLP(transformer_d_model, param_embed_hidden_dims, 1, param_embed_num_layers)
        self.influence_compress = AdjustableMLP(transformer_d_model, physic_embed_hidden_dims, 1, physic_embed_num_layers)

        self.transformer_d_model = transformer_d_model
        # å¯¹ physic_embed æ·»åŠ æ®‹å·®å’Œ norm
        self.prob_norm = nn.LayerNorm(transformer_d_model)

        # å¯¹ ion_embeddings æ·»åŠ æ®‹å·®å’Œ norm
        self.ion_norm = nn.LayerNorm(transformer_d_model)


        self.envMLP = AdjustableMLP(envMLP_input_dim, env_mlp_hidden_dims, mlp_output_dims, env_mlp_num_layers)
        self.epMLP = AdjustableMLP(ep_input_dim, ep_mlp_hidden_dims, mlp_output_dims, ep_mlp_num_layers)

        self.Z_encoder =  MyTransformerWithAttn(d_model=transformer_d_model, nhead=nhead, num_layers=Z_encoder_num_layers )

        self.conc_head = nn.Linear(5 * transformer_d_model, 1)

        # 1ï¸âƒ£ å®šä¹‰ä¸€ä¸ªå¯å­¦ä¹ çš„ cls_token (åˆå§‹å€¼å¯ç”¨æ­£æ€åˆ†å¸ƒ)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, transformer_d_model))
        nn.init.trunc_normal_(self.cls_token, std=0.02)




        # çœŸå®é¢‘ç‡åˆ—è¡¨ï¼ˆ64ä¸ªç‚¹ï¼‰
        if num_freq_points==64:
            freq_values_hz = torch.tensor([
                19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
                794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
                79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
                7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
                0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
                0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259, 0.01
            ], dtype=torch.float32)
        elif num_freq_points==63:
            freq_values_hz = torch.tensor([
            19950, 15850, 12590, 10000, 7943, 6310, 5010, 3980, 3160, 2510, 1990, 1590, 1260, 1000,
            794.3, 631.0, 501.2, 398.1, 316.2, 251.2, 199.5, 158.5, 125.9, 100.0,
            79.43, 63.10, 50.12, 39.81, 31.62, 25.12, 19.95, 15.85, 12.59, 10.0,
            7.94, 6.31, 5.01, 3.98, 3.16, 2.51, 1.99, 1.59, 1.26, 1.0,
            0.7943, 0.6310, 0.5012, 0.3981, 0.3162, 0.2512, 0.1995, 0.1585, 0.1259, 0.1,
            0.07943, 0.06310, 0.05012, 0.03981, 0.03162, 0.02512, 0.01995, 0.01585, 0.01259
            ], dtype=torch.float32)
        else:
            print("num_freq_points:",num_freq_points)
        # å½’ä¸€åŒ–åˆ° å¯¹æ•°åæ ‡ç³»[0, 1]
        freq_values_log = torch.log10(freq_values_hz)  # log10å˜æ¢
        freq_values_norm = (freq_values_log - freq_values_log.min()) / (freq_values_log.max() - freq_values_log.min())
        self.register_buffer("freq_values_tensor", freq_values_norm)     

        # å®šä¹‰ frequency encoder MLP,å°†é¢‘ç‡æ•°æ®å˜ä¸ºå’ŒMLPåŒæ ·çš„å¤§å°ï¼Œè¿™æ ·å¯ä»¥å¯¹é˜»æŠ—æ•°æ®è¿›è¡Œé¢‘ç‡ç¼–ç 
        self.freq_encoder = AdjustableMLP(1, freq_encoder_hidden_dims, mlp_output_dims, freq_encoder_num_layers)#(64ï¼Œ1)->(64,32)
        # æ—¶é—´ç¼–ç å™¨
        self.time_embedding = AdjustableMLP(1, time_encoder_hidden_dims, mlp_output_dims, time_encoder_num_layers)  # nn.Embedding é»˜è®¤åªèƒ½å¤„ç†ç¦»æ•£ç´¢å¼•ï¼ˆå¦‚ 0ã€1ã€2ã€3ï¼‰ï¼Œå®ƒå¹¶ä¸ä¼šæ˜¾å¼æ•æ‰åˆ°è¿™äº›æ—¶é—´ç‚¹åœ¨ç‰©ç†ä¸Šæ˜¯â€œè¿ç»­ä¸”é—´éš”ä¸º2å°æ—¶â€çš„ã€‚é‚£ä¸ºä»€ä¹ˆè¿˜ä¼šç”¨ nn.Embedding è¡¨ç¤ºæ—¶é—´ï¼Ÿè¿™æ˜¯å› ä¸ºåœ¨å¾ˆå¤šåœºæ™¯ä¸‹ï¼ˆç‰¹åˆ«æ˜¯åœ¨ Transformer ç­‰ç»“æ„ä¸­ï¼‰ï¼Œæ—¶é—´ç‚¹åªä½œä¸ºä¸€ä¸ªâ€œä½ç½®æ ‡è¯†ç¬¦â€å­˜åœ¨ï¼Œä¾‹å¦‚ç¬¬å‡ ä¸ªæ—¶é—´æ­¥ï¼Œå®ƒæœ¬èº«çš„ç»å¯¹å«ä¹‰å¹¶ä¸é‚£ä¹ˆé‡è¦ã€‚
        
        ion_attr_tensor = torch.tensor(ion_attr_list_plus_1117, dtype=torch.float32)
        self.register_buffer("ion_attr_tensor", ion_attr_tensor)
        
        self.ion_attr_dim = 7
        self.rule_dim = ion_attr_tensor.size(1) - self.ion_attr_dim  # ä¸€èˆ¬æ˜¯ 8
        self.num_ions = ion_attr_tensor.size(0)

        # æå–ç±»åˆ«çº§ rule åŸå‹ r_kï¼ˆåªç”¨å 8 ç»´ï¼‰
        self.register_buffer("ion_rule_proto", ion_attr_tensor[:, self.ion_attr_dim:])  # (num_ions, rule_dim)
        
        # ========= 4. â˜… æ–°å¢ï¼šrule_head + å±‚çº§å¤šä»»åŠ¡ heads =========
        # ä» last_output (B, transformer_d_model) ->  rule-space (B, rule_dim)
        self.rule_head = nn.Linear(transformer_d_model, self.rule_dim)

        # é«˜/ä½å½±å“ç»„ï¼ˆ2ç±»ï¼‰ï¼Œä»¥åŠé«˜ç»„ 3 ç±»ã€ä½ç»„ 3 ç±»
        self.head_group = nn.Linear(transformer_d_model, 2)   # 0: low-impact, 1: high-impact
        self.head_high3 = nn.Linear(transformer_d_model, 3)   # [Ca, Na, Ni]
        self.head_low3  = nn.Linear(transformer_d_model, 3)   # [Cu, Fe, Cr]
        # æ–°å¢ï¼šæ¨ç†æ—¶ç”¨çš„ç¼“å­˜
        self.ion_embeddings_eval = None

    def forward(self, volt_data, impe_data, env_params,electrolyzer_parameters,concentration):
        B, T, F, C = impe_data.shape  # (B,4,64,2)
        B2,T2,C2 = volt_data.shape
         # ==== æ·»åŠ æ–­è¨€æ£€æŸ¥ ====
        assert B == B2, f"Batch size mismatch: impe_data B={B}, volt_data B={B2}"
        assert T == T2, f"Time steps mismatch: impe_data T={T}, volt_data T={T2}"

        batch_size = B
        num_time_points = T
        num_freq_points = F
        device = volt_data.device

        # ========== ç”µå‹ç‰¹å¾ ==========
        volt_output = self.voltMLP(volt_data)  # (B,4,1)->(B,4,32)

        # ======== æ—¶é—´ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        time_base_input = torch.arange(T, device=device).float().view(T, 1)   # (T,1)
        time_encoded_base = self.time_embedding(time_base_input)              # (T,32)

        # æ‰©å±•åˆ° batch ç»´åº¦ (B,T,32)
        time_encoded = time_encoded_base.unsqueeze(0).expand(B, T, -1)

        # ç”µå‹ç‰¹å¾åŠ æ—¶é—´ç¼–ç 
        volt_output = volt_output + time_encoded

        # ========== é˜»æŠ—ç‰¹å¾ ==========
        impe_output = self.impeMLP(impe_data.view(B, T * F, -1))  # (B,256,32)
        impe_output = impe_output.view(B, T, F, -1)               # (B,4,64,32)

        # ======== é¢‘ç‡ç¼–ç (ä¸€æ¬¡æ€§è®¡ç®—) ========
        freq_base_input = self.freq_values_tensor.view(F, 1)        # (64,1)
        freq_encoded_base = self.freq_encoder(freq_base_input)      # (64,32)
        freq_encoded = freq_encoded_base.unsqueeze(0).unsqueeze(0).expand(B, T, F, -1)

        # ======== æ—¶é—´ç¼–ç æ‰©å±•åˆ°é˜»æŠ—ç‰¹å¾ ========
        time_encoded_impe = time_encoded.unsqueeze(2).expand(B, T, F, -1)    # (B,4,64,32)

        # åŠ å…¥æ—¶é—´ç¼–ç +é¢‘ç‡ç¼–ç 
        impe_output = impe_output + time_encoded_impe + freq_encoded

        # reshapeå›åŸå§‹å½¢çŠ¶
        impe_output = impe_output.view(B, T * F, -1)   # (B,256,32)


        # æå–ç¯å¢ƒå‚æ•°
        temperature = env_params[:, 0].unsqueeze(1)      # (B,1)
        flow = env_params[:, 1].unsqueeze(1)             # (B,1)
        current = env_params[:, 2].unsqueeze(1)          # (B,1)


        # (1) ç¼–ç å¾—åˆ°ä¸¤ä¸ª 32 ç»´ token
        env_coding = self.envMLP(env_params)                 # (B, 32)
        ep_coding  = self.epMLP(electrolyzer_parameters)     # (B, 32)

        # (2) æ‰©å±•å¯å­¦ä¹  cls_token åˆ° batch
        B = env_coding.size(0)
        cls_token = self.cls_token.expand(B, -1, -1)         # (B, 1, 32)

        # (3) åœ¨â€œåºåˆ—ç»´â€æ‹¼æ¥æˆ 3 ä¸ª token
        cls_inputs = torch.cat([
            env_coding.unsqueeze(1),                         # (B, 1, 32)
            ep_coding.unsqueeze(1),
            cls_token                                        # (B, 1, 32)
        ], dim=1)                                            # => (B, 3, 32)

        # (4) é€šè¿‡ Transformer ç¼–ç å™¨
        enc_out,Z_attn = self.Z_encoder(cls_inputs)                 # å¯èƒ½è¿”å› Tensor æˆ– (Tensor, attn) / (Tensor, list_of_attn)

        # (5) å– CLS ä½ç½®ï¼ˆæœ€åä¸€ä¸ª tokenï¼‰
        cls_token_feat = enc_out[:, -1:, :]               # (B, 1,32)
        # print("cls_token_feat.shape",cls_token_feat.shape)

        # æ‹¼æ¥åˆ°volt_outputå’Œimpe_outputåé¢
        tokens = torch.cat([ volt_output, impe_output,cls_token_feat], dim=1)  # (B,1+4+256,32)
        transformer_output, attn_weights = self.transformer(tokens)  # (B, 261, 32)
 
        # å‡è®¾åªç”¨æœ€åä¸€å±‚çš„transformerçš„æ³¨æ„åŠ›
        attn_map = attn_weights[-1]  # shape: (B, num_heads, tgt_len, src_len)
        cls_attn = attn_map[:, :, -1, :]  # (B, num_heads, 261)
        cls_attn_mean = cls_attn.mean(dim=1)    # (B, 261)  â† å¹³å‡æ‰€æœ‰å¤´
        # å–æœ€åä¸€ä¸ªè¾“å‡º (blank input çš„è¾“å‡º)
        last_output = transformer_output[:, -1, :] # Shape: (batch_size, feature_dim)

        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹4ä¸ªtoken)
        freq_attn = cls_attn_mean[:, num_time_points:-1]  # (B, T*F)
        freq_attn = freq_attn.view(B, T, F)  # (B, T, F)


        # =============== ç”¨äºé¢„æµ‹äº”ä¸ªåˆå§‹çŠ¶æ€çš„ç‰©ç†å‚æ•° ===============
        # =============== 5ï¸âƒ£ ä½¿ç”¨ä¸»åˆ†æ”¯embeddingé¢„æµ‹åˆå§‹ç‰©ç†å‚æ•° ===============

        # 1ï¸âƒ£ å–ç”µå‹ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„embedding (B,32)
        volt_first_feat = volt_output[:, 0, :].unsqueeze(1)  # (B,1,32)

        # 2ï¸âƒ£ å–é˜»æŠ—ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹çš„embedding (B,64,32)
        impe_first_feat = impe_output[:, 0:num_freq_points, :]  # (B,64,32)

        # 3ï¸âƒ£ æ‹¼æ¥ cls_token_featã€ç”µå‹å’Œé˜»æŠ—
        param_tokens = torch.cat([ cls_token_feat,volt_first_feat, impe_first_feat], dim=1)  # (B,66,32)

        # 4ï¸âƒ£ é€å…¥å‚æ•° Transformer
        param_encoded,param_attn_weights = self.param_transformer(param_tokens)  # (B,66,32)

        # Transformer
        param_attn_map = param_attn_weights[-1]  # (B, nhead, N_tokens, N_tokens)
        param_attn_weights = param_attn_map[:, :, 0, :]  # (B, nhead, N_tokens)
        param_attn_mean = param_attn_weights.mean(dim=1)  # (B, N_tokens)
        
        # æå–é˜»æŠ—é¢‘ç‡éƒ¨åˆ†æ³¨æ„åŠ› (å¿½ç•¥ç”µå‹1ä¸ªtoken)
        freq_attn_param = param_attn_mean[:, 2:]  # (B, 1*F)
        freq_attn_param = freq_attn_param.view(B, F)  # (B, 1, F)
        
        first_cls_output = param_encoded[:, 0,:]  # (B, D)
        # print("first_cls_output.shape:",first_cls_output.shape)

        # 8. é€šè¿‡ paramMLP è¾“å‡º5ä¸ªåˆå§‹çŠ¶æ€å‚æ•°åŸå§‹ç‰©ç†é‡ï¼ˆä¸åš clampï¼Œä¸åš sigmoidï¼‰
        param_output = self.paramMLP(first_cls_output) # (B, 5*32)ï¼Œå…è®¸å…¶è‡ªç”±è¡¨è¾¾å¤§å¹…ç¯å¢ƒå˜åŒ–
        # print("param_output.shape:",param_output.shape)
        param_output = param_output.view(B, 5, self.transformer_d_model)       # (B, 5, 32)
        # æ¯ä¸ªå‚æ•°ç‹¬ç«‹å‹ç¼©ä¸ºæ ‡é‡
        param_values = self.param_compress(param_output) # (B, 5, 1)
        param_values = param_values.squeeze(-1)          # (B, 5)


        # ========== 2ï¸âƒ£ ç‰©ç†å½±å“å› å­éƒ¨åˆ† ==========
        raw_physic_output = self.physicMLP(last_output)  # (B, 160)
        raw_physic_output = raw_physic_output.view(B, 5, self.transformer_d_model)




        # æ¯ä¸ªå½±å“å› å­ç‹¬ç«‹å‹ç¼©
        influence_values = self.influence_compress(raw_physic_output)  # (B, 5, 1)
        influence_values = influence_values.squeeze(-1)   
        # print("influence_values.shape:",influence_values.shape) 



        sigmoid = torch.nn.Sigmoid()
        
        # # ------------------- è¡Œä¸ºå½±å“å› å­ï¼ˆphysic_outputï¼‰ -------------------
        theta_min, theta_max = 0.001, 0.999
        phi_min, phi_max     = 0.001, 0.999

        theta_ca = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 0:1])
        phi_ca   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 1:2])
        theta_an = theta_min + (theta_max - theta_min) * sigmoid(influence_values[:, 2:3])
        phi_an   = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 3:4])
        psi      = phi_min   + (phi_max   - phi_min)   * sigmoid(influence_values[:, 4:5])  # åŒæ ·é™åˆ¶


        # ------------------- å›ºæœ‰ç‰©æ€§å‚æ•°ï¼ˆparam_outputï¼‰ -------------------
        sigma_min, sigma_max = 0.01, 2  # S/cm
        sigma_mem = sigma_min + (sigma_max - sigma_min) * sigmoid(param_values[:, 0:1])

        alpha_min, alpha_max = 0.01, 2# S/cm
        alpha_ca = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 1:2])
        alpha_an = alpha_min + (alpha_max - alpha_min) * sigmoid(param_values[:, 2:3])

        log_i0ca_min, log_i0ca_max = -9, 0# S/cm
        log_i0an_min, log_i0an_max = -9, 0# S/cm
        log_i0ca = log_i0ca_min + (log_i0ca_max - log_i0ca_min) * sigmoid(param_values[:, 3:4])
        log_i0an = log_i0an_min + (log_i0an_max - log_i0an_min) * sigmoid(param_values[:, 4:5])
        i_0ca = torch.pow(10, log_i0ca)
        i_0an = torch.pow(10, log_i0an)


        
        
        # ========== Cross Attention èåˆå½±å“å› å­å’Œç‰©æ€§å‚æ•° ==========
        # ========== 3ï¸âƒ£ Cross Attention äº¤äº’ ==========
        # Query: param_output, Key: raw_physic_output
        cross_output, cross_attn_weights = self.my_cross_attn_transformer(
            query=param_output,
            key_value=raw_physic_output
        )

         # ******** æ–°å¢ï¼šç”¨ cross_output åšâ€œå…¨å±€è¡¨ç¤ºâ€ ********
        # å¯¹ 5 ä¸ª token åšå¹³å‡æ± åŒ–ï¼Œå¾—åˆ° (B, D) çš„æ•´ä½“è¡¨ç¤º
        cross_pooled = cross_output.mean(dim=1)  # (B, D)

        # åœ¨ rule-space ä¸­é¢„æµ‹ç±»åˆ«çº§è§„åˆ™åŸå‹
        rule_pred = self.rule_head(cross_pooled)      # (B, rule_dim)

        # é«˜/ä½å½±å“ç»„ & ç»„å†… 3 ç±»ç¦»å­
        group_logits = self.head_group(cross_pooled)  # (B, 2)
        high3_logits = self.head_high3(cross_pooled)  # (B, 3)
        low3_logits  = self.head_low3(cross_pooled)   # (B, 3)

        #é¢„æµ‹æµ“åº¦
        # 1) å½¢çŠ¶è½¬æ¢ -> (B, 5*self.transformer_d_model)
        cross_output_pred = cross_output.flatten(start_dim=1)  # #(B, 5, self.transformer_d_model)->(B,5*self.transformer_d_model)

        # 2) é¢„æµ‹æµ“åº¦
        conc_pred = self.conc_head(cross_output_pred)

        # 1ï¸âƒ£ æŠ•å½±
        # physic_embed = self.probMLP(param_output)  # (B, 5, 32)
        physic_embed_raw = self.probMLP(cross_output)  # (B, 5, 32)
        physic_embed = self.prob_norm(param_output + physic_embed_raw)  # âœ… æ®‹å·® + norm

        
        # 1ï¸âƒ£ è¾“å…¥ ion_attr_tensorï¼ˆé™æ€å±æ€§è¡¨ï¼‰ï¼Œå½¢çŠ¶ä¸º (6, 7)
        ion_attr_proj = self.ion_attr_embed(self.ion_attr_tensor)  # (6, 32)

        # 2ï¸âƒ£ æ·»åŠ  batch ç»´åº¦ï¼Œè¾“å…¥ Transformer
        ion_attr_proj = ion_attr_proj.unsqueeze(0)  # (1, 6, 32)

        # 3ï¸âƒ£ ç¼–ç ï¼Œè¿”å›ç¼–ç ç»“æœå’Œæ³¨æ„åŠ›æƒé‡
        ion_encoded, ion_attn_weights = self.ion_encoder(ion_attr_proj)  # (1, 6, 32), list of attn

        # 4ï¸âƒ£ å»æ‰ batch ç»´åº¦ï¼Œä½œä¸ºæ¯ä¸ªç¦»å­çš„åµŒå…¥å‘é‡
        ion_embeddings_raw = self.ion_postMLP(ion_encoded.squeeze(0))   # (6, 32)
        ion_embeddings = self.ion_norm(ion_encoded.squeeze(0) + ion_embeddings_raw)  # âœ… æ®‹å·® + norm

        # 5ï¸âƒ£ ç‚¹ç§¯ï¼šparam_output -> physic_embed (B, 5, 32)
        #         ion_embeddings -> (6, 32)
        raw_scores = torch.matmul(physic_embed, ion_embeddings.T)  # (B, 5, 6)

        # 6ï¸âƒ£ èšåˆ
        raw_prob_output = raw_scores.mean(dim=1)  # (B, 6)

        # 4ï¸âƒ£ Softmax
        prob_output = torch.softmax(raw_prob_output, dim=1)
        

    


        # ---------- ç”µå‹è®¡ç®—ï¼šåŸºäºé˜²NaNã€ç¨³å®šåçš„å‚æ•° ----------
        predicted_voltages = []
        for i in range(batch_size):
            pred_v, _, _, _, _ = calculate_predicted_voltage_plus(
                theta_ca[i], phi_ca[i], theta_an[i], phi_an[i], psi[i],
                temperature[i], flow[i], current[i],
                sigma_mem[i], alpha_ca[i], alpha_an[i], i_0ca[i], i_0an[i],
                electrolyzer_parameters= electrolyzer_parameters[i]  
            )
            predicted_voltages.append(pred_v)

        # ç»„åˆ batch çš„ç”µå‹ç»“æœ
        predicted_voltage = torch.stack(predicted_voltages).unsqueeze(1)  # shape: (B, 1)

        wuxing = [sigma_mem,alpha_ca,alpha_an,i_0ca,i_0an]
        influence=[psi,theta_ca,theta_an,phi_ca,phi_an]

        # åœ¨æœ€åå¤š return rule_pred / group_logits / high3_logits / low3_logits
        return (
            prob_output,         # (B, 6)
            predicted_voltage,   # (B, 1)
            conc_pred,           # (B, 1)
            wuxing,              # list of tensors
            cls_attn_mean,       # (B, seq_len)
            raw_prob_output,     # (B, 6)
            influence,           # list of tensors
            param_attn_mean,     # (B, N_tokens)
            freq_attn,           # (B, T, F)
            freq_attn_param,     # (B, F)
            rule_pred,           # (B, rule_dim)
            group_logits,        # (B, 2)
            high3_logits,        # (B, 3)
            low3_logits          # (B, 3)
        )

class Model_MLP_ClassifierOnly(nn.Module):
    """
    çº¯ MLP åˆ†ç±»åŸºçº¿ï¼š
    - æ‰€æœ‰è¾“å…¥å±•å¹³åæ‹¼æ¥ -> ä¸»å¹² MLP -> åˆ†ç±»å¤´ï¼ˆlogitsï¼‰
    - ä»…ä¿ç•™åˆ†ç±»ä»»åŠ¡ï¼ˆæ— ç”µå‹/æµ“åº¦/å½±å“å› å­/ç‰©ç†é‡/æ³¨æ„åŠ›ï¼‰
    """
    def __init__(
        self,
        num_time_points: int,
        num_freq_points: int,
        volt_channel_dim: int,   # volt_data æœ€åä¸€ç»´
        impe_channel_dim: int,   # impe_data æœ€åä¸€ç»´ï¼ˆå¦‚ 2=å®+è™šï¼‰
        env_dim: int,
        ep_dim: int,
        conc_dim: int,
        num_classes: int = 6,
        # ä¸»å¹²ä¸å¤´éƒ¨è¶…å‚
        backbone_hidden=(1024, 512, 256),
        backbone_out_dim=256,
        backbone_use_bn=True,
        backbone_dropout=0.0,
        head_hidden=(128, 64),
        head_use_bn=False,
        head_dropout=0.0,
    ):
        super().__init__()
        self.T  = num_time_points
        self.F  = num_freq_points
        self.Cv = volt_channel_dim
        self.Ci = impe_channel_dim
        self.env_dim  = env_dim
        self.ep_dim   = ep_dim
        self.conc_dim = conc_dim

        # å±•å¹³+æ‹¼æ¥åçš„æ€»è¾“å…¥ç»´åº¦
        input_dim = (self.T * self.Cv) + (self.T * self.F * self.Ci) + self.env_dim + self.ep_dim + self.conc_dim

        # ä¸»å¹²
        self.backbone = AdjustableMLP(
            input_dim=input_dim,
            hidden_dims=backbone_hidden,
            output_dim=backbone_out_dim,
            use_bn=backbone_use_bn,
            dropout=backbone_dropout
        )

        # åˆ†ç±»å¤´ï¼ˆè¾“å‡º logitsï¼‰
        self.classifier = AdjustableMLP(
            input_dim=backbone_out_dim,
            hidden_dims=head_hidden,
            output_dim=num_classes,
            use_bn=head_use_bn,
            dropout=head_dropout
        )

    def forward(self, volt_data, impe_data, env_params, electrolyzer_parameters, concentration):
        """
        è¾“å…¥ï¼š
            volt_data: (B, T, Cv)
            impe_data: (B, T, F, Ci)
            env_params: (B, E_env)
            electrolyzer_parameters: (B, E_ep)
            concentration: (B, E_conc)
        è¿”å›ï¼š
            logits: (B, num_classes)  â€”â€” ç›´æ¥ç”¨äº nn.CrossEntropyLoss
        """
        B = volt_data.size(0)

        x = torch.cat([
            volt_data.reshape(B, self.T * self.Cv),
            impe_data.reshape(B, self.T * self.F * self.Ci),
            env_params.reshape(B, -1),
            electrolyzer_parameters.reshape(B, -1),
            concentration.reshape(B, -1),
        ], dim=1)

        feat = self.backbone(x)      # (B, D)
        logits = self.classifier(feat)# (B, num_classes)
        return logits