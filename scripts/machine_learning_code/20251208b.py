import os
import csv
import json
import re
import argparse
import traceback
import multiprocessing
import random
import shutil
from pathlib import Path
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed

import torch
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
import pandas as pd

# ==== é¡¹ç›®å†…éƒ¨æ¨¡å— ====
from model_datasets import Dataset_2_Stable_plus
from model_models_schemeB import Model_three_system_1117
from model_test import test_single_xlsx_and_generate_explanations_three_system_1117
from model_train import Trainer_ThreeSystem_1117
from paired_dataset import SlidingWindowPairDataset, collate_pairs
from torch.utils.data import DataLoader, random_split, WeightedRandomSampler


# ==========================
# å…¨å±€åŸºç¡€é…ç½®ï¼ˆåç»­å¯ä»¥è¿ç§»åˆ° YAMLï¼‰
# ==========================

# é¢‘ç‚¹æ•° / æ—¶é—´ç‚¹æ•°ï¼ˆè·Ÿæ•°æ®é›†å’Œæ¨¡å‹ä¿æŒä¸€è‡´ï¼‰
NUM_FREQ_POINTS = 63
NUM_TIME_POINTS = 3

# è®­ç»ƒç›¸å…³ï¼ˆå°æ•°æ®é›†æ¨èï¼‰
BATCH_SIZE = 8          # æ ·æœ¬å¾ˆå°‘æ—¶ï¼Œå‡å° batchï¼Œæå‡æ¢¯åº¦å¤šæ ·æ€§
NUM_EPOCHS = 2000       # å°æ•°æ®é›†ä¸‹ä¸éœ€è¦å¤ªå¤š epochï¼Œé¿å…ä¸¥é‡è¿‡æ‹Ÿåˆ
LEARNING_RATE = 1e-4    # æ¯” 1e-5 ç•¥å¤§ä¸€äº›ï¼Œè®©æ¨¡å‹èƒ½æ›´å¿«æ”¶æ•›


# å¤šè¿›ç¨‹æµ‹è¯•ç›¸å…³ï¼šæœ€å¤šä½¿ç”¨ CPU ä¸€åŠ
CPU_COUNT = multiprocessing.cpu_count()
MAX_WORKERS = max(1, CPU_COUNT // 2)
test_device_str = "cpu"


def build_device():
    """è·å–è®¾å¤‡ä¸ device å­—ç¬¦ä¸²ï¼Œä¿è¯å¤šè¿›ç¨‹é‡Œç»Ÿä¸€ä½¿ç”¨åŒæ ·çš„ device_strã€‚"""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        device_str = "cuda"
    else:
        device = torch.device("cpu")
        device_str = "cpu"
    return device, device_str


def build_paths():
    """é›†ä¸­ç®¡ç†æœ¬è„šæœ¬ä¾èµ–çš„æ‰€æœ‰è·¯å¾„ã€‚"""
    current_file = Path(__file__).resolve()
    folder_name = current_file.stem             # å½“å‰è„šæœ¬æ–‡ä»¶åï¼ˆä¸å«åç¼€ï¼‰
    base_dir = current_file.parent.parent.parent

    # è®­ç»ƒé›† / æµ‹è¯•é›†
    data_folder = base_dir / "datasets" / "datasets_for_all"
    # æ³¨æ„ï¼šçœŸæ­£ä½¿ç”¨æ—¶ä¼šåœ¨æ­¤åŸºç¡€ä¸ŠåŠ ä¸Š folder_name ä½œä¸ºå­ç›®å½•
    test_folder_path = base_dir / "datasets" / "datasets_for_all_test"
    # test_folder_path = base_dir / "datasets" / "datasets_for_range_ion_0_6_2ppm"

    # æ¨¡å‹ä¿å­˜ç›®å½•
    model_save_folder = base_dir / "output" / "trained_model_save" / folder_name
    model_save_folder.mkdir(parents=True, exist_ok=True)

    # é»˜è®¤æµ‹è¯•æ¨¡å‹ï¼ˆæœ€ç»ˆæ¨¡å‹ï¼‰
    test_model_path = model_save_folder / "trained_model_epoch_final.pth"

    # ç»Ÿè®¡é‡ä¸æ ‡ç­¾æ˜ å°„
    stats_file = base_dir / "datasets" / "stats_dataset.json"
    json_path = current_file.parent / "label_mapping.json"

    # debug æ—¥å¿—ç›®å½•ï¼ˆæµ‹è¯•æ—¶å†™å…¥å•æ–‡ä»¶ logï¼‰
    debug_log_dir = base_dir / "output" / "debug_logs"
    debug_log_dir.mkdir(parents=True, exist_ok=True)

    # æ–°å¢ï¼šTensorBoard æ—¥å¿—ç›®å½•
    tb_log_dir = base_dir / "output" / "tensorboard" / folder_name
    tb_log_dir.mkdir(parents=True, exist_ok=True)

    return {
        "current_file": current_file,
        "folder_name": folder_name,
        "base_dir": base_dir,
        "data_folder": data_folder,
        "test_folder_path": test_folder_path,   # åç»­ä¼šè¢«ç»†åŒ–ä¸ºå¸¦ folder_name çš„å­ç›®å½•
        "model_save_folder": model_save_folder,
        "test_model_path": test_model_path,
        "stats_file": stats_file,
        "json_path": json_path,
        "debug_log_dir": debug_log_dir,
        "tb_log_dir": tb_log_dir,
    }


def load_label_mapping(json_path: Path):
    """è¯»å– label_mapping.jsonï¼Œæ–¹ä¾¿åç»­éœ€è¦æ—¶ä½¿ç”¨ã€‚"""
    if not json_path.exists():
        print(f"âš ï¸ æœªæ‰¾åˆ° label_mapping.json: {json_path}ï¼Œåç»­å¦‚æœä¸éœ€è¦å¯ä»¥å¿½ç•¥ã€‚")
        return {}
    with open(json_path, "r", encoding="utf-8") as f:
        label_mapping = json.load(f)
    return label_mapping


# ==========================
# æ–°å¢ï¼šæ ¹æ®æ–‡ä»¶åæ¨æ–­ç¦»å­ç±»åˆ« & åˆ‡åˆ†æµ‹è¯•é›†
# ==========================

def infer_label_from_filename(fname: str, label_mapping: dict) -> str | None:
    """
    å°è¯•ä»æ–‡ä»¶åæ¨æ–­ç¦»å­ç±»åˆ«ï¼š
    1. ä¼˜å…ˆä½¿ç”¨ label_mapping.json ä¸­çš„é”®ï¼ˆå¦‚æœæ˜¯ä¸­æ–‡æ ‡ç­¾ï¼Œä¼šç›´æ¥åŒ¹é…ï¼‰
    2. é€€è€Œæ±‚å…¶æ¬¡ï¼Œä½¿ç”¨å›ºå®šçš„ä¸­æ–‡å…³é”®è¯åŒ¹é…
    è¿”å›ï¼šåŒ¹é…åˆ°çš„â€œç±»åˆ«åâ€ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼›è‹¥æ— æ³•è¯†åˆ«åˆ™è¿”å› Noneã€‚
    """
    # å…ˆå°è¯•ç”¨ label_mapping çš„ key åšå­ä¸²åŒ¹é…
    if label_mapping:
        for label in label_mapping.keys():
            try:
                if isinstance(label, str) and label in fname:
                    return label
            except Exception:
                continue

    # å…³é”®è¯å…œåº•ï¼ˆæŒ‰ä½ å½“å‰ä»»åŠ¡é‡Œçš„ 7 ç±»æ¥å†™ï¼‰
    keywords = [
        "é’™ç¦»å­",
        "é’ ç¦»å­",
        "é•ç¦»å­",
        "é“¬ç¦»å­",
        "é“œç¦»å­",
        "é“ç¦»å­",
        "æ— æ±¡æŸ“",
    ]
    for kw in keywords:
        if (kw in fname) and  ("ion_column" not in fname):
            return kw

    # å¸¸è§â€œæ— æ±¡æŸ“â€åˆ«åå…œåº•
    lower = fname.lower()
    if ("blank" in lower) or ("çº¯æ°´" in lower) or ("ion_column" in fname):
        return "æ— æ±¡æŸ“"

    return None
def is_valid_xlsx_for_model(xlsx_path: Path,
                            num_time_points: int,
                            num_freq_points: int) -> bool:
    """
    åˆ¤æ–­ä¸€ä¸ª xlsx æ˜¯å¦â€œå¯ç”¨â€ï¼Œç”¨äºåˆ’åˆ†æµ‹è¯•é›†æ—¶è¿‡æ»¤åæ ·æœ¬ã€‚

    åˆ¤å®šè§„åˆ™ï¼ˆä¸ä½ ç»™çš„é€»è¾‘ä¿æŒä¸€è‡´ï¼‰ï¼š
    1) èƒ½æˆåŠŸè¯»å–ä¸º DataFrame
    2) å¿…é¡»åŒ…å«åˆ—ï¼šTime(h), mean_voltage, Zreal, Zimag, Freq(Hz)
    3) è‡³å°‘æœ‰ num_time_points ä¸ªä¸åŒçš„ Time(h)
    4) é€‰å®š num_time_points ä¸ªæ—¶é—´ç‚¹ï¼ˆå‡åºå‰ num_time_pointsï¼‰ï¼Œ
       å¯¹äºæ¯ä¸ªæ—¶é—´ç‚¹ï¼š
       - è¡Œæ•° >= num_freq_points
       - mean_voltage é NaN
    """
    try:
        df = pd.read_excel(xlsx_path)
    except Exception as e:
        print(f"[SKIP] æ— æ³•è¯»å–æ–‡ä»¶ {xlsx_path.name}: {e}")
        return False

    required_cols = ["Time(h)", "mean_voltage", "Zreal", "Zimag", "Freq"]
    for col in required_cols:
        if col not in df.columns:
            print(f"[SKIP] æ–‡ä»¶ {xlsx_path.name} ç¼ºå°‘å¿…è¦åˆ—: {col}")
            return False

    # å»æ‰ NaN åç»Ÿè®¡æ—¶é—´ç‚¹
    time_series = df["Time(h)"].dropna().unique().tolist()
    if len(time_series) < num_time_points:
        print(
            f"[SKIP] æ–‡ä»¶ {xlsx_path.name}: ä»…æœ‰ {len(time_series)} ä¸ªæ—¶é—´ç‚¹ "
            f"< è¦æ±‚çš„ {num_time_points}"
        )
        return False

    # é€‰å®šéœ€è¦æ£€æŸ¥çš„æ—¶é—´ç‚¹ï¼šå‡åºå–å‰ num_time_points ä¸ª
    time_points = sorted(time_series)[:num_time_points]

    for t in time_points:
        time_data = df[df["Time(h)"] == t]
        if time_data.empty:
            print(f"[SKIP] æ–‡ä»¶ {xlsx_path.name}: ç¼ºå°‘æ—¶é—´ç‚¹ {t}h")
            return False

        # æŒ‰é¢‘ç‡æ’åº
        time_data = time_data.sort_values(by="Freq")

        voltage = time_data["mean_voltage"].values[0]
        if pd.isna(voltage):
            print(f"[SKIP] æ–‡ä»¶ {xlsx_path.name}: æ—¶é—´ç‚¹ {t}h çš„ mean_voltage ä¸º NaN")
            return False

        impedance_np = time_data[["Zreal", "Zimag"]].values
        if impedance_np.shape[0] < num_freq_points:
            print(
                f"[SKIP] æ–‡ä»¶ {xlsx_path.name}: æ—¶é—´ç‚¹ {t}h çš„é˜»æŠ—ç‚¹æ•° "
                f"{impedance_np.shape[0]} < è¦æ±‚çš„ {num_freq_points}"
            )
            return False

    # æ‰€æœ‰æ£€æŸ¥é€šè¿‡
    print(f"[OK] å¯ä½œä¸ºæµ‹è¯•æ ·æœ¬: {xlsx_path.name}")
    return True

def prepare_test_folder(paths: dict, label_mapping: dict,num_time_points, num_freq_points):
    """
    ç›®æ ‡ï¼š
    - å¦‚æœ test_folder_path == .../datasets_for_all_testï¼š
        ä» data_folder ä¸­ä¸ºæ¯ç§ç¦»å­æŠ½å– 3 ä¸ªâ€œå¯ç”¨â€æ ·æœ¬ï¼Œç§»åŠ¨åˆ°
        test_folder_path / folder_name å¯¹åº”çš„å­ç›®å½•ï¼Œä½œä¸ºè‡ªåŠ¨åˆ’åˆ†çš„æµ‹è¯•é›†ï¼›
    - å¦åˆ™ï¼ˆç”¨æˆ·æŒ‡å®šäº†å…¶å®ƒæµ‹è¯•ç›®å½•ï¼‰ï¼š
        ä¸åšä»»ä½•åˆ’åˆ†/ç§»åŠ¨ï¼Œç›´æ¥ä½¿ç”¨è¯¥ç›®å½•ä¸‹å·²æœ‰çš„ .xlsx ä½œä¸ºæµ‹è¯•/éªŒè¯é›†ã€‚
    """
    data_folder: Path = paths["data_folder"]
    base_test_folder: Path = paths["test_folder_path"]
    folder_name: str = paths["folder_name"]

    # ========= æ–°å¢ï¼šåŒºåˆ†â€œè‡ªåŠ¨åˆ’åˆ†æ¨¡å¼â€å’Œâ€œå›ºå®šæµ‹è¯•ç›®å½•æ¨¡å¼â€ =========
    if base_test_folder.name != "datasets_for_all_test":
        # å›ºå®šæµ‹è¯•ç›®å½•æ¨¡å¼ï¼šä¸å†åˆ›å»ºå­ç›®å½•ï¼Œä¹Ÿä¸åšåˆ’åˆ†/ç§»åŠ¨
        base_test_folder.mkdir(parents=True, exist_ok=True)
        existing_xlsx = list(base_test_folder.glob("*.xlsx"))

        if existing_xlsx:
            print(
                f"ğŸ“‚ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æµ‹è¯•ç›®å½•ï¼š{base_test_folder} "
                f"ï¼ˆå‘ç° {len(existing_xlsx)} ä¸ª .xlsx æ–‡ä»¶ï¼‰"
            )
        else:
            print(
                f"âš ï¸ æŒ‡å®šçš„æµ‹è¯•ç›®å½• {base_test_folder} ä¸­æ²¡æœ‰ä»»ä½• .xlsx æ–‡ä»¶ï¼Œ"
                f"åç»­æµ‹è¯•/éªŒè¯å°†æ²¡æœ‰æ ·æœ¬å¯ç”¨ã€‚"
            )

        # ç›´æ¥æŠŠ test_folder_path å›ºå®šä¸ºè¿™ä¸ªç›®å½•
        paths["test_folder_path"] = base_test_folder
        return

    # ========= ä»¥ä¸‹æ˜¯åŸæ¥çš„â€œè‡ªåŠ¨åˆ’åˆ†æµ‹è¯•é›†â€é€»è¾‘ï¼Œä»…åœ¨
    #          test_folder_path == .../datasets_for_all_test æ—¶ç”Ÿæ•ˆ =========
    current_test_folder = base_test_folder / folder_name
    current_test_folder.mkdir(parents=True, exist_ok=True)

    # è‹¥è¯¥ç›®å½•å·²å­˜åœ¨æµ‹è¯•æ ·æœ¬ï¼Œåˆ™ä¸å†é‡æ–°åˆ’åˆ†
    existing_xlsx = list(current_test_folder.glob("*.xlsx"))
    if existing_xlsx:
        print(
            f"ğŸ“‚ æ£€æµ‹åˆ°å·²æœ‰æµ‹è¯•æ ·æœ¬ï¼ˆå…± {len(existing_xlsx)} ä¸ªï¼‰ï¼Œ"
            f"ç›´æ¥ä½¿ç”¨ï¼š{current_test_folder}"
        )
        paths["test_folder_path"] = current_test_folder
        return

    # ----------------- è‡ªåŠ¨åˆ’åˆ†æµ‹è¯•é›† -----------------
    all_xlsx = sorted(data_folder.glob("*.xlsx"))

    if not all_xlsx:
        print(f"âš ï¸ åœ¨ {data_folder} ä¸‹æœªæ‰¾åˆ°ä»»ä½• .xlsx æ–‡ä»¶ï¼Œæ— æ³•åˆ’åˆ†æµ‹è¯•é›†ã€‚")
        paths["test_folder_path"] = current_test_folder
        return

    label_to_files: dict[str, list[Path]] = defaultdict(list)

    for f in all_xlsx:
        label = infer_label_from_filename(f.name, label_mapping)
        if label is None:
            # ä¸è¯†åˆ«çš„æ ·æœ¬æš‚æ—¶å¿½ç•¥ï¼Œä¸å‚ä¸â€œæ¯ç±» 3 ä¸ªâ€çš„åˆ’åˆ†
            continue

        # åœ¨åˆ’åˆ†é˜¶æ®µå°±åšâ€œå¯ç”¨æ€§æ£€æŸ¥â€ï¼ŒåªæŠŠåˆæ ¼æ ·æœ¬æ”¾å…¥å€™é€‰æ± 
        if not is_valid_xlsx_for_model(f,num_time_points, num_freq_points):
            continue

        label_to_files[label].append(f)

    if not label_to_files:
        print("âš ï¸ æ²¡æœ‰ä»»ä½•å¯ç”¨äºåˆ’åˆ†æµ‹è¯•é›†çš„åˆæ ¼æ ·æœ¬ã€‚")
        paths["test_folder_path"] = current_test_folder
        return

    # ä¸ºä¿è¯å¯å¤ç°ï¼Œè¿™é‡Œä¸æ‰“ä¹±ï¼ŒåªæŒ‰æ–‡ä»¶åæ’åºåå–å‰ 3 ä¸ª
    random.seed(42)

    total_moved = 0
    for label, files in label_to_files.items():
        if not files:
            continue

        files_sorted = sorted(files, key=lambda p: p.name)
        # å„ç±»æœ€å¤š 3 ä¸ªï¼Œå¦‚æœä¸è¶³ 3 ä¸ªï¼Œå°±å…¨æ‹¿
        selected = files_sorted[:3]

        print(f"ğŸ§ª ç±»åˆ« [{label}] é€‰ä¸­ {len(selected)} ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†ã€‚")
        for f in selected:
            dest = current_test_folder / f.name
            print(f"  - ç§»åŠ¨ {f} -> {dest}")
            shutil.move(str(f), str(dest))
            total_moved += 1

    print(
        f"âœ… æµ‹è¯•é›†åˆ’åˆ†å®Œæˆï¼Œå…±ç§»åŠ¨ {total_moved} ä¸ªæ ·æœ¬åˆ° {current_test_folder}"
    )
    paths["test_folder_path"] = current_test_folder


# ==========================
# æ¨¡å‹æ„å»º
# ==========================

def build_model(device: torch.device):
    """
    æ„é€  Model_three_system_1117 æ¨¡å‹ã€‚

    å½“å‰å‚æ•°ä»ç„¶æ˜¯ç¡¬ç¼–ç çš„ï¼Œå¯ä»¥åœ¨æœªæ¥è¿ç§»åˆ° config æ–‡ä»¶ä¸­ã€‚
    """
    model = Model_three_system_1117(
        # ---- ç”µå‹åˆ†æ”¯ ----
        volt_input_dim=1,
        volt_mlp_hidden_dims=[64],
        mlp_output_dims=64,
        volt_mlp_num_layers=1,

        # ---- é˜»æŠ—åˆ†æ”¯ ----
        impe_input_dim=2,
        impe_mlp_hidden_dims=[64],
        impe_mlp_num_layers=1,

        # ---- Transformer é…ç½® ----
        transformer_d_model=64,
        nhead=4,
        transformer_num_layers=1,
        param_transformer_num_layers=1,

        # ---- ç‰©ç† MLP ----
        physic_mlp_hidden_dims=[64],
        physic_mlp_num_layers=1,

        # ---- Ion å±æ€§ ----
        ion_attr_embed_hidden_dims=[64],
        ion_attr_embed_num_layers=1,
        ion_encoder_num_layers=1,
        ion_post_hidden_dims=[64],
        ion_post_num_layers=1,

        # ---- åˆ†ç±»å¤´ ----
        probMLP_input_dims=64,
        probMLP_hidden_dims=[64],
        probMLP_num_layers=1,

        # ---- ç‰©æ€§å‚æ•° MLP ----
        param_mlp_hidden_dims=[64],
        param_mlp_num_layers=1,

        # ---- ç¼–ç å™¨ ----
        freq_encoder_hidden_dims=[64],
        freq_encoder_num_layers=1,
        time_encoder_hidden_dims=[64],
        time_encoder_num_layers=1,

        # ---- Cross Transformer ----
        cross_transformer_num_layers=1,

        # ---- å‚æ•° / ç‰©ç† embedding ----
        param_embed_hidden_dims=[64],
        param_embed_num_layers=0,
        physic_embed_hidden_dims=[64],
        physic_embed_num_layers=0,

        # ---- ç¯å¢ƒå‚æ•° / EP åˆ†æ”¯ ----
        envMLP_input_dim=3,
        env_mlp_hidden_dims=[64],
        env_mlp_num_layers=1,
        ep_input_dim=8,
        ep_mlp_hidden_dims=[64],
        ep_mlp_num_layers=1,

        # ---- Z ç¼–ç å™¨ ----
        Z_encoder_num_layers=1,

        num_freq_points=NUM_FREQ_POINTS,
        num_time_points=NUM_TIME_POINTS,
    ).to(device)

    return model


def build_dataloaders(paths: dict):
    """
    æ„å»º Dataset å’Œ Dataloaderï¼ˆè®­ç»ƒ/éªŒè¯ï¼‰ã€‚

    - åŸºç¡€æ•°æ®ï¼šDataset_2_Stable_plus
    - Pair æ•°æ®ï¼šSlidingWindowPairDataset
    - è®­ç»ƒé›†ï¼špaths["data_folder"]
    - éªŒè¯/æµ‹è¯•é›†ï¼špaths["test_folder_path"]ï¼ˆå½“å‰è„šæœ¬å¯¹åº”çš„å­ç›®å½•ï¼‰
    """
    data_folder = paths["data_folder"]
    val_data_folder = paths["test_folder_path"]
    stats_file = paths["stats_file"]

    # 1) æ„å»ºåŸºç¡€è®­ç»ƒæ•°æ®é›†ï¼ˆå¸¦å½’ä¸€åŒ–, å¹¶å†™å…¥ statsï¼‰
    base_train = Dataset_2_Stable_plus(
        data_folder=data_folder,
        stats_file=str(stats_file),   # json è·¯å¾„ç”¨ str
        save_stats=True,
        num_time_points=NUM_TIME_POINTS
    )

    # 2) æ„å»ºéªŒè¯æ•°æ®é›†ï¼ˆä½¿ç”¨åŒä¸€ä»½ stats_fileï¼Œåªè¯»ä¸å†å†™ï¼‰
    base_val = Dataset_2_Stable_plus(
        data_folder=val_data_folder,
        stats_file=str(stats_file),
        save_stats=False,             # éªŒè¯é›†åªå¤ç”¨ç»Ÿè®¡é‡ï¼Œé¿å…è¦†ç›–
        num_time_points=NUM_TIME_POINTS
    )

    # 3) æ„å»ºæ»‘åŠ¨çª—å£ Pair æ•°æ®é›†
    pair_train = SlidingWindowPairDataset(
        base_train,
        keep_unpaired="drop",
        debug=True,
        focus_prefix_contains="20240915_2ppmé“œç¦»å­æ±¡æŸ“æµ‹è¯•_æ—§ç‰ˆç”µè§£æ§½_ion_firecloud_",
        max_print=0,
        num_time_points=NUM_TIME_POINTS
    )

    # éªŒè¯é›†ä¸åšå‰ç¼€è¿‡æ»¤ï¼Œå®Œæ•´ä½¿ç”¨æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ ·æœ¬
    pair_val = SlidingWindowPairDataset(
        base_val,
        keep_unpaired="drop",
        debug=False,
        focus_prefix_contains=None,
        max_print=0,
        num_time_points=NUM_TIME_POINTS
    )

    print("num base train samples:", len(base_train))
    print("num base val   samples:", len(base_val))
    print("num pair train samples:", len(pair_train))
    print("num pair val   samples:", len(pair_val))

    if len(pair_train) > 0:
        (A, B, dummy_mask) = collate_pairs([pair_train[0]])
        print("dummy_mask[0] =", bool(dummy_mask[0]))

    # 4) è®­ç»ƒé›†é‡‡æ ·æƒé‡ï¼ˆæŒ‰é˜¶æ®µ upweight å‰§çƒˆé˜¶æ®µï¼‰
    train_dataset = pair_train
    val_dataset = pair_val

    base_weight = 1.0        # stage = 0 æ—¶çš„æƒé‡
    rapid_weight = 3.0       # stage = 1 æ—¶çš„æƒé‡ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒå¤§/è°ƒå°ï¼‰

    sample_weights = []
    for i in range(len(train_dataset)):
        sampleA, sampleB, *others = train_dataset[i]
        stageA = int(sampleA[-1].item())
        stageB = int(sampleB[-1].item())

        # åªè¦ B æ˜¯å‰§çƒˆé˜¶æ®µï¼Œå°±è®¤ä¸ºè¿™ä¸ª pair å±äº "rapid" pair
        if stageB == 1:
            sample_weights.append(rapid_weight)
        else:
            sample_weights.append(base_weight)

    # è½¬æˆ tensor ä¾› WeightedRandomSampler ä½¿ç”¨
    sample_weights_tensor = torch.tensor(sample_weights, dtype=torch.double)

    sampler = WeightedRandomSampler(
        weights=sample_weights_tensor,
        num_samples=len(sample_weights_tensor),  # æ¯ä¸ª epoch é‡‡æ ·è¿™ä¹ˆå¤š pair
        replacement=True,                        # å…è®¸é‡å¤é‡‡æ ·
    )

    # è®­ç»ƒé›†ï¼šä½¿ç”¨åŠ æƒé‡‡æ ·ï¼Œä¸å†ç”¨ shuffle=True
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        sampler=sampler,
        collate_fn=collate_pairs,
    )

    # éªŒè¯é›†ï¼šä¿æŒåŸæ¥çš„å‡åŒ€é¡ºåº
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        collate_fn=collate_pairs,
    )

    return train_loader, val_loader


# ==========================
# æµ‹è¯•/æœç´¢ç”¨çš„å•æ–‡ä»¶å¤„ç†å‡½æ•°
# ==========================

def process_single_file(
    xlsx_path_str: str,
    model_path: str,
    device_str: str,
    num_freq_points: int,
    num_time_points: int,
    folder_name: str,
    log_dir: Path,
):
    """
    å•ä¸ª xlsx æ–‡ä»¶çš„å®Œæ•´æµ‹è¯•æµç¨‹ï¼š
    - åŠ è½½æ¨¡å‹
    - è°ƒç”¨ test_single_xlsx_and_generate_explanations_three_system_1117
    - è¿”å› (correct, predict, truth)

    âš ï¸ å½“å‰å®ç°ï¼šæ¯ä¸ªè¿›ç¨‹ / æ–‡ä»¶éƒ½ä¼šé‡æ–°åŠ è½½æ¨¡å‹ï¼Œé€»è¾‘è¾ƒç®€å•ä½†æ•ˆç‡ç•¥ä½ã€‚
       å¦‚æœåç»­æµ‹è¯•æ–‡ä»¶éå¸¸å¤šï¼Œå¯ä»¥ç”¨â€œè¿›ç¨‹åˆå§‹åŒ–æ—¶åŠ è½½æ¨¡å‹â€çš„æ–¹å¼è¿›è¡Œä¼˜åŒ–ã€‚
    """
    xlsx_path = Path(xlsx_path_str)
    log_path = log_dir / f"{xlsx_path.stem}.log"

    with open(log_path, "w", encoding="utf-8") as logf:
        try:
            device = torch.device(device_str)
            model = build_model(device)
            state = torch.load(model_path, map_location=device, weights_only=True)
            model.load_state_dict(state, strict=False)
            model.eval()

            correct, predict, truth = test_single_xlsx_and_generate_explanations_three_system_1117(
                xlsx_path=xlsx_path_str,
                model=model,
                device=device,
                num_time_points=num_time_points,
                num_freq_points=num_freq_points,
                folder_name=folder_name,
            )
            return correct, predict, truth
        except Exception:
            print("âŒ æ–‡ä»¶å¤„ç†å‡ºé”™:", file=logf)
            traceback.print_exc(file=logf)
            print("=== å¤„ç†å¤±è´¥ ===", file=logf)
            return None, None, None


# ==========================
# ä¸‰ç§æ¨¡å¼ï¼štrain / test / search
# ==========================

def run_train(device, paths):
    print("ğŸš€ è¿›å…¥è®­ç»ƒæ¨¡å¼ (--train)")
    train_loader, val_loader = build_dataloaders(paths)

    model = build_model(device)
    # ä½¿ç”¨é€‚åº¦çš„ weight_decayï¼Œç¼“è§£å°æ•°æ®é›†ä¸‹çš„è¿‡æ‹Ÿåˆ
    optimizer = optim.Adam(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=1e-4,
    )

    # âœ… æ–°å¢ï¼šåˆ›å»º TensorBoard writer
    tb_log_dir = paths["tb_log_dir"]
    print(f"ğŸ“ TensorBoard æ—¥å¿—ç›®å½•: {tb_log_dir}")
    writer = SummaryWriter(log_dir=str(tb_log_dir))

    trainer = Trainer_ThreeSystem_1117(
        model=model,
        optimizer=optimizer,
        device=device,
        model_save_folder=paths["model_save_folder"],

        # ---- ä¸»ä»»åŠ¡ï¼šåˆ†ç±»ä»ç„¶ç»å¯¹ä¸»å¯¼ ----
        alpha=0.05,     # ç”µå‹åšä¸€ç‚¹ç‚¹è¾…åŠ©ï¼Œä¸è¦å¤ªå¤§
        beta=1.0,       # åˆ†ç±»ä¸»ä»»åŠ¡
        gamma=6e-5,      # æµ“åº¦æš‚æ—¶ä¸å¼€å¯ï¼Œé¿å…ç›®æ ‡å¤ªå¤š

        # ---- è§„åˆ™ / åˆ†ç»„ / å†³ç­–æ ‘ / é¢‘æ®µï¼šå…¨éƒ¨ä½œä¸ºå¼±æ­£åˆ™ ----
        lambda_rule=5e-4,   # rule åŸå§‹ ~40 â†’ åŠ æƒ ~0.02
        lambda_group=5e-4,  # group åŸå§‹ ~1.3 â†’ åŠ æƒ ~0.0007
        lambda_tree=5e-4,   # tree åŸå§‹ ~3.2 â†’ åŠ æƒ ~0.0016
        lambda_band=2e-3,   # band åŸå§‹ ~0.13 â†’ åŠ æƒ ~0.00026

        save_every=10,
        label_smoothing=0.05   # å¯ä»¥ä¿ç•™ 0.05 æˆ–è€…ç”¨ä½ åœ¨é˜¶æ®µä¸€ä¸­è¡¨ç°æœ€å¥½çš„å€¼
    )

    trainer.train_pairs(
        train_loader,
        num_epochs=NUM_EPOCHS,
        lambda_consistency=5e-4,  # consistency åŸå§‹ ~0.48 â†’ åŠ æƒ ~0.00024
        eps=1e-9,
        use_log_space=True,
        lambda_monodec=5e-4,      # monodec åŸå§‹ ~3 å·¦å³ â†’ åŠ æƒ ~0.0015
        lambda_polarity=2e-4,     # polarity åŸå§‹ ~2 å·¦å³ â†’ åŠ æƒ ~0.0004
        weight_ratio=3.0          # è®© stage=1 ç¨å¾®æ›´é‡ä¸€ç‚¹ï¼Œä½†ä¸è¦å¤ªæç«¯
    )

    # âœ… è®­ç»ƒç»“æŸè®°å¾—å…³æ‰
    writer.close()


def run_test(device_str: str, paths: dict):
    """æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æœ€ç»ˆfinalæ¨¡å‹ test_model_pathï¼Œå¯¹ test_folder_path ä¸‹æ‰€æœ‰ .xlsx åšå¹¶è¡Œæµ‹è¯•ã€‚"""
    print("ğŸ” è¿›å…¥æµ‹è¯•æ¨¡å¼ (--test)")

    test_model_path = paths["test_model_path"]
    test_folder_path = paths["test_folder_path"]
    folder_name = paths["folder_name"]
    log_dir = paths["debug_log_dir"]

    print(f"ğŸ” æ­£åœ¨åŠ è½½æ¨¡å‹: {test_model_path}")
    xlsx_paths = [
        str(test_folder_path / f)
        for f in os.listdir(test_folder_path)
        if f.endswith(".xlsx")
    ]
    print(f"ğŸ“Š å…±æ£€æµ‹åˆ° {len(xlsx_paths)} ä¸ª .xlsx æ–‡ä»¶ï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç†...")

    correct_count = 0
    total = len(xlsx_paths)

    # confusion_counter[true_label][predicted_label] = count
    confusion_counter = defaultdict(lambda: defaultdict(int))

    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [
            executor.submit(
                process_single_file,
                path,
                str(test_model_path),
                test_device_str,   # â† è¿™é‡Œä½¿ç”¨ "cpu"
                NUM_FREQ_POINTS,
                NUM_TIME_POINTS,
                folder_name,
                log_dir,
            )
            for path in xlsx_paths
        ]

        for future in as_completed(futures):
            try:
                correct, predict, truth = future.result()
                if correct is None:
                    continue
                if correct:
                    correct_count += 1
                confusion_counter[truth][predict] += 1
            except Exception as e:
                print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

    if total > 0:
        accuracy = correct_count / total
        print(f"\nâœ… æ€»å…±æµ‹è¯•æ ·æœ¬æ•°: {total}")
        print(f"ğŸ¯ é¢„æµ‹æ­£ç¡®æ ·æœ¬æ•°: {correct_count}")
        print(f"ğŸ“Š å‡†ç¡®ç‡: {accuracy:.2%}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½• .xlsx æµ‹è¯•æ–‡ä»¶")

    # æ‰“å°è¯¦ç»†çš„é”™è¯¯åˆ†å¸ƒï¼ˆconfusion matrixï¼‰
    print("\nğŸ“‰ é”™è¯¯åˆ†æï¼ˆçœŸå®æ ‡ç­¾ â†’ é¢„æµ‹æ ‡ç­¾ â†’ ä¸ªæ•°ï¼‰:")
    for truth_label, pred_dict in confusion_counter.items():
        for predicted_label, count in pred_dict.items():
            print(f"  çœŸå®: {truth_label} â†’ é¢„æµ‹: {predicted_label} : {count} ä¸ª")


def _parse_epoch_num(path: Path) -> int:
    """ä» checkpoint æ–‡ä»¶åä¸­è§£æ epoch æ•°å­—ã€‚å½¢å¦‚ trained_model_epoch_123.pthã€‚"""
    m = re.search(r"trained_model_epoch_(\d+)\.pth$", path.name)
    return int(m.group(1)) if m else -1


def run_search(device_str: str, paths: dict):
    """
    æœç´¢æ¨¡å¼ï¼šéå†æŸä¸ªç›®å½•ä¸‹æ‰€æœ‰ checkpointï¼Œå¯¹æµ‹è¯•é›†åšå®Œæ•´è¯„ä¼°ï¼Œå¯¼å‡º CSV + æ›²çº¿å›¾ã€‚
    """
    print("ğŸ” è¿›å…¥æœç´¢æ¨¡å¼ (--search)")

    model_save_folder = paths["model_save_folder"]
    test_folder_path = paths["test_folder_path"]
    folder_name = paths["folder_name"]
    log_dir = paths["debug_log_dir"]

    # 1) æœç´¢æ‰€æœ‰ checkpointï¼ˆä¸å« finalï¼‰
    ckpts = [
        p for p in model_save_folder.glob("trained_model_epoch_*.pth")
        if p.name != "trained_model_epoch_final.pth"
    ]
    ckpts = sorted(ckpts, key=_parse_epoch_num)

    if not ckpts:
        print(f"âš ï¸ æœªåœ¨ {model_save_folder} æ‰¾åˆ°ä»»ä½• trained_model_epoch_*.pth")
        return

    # 2) å‡†å¤‡æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    xlsx_paths = [
        str(test_folder_path / f)
        for f in os.listdir(test_folder_path)
        if f.endswith(".xlsx")
    ]
    if not xlsx_paths:
        print(f"âš ï¸ æœªåœ¨ {test_folder_path} æ‰¾åˆ°ä»»ä½• .xlsx æµ‹è¯•æ–‡ä»¶")
        return

    print(f"ğŸ” å…±å‘ç° {len(ckpts)} ä¸ª checkpointï¼Œå°†é€ä¸ªè¯„æµ‹ï¼›æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{len(xlsx_paths)}")

    # ç»“æœåˆ—è¡¨ï¼š[(epoch, acc, correct, total, ckpt_file), ...]
    results = []

    # 3) é€ä¸ª checkpoint åšè¯„ä¼°
    for i, ckpt_path in enumerate(ckpts, 1):
        epoch_num = _parse_epoch_num(ckpt_path)
        if epoch_num < 0:
            print(f"è·³è¿‡æ— æ³•è¯†åˆ« epoch çš„æ–‡ä»¶ï¼š{ckpt_path.name}")
            continue

        print(f"\n[{i}/{len(ckpts)}] ğŸ” è¯„æµ‹ checkpoint: {ckpt_path.name}  (epoch={epoch_num})")
        correct_count, total = 0, len(xlsx_paths)
        confusion_counter = defaultdict(lambda: defaultdict(int))

        # å¯¹å½“å‰ ckpt è·‘ä¸€éå®Œæ•´æµ‹è¯•
        with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [
                executor.submit(
                    process_single_file,
                    path,
                    str(ckpt_path),   # æ³¨æ„è¿™é‡Œä½¿ç”¨å½“å‰ epoch å¯¹åº”çš„æƒé‡
                    test_device_str,  # â† è¿™é‡Œä½¿ç”¨ "cpu"
                    NUM_FREQ_POINTS,
                    NUM_TIME_POINTS,
                    folder_name,
                    log_dir,
                )
                for path in xlsx_paths
            ]

            for future in as_completed(futures):
                try:
                    correct, predict, truth = future.result()
                    if correct is None:
                        continue
                    if correct:
                        correct_count += 1
                    confusion_counter[truth][predict] += 1
                except Exception as e:
                    print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

        acc = (correct_count / total) if total > 0 else 0.0
        print(f"ğŸ¯ epoch={epoch_num} | æ­£ç¡® {correct_count}/{total} | å‡†ç¡®ç‡={acc:.2%}")
        results.append((epoch_num, acc, correct_count, total, ckpt_path.name))

    # 4) å¯¼å‡º CSVï¼ˆæŒ‰ epoch å‡åºï¼‰
    results_by_epoch = sorted(results, key=lambda x: x[0])
    csv_path_epoch = model_save_folder / "search_checkpoints_epoch_sorted.csv"
    with open(csv_path_epoch, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["rank_by_epoch", "epoch", "accuracy", "correct", "total", "ckpt_file"])
        for rank, (ep, acc, cor, tot, name) in enumerate(results_by_epoch, 1):
            w.writerow([rank, ep, f"{acc:.6f}", cor, tot, name])
    print(f"\nâœ… å·²ä¿å­˜ CSVï¼ˆæŒ‰ epoch å‡åºï¼‰ï¼š{csv_path_epoch}")

    # 5) å¦å­˜ä¸€ä»½â€œæŒ‰å‡†ç¡®ç‡å‡åºâ€çš„ CSV
    results_by_acc = sorted(results, key=lambda x: x[1])
    csv_path_acc = model_save_folder / "search_checkpoints_accuracy_sorted.csv"
    with open(csv_path_acc, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["rank_by_acc", "epoch", "accuracy", "correct", "total", "ckpt_file"])
        for rank, (ep, acc, cor, tot, name) in enumerate(results_by_acc, 1):
            w.writerow([rank, ep, f"{acc:.6f}", cor, tot, name])
    print(f"âœ… å·²ä¿å­˜ CSVï¼ˆæŒ‰å‡†ç¡®ç‡å‡åºï¼‰ï¼š{csv_path_acc}")

    # 6) ç”»å›¾ï¼ˆä¸»å›¾æŒ‰ epoch å‡åºï¼‰
    try:
        # å›¾1ï¼šAccuracy vs Epoch
        eps = [r[0] for r in results_by_epoch]
        acc_e = [r[1] for r in results_by_epoch]
        plt.figure()
        plt.plot(eps, acc_e, marker="o")
        plt.xlabel("Epoch (ascending)")
        plt.ylabel("Accuracy")
        plt.title("Checkpoint Accuracy vs Epoch (epoch-ascending)")
        plt.grid(True)
        plt.tight_layout()
        png_path_by_epoch = model_save_folder / "search_checkpoints_accuracy_by_epoch.png"
        plt.savefig(png_path_by_epoch, dpi=150)
        plt.close()
        print(f"ğŸ–¼ï¸ å·²ä¿å­˜æ›²çº¿å›¾ï¼ˆæŒ‰ epoch å‡åºï¼‰ï¼š{png_path_by_epoch}")

        # å›¾2ï¼šæŒ‰å‡†ç¡®ç‡å‡åºçš„æŠ˜çº¿å›¾
        ranks = list(range(1, len(results_by_acc) + 1))
        accs = [r[1] for r in results_by_acc]
        epochs_sorted_for_acc = [r[0] for r in results_by_acc]
        plt.figure()
        plt.plot(ranks, accs, marker="o")
        plt.xlabel("Rank by Accuracy (ascending)")
        plt.ylabel("Accuracy")
        plt.title("Checkpoint Accuracy (sorted by accuracy ascending)")
        if len(ranks) <= 20:
            plt.xticks(ranks, [f"ep{e}" for e in epochs_sorted_for_acc], rotation=45, ha="right")
        plt.grid(True)
        plt.tight_layout()
        png_path_sorted = model_save_folder / "search_checkpoints_accuracy_sorted.png"
        plt.savefig(png_path_sorted, dpi=150)
        plt.close()
        print(f"ğŸ–¼ï¸ å·²ä¿å­˜æ›²çº¿å›¾ï¼ˆæŒ‰å‡†ç¡®ç‡å‡åºï¼‰ï¼š{png_path_sorted}")
    except Exception as e:
        print(f"âš ï¸ ç”»å›¾å¤±è´¥ï¼š{e}")


# ==========================
# main å…¥å£
# ==========================

def parse_args():
    parser = argparse.ArgumentParser(description="è¿è¡Œ ion_detect æ¨¡å‹è®­ç»ƒ / æµ‹è¯• / æœç´¢è„šæœ¬")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--train", action="store_true", help="è¿è¡Œè®­ç»ƒæ¨¡å¼")
    group.add_argument("--test", action="store_true", help="è¿è¡Œæµ‹è¯•æ¨¡å¼")
    group.add_argument("--search", action="store_true", help="éå†ç›®å½•ä¸­çš„æ‰€æœ‰ epoch æƒé‡å¹¶è¯„æµ‹")
    return parser.parse_args()


def main():
    args = parse_args()
    device, device_str = build_device()
    paths = build_paths()
    num_time_points = NUM_TIME_POINTS
    num_freq_points = NUM_FREQ_POINTS

    # è¯»å–æ ‡ç­¾æ˜ å°„ï¼Œå¹¶æ®æ­¤ + æ–‡ä»¶åè§„åˆ™ï¼Œè‡ªåŠ¨åˆ’åˆ†æµ‹è¯•æ ·æœ¬
    label_mapping = load_label_mapping(paths["json_path"])
    prepare_test_folder(paths, label_mapping,num_time_points, num_freq_points)

    print(f"âœ… å½“å‰ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ“‚ è®­ç»ƒé›†ç›®å½•: {paths['data_folder']}")
    print(f"ğŸ“‚ æµ‹è¯•/éªŒè¯é›†ç›®å½•: {paths['test_folder_path']}")

    if args.train:
        run_train(device, paths)
    elif args.test:
        run_test(device_str, paths)
    elif args.search:
        run_search(device_str, paths)
    else:
        # ç†è®ºä¸Šä¸ä¼šåˆ°è¿™é‡Œï¼Œå› ä¸ºäº’æ–¥ç»„ required=True
        raise ValueError("å¿…é¡»æŒ‡å®š --train / --test / --search ä¹‹ä¸€")


if __name__ == "__main__":
    main()
