import os
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import pandas as pd
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
class BaseTrainer:
    def __init__(self, model, optimizer, device, model_save_folder, scheduler=None, save_every=100):
        self.model = model.to(device)
        self.optimizer = optimizer
        self.device = device
        self.scheduler = scheduler
        self.model_save_folder = model_save_folder
        self.save_every = save_every
        os.makedirs(self.model_save_folder, exist_ok=True)

        # è®°å½•å„ç±»æŸå¤±çš„å†å²ï¼ˆç”¨äºä¿å­˜ csv / ç”»å›¾ï¼‰
        self.loss_history = {}  # { "total": [], "ce": [], "voltage": [] }

        # ====== TensorBoard æ—¥å¿—å™¨ ======
        # æ—¥å¿—ç›®å½•ï¼š<model_save_folder>/runs
        self.log_dir = os.path.join(self.model_save_folder, "runs")
        os.makedirs(self.log_dir, exist_ok=True)

        # SummaryWriter ç”¨äºå†™å…¥ TensorBoard æ—¥å¿—
        self.writer = SummaryWriter(log_dir=self.log_dir)

        # è¿™é‡Œç”¨ global_step åšâ€œæ¨ªè½´â€ï¼Œå½“å‰è®¾è®¡æ˜¯ï¼šæ¯ä¸ª epoch è°ƒä¸€æ¬¡ _record_lossï¼Œstep++
        self.global_step = 0

    def compute_loss(self, batch):
        """
        å­ç±»å¿…é¡»é‡å†™è¯¥å‡½æ•°
        è¾“å…¥: batch -> (volt_data, impe_data, env_params, true_labels, true_voltages)
        è¾“å‡º: loss, loss_items(dict)
        """
        raise NotImplementedError

    def _record_loss(self, loss_items):
        """
        è®°å½•ä¸€ä¸ª epoch çš„å¹³å‡æŸå¤±åˆ°å†…å­˜ï¼Œå¹¶åŒæ­¥å†™å…¥ TensorBoardã€‚
        loss_items: dictï¼Œæ¯”å¦‚ {"total": 1.23, "voltage": 0.45, ...}
        """
        # è®¤ä¸ºæ¯è°ƒç”¨ä¸€æ¬¡ _record_loss å°±æ˜¯ä¸€ä¸ªâ€œå…¨å±€ stepâ€ï¼ˆé€šå¸¸å¯¹åº”ä¸€ä¸ª epochï¼‰
        self.global_step += 1

        for k, v in loss_items.items():
            # 1) ä¿å­˜åœ¨æœ¬åœ° historyï¼ˆåé¢ç”»å›¾ / å­˜ csv ç”¨ï¼‰
            if k not in self.loss_history:
                self.loss_history[k] = []
            self.loss_history[k].append(v)

            # 2) å†™å…¥ TensorBoard
            if self.writer is not None:
                # æ›²çº¿åç§°ç»Ÿä¸€åŠ ä¸Šå‰ç¼€ "loss/"
                self.writer.add_scalar(f"loss/{k}", v, self.global_step)


    def train(self, train_loader, num_epochs):
        for epoch in range(num_epochs):
            self.model.train()
            running_losses = {}

            for batch in train_loader:
                batch = [b.to(self.device) if torch.is_tensor(b) else b for b in batch]
                loss, loss_items = self.compute_loss(batch)

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                # ç´¯åŠ  batch æŸå¤±
                for k, v in loss_items.items():
                    running_losses[k] = running_losses.get(k, 0) + v

            # è®°å½• epoch å¹³å‡æŸå¤±
            avg_losses = {k: v / len(train_loader) for k, v in running_losses.items()}
            self._record_loss(avg_losses)

            # é¢å¤–æŠŠå½“å‰å­¦ä¹ ç‡å†™è¿› TensorBoard
            if self.writer is not None:
                lr = self.optimizer.param_groups[0]["lr"]
                self.writer.add_scalar("lr", lr, self.global_step)

            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler is not None:
                self.scheduler.step()


            # ä¿å­˜æ¨¡å‹å’ŒæŸå¤±æ›²çº¿
            if (epoch + 1) % self.save_every == 0:
                self.save_model(epoch+1)
                self.save_losses(epoch+1)

        # è®­ç»ƒå®Œæˆåä¿å­˜æœ€ç»ˆæ¨¡å‹ä¸æ›²çº¿
        self.save_model("final")
        self.save_losses("final")

    def save_model(self, epoch):
        save_path = os.path.join(self.model_save_folder, f"trained_model_epoch_{epoch}.pth")
        torch.save(self.model.state_dict(), save_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {save_path}")

    def save_losses(self, epoch):
        # ä¿å­˜ CSV
        csv_path = os.path.join(self.model_save_folder, f"training_loss_epoch_{epoch}.csv")
        pd.DataFrame(self.loss_history).to_csv(csv_path, index=False)
        print(f"ğŸ“ æŸå¤±å€¼å·²ä¿å­˜åˆ° {csv_path}")

        # ç»˜åˆ¶æŸå¤±æ›²çº¿
        plt.figure()
        for k, v in self.loss_history.items():
            plt.plot(v, label=f"{k} Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("Training Loss Curves")
        plt.legend()
        plt.savefig(os.path.join(self.model_save_folder, f"loss_epoch_{epoch}.png"))
        plt.close()


# ================== ç¤ºä¾‹å­ç±» ==================

class Trainer_MLP_Only(BaseTrainer):
    def __init__(self, model, optimizer, device, model_save_folder, scheduler=None, class_weights=None, save_every=100):
        super().__init__(model, optimizer, device, model_save_folder, scheduler, save_every)
        self.ce_loss_fn = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)

    def compute_loss(self, batch):
        volt_data, impe_data, env_params, labels, _ = batch
        logits, raw_logits = self.model(volt_data, impe_data, env_params=None)
        loss = self.ce_loss_fn(raw_logits, labels)
        return loss, {"total": loss.item(), "ce": loss.item()}


class Trainer_Voltage_CE(BaseTrainer):
    def __init__(self, model, optimizer, device, model_save_folder, alpha=1.0, beta=1.0, save_every=100):
        super().__init__(model, optimizer, device, model_save_folder, save_every=save_every)
        self.alpha = alpha
        self.beta = beta
        self.ce_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
        self.mse_loss_fn = nn.MSELoss()

    def compute_loss(self, batch):
        volt_data, impe_data, env_params, labels, true_voltages = batch
        prob_output, predicted_voltage, _, _, raw_prob_output, _ = self.model(volt_data, impe_data, env_params)
        loss_voltage = self.mse_loss_fn(predicted_voltage, true_voltages)
        loss_ce = self.ce_loss_fn(raw_prob_output, labels)
        total_loss = self.alpha * loss_voltage + self.beta * loss_ce
        return total_loss, {
            "total": total_loss.item(),
            "voltage": loss_voltage.item(),
            "ce": loss_ce.item()
        }

# è®¡ç®—æ³¨æ„åŠ›é›†ä¸­æŸå¤±
def compute_attention_focus_loss(freq_attn, top_k=3):
    """
    è®¡ç®—æ³¨æ„åŠ›é›†ä¸­æŸå¤±ï¼Œé¼“åŠ±æ³¨æ„åŠ›é›†ä¸­åœ¨å‰ top_k ä¸ªé¢‘ç‡ç‚¹
    freq_attn: [B, T, F] æˆ– [B, F]
    """
    if freq_attn is None:
        return torch.tensor(0.0, device='cuda' if torch.cuda.is_available() else 'cpu')

    # å¤„ç†æ—¶é—´ç»´åº¦
    if freq_attn.dim() == 3:
        freq_attn = freq_attn.mean(dim=1)  # [B, F]
    freq_attn = torch.softmax(freq_attn, dim=-1)

    topk_values, _ = torch.topk(freq_attn, k=min(top_k, freq_attn.shape[-1]), dim=-1)
    focus_loss = 1.0 - topk_values.sum(dim=-1).mean()
    return focus_loss


class Trainer_ThreeSystem_plus(BaseTrainer):
    def __init__(self, model, optimizer, device, model_save_folder, alpha=1.0, beta=1.0, gamma=0.1, save_every=100):
        """
        ä¸‰ç³»ç»Ÿè®­ç»ƒå™¨
        Args:
            model: PyTorchæ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            device: torch.device
            model_save_folder: ä¿å­˜è·¯å¾„
            alpha: ç”µå‹é¢„æµ‹æŸå¤±æƒé‡
            beta: åˆ†ç±»æŸå¤±æƒé‡
            gamma: æ³¨æ„åŠ›é›†ä¸­æŸå¤±æƒé‡
            save_every: æ¯éš”å¤šå°‘epochä¿å­˜ä¸€æ¬¡
        """
        super().__init__(model, optimizer, device, model_save_folder, save_every=save_every)
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.ce_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
        self.mse_loss_fn = nn.MSELoss()

    def compute_loss(self, batch):
        """
        è®¡ç®—æ€»æŸå¤± = Î± * ç”µå‹æŸå¤± + Î² * åˆ†ç±»æŸå¤± + Î³ * æ³¨æ„åŠ›é›†ä¸­æŸå¤±
        """
        volt_data, impe_data, env_params, true_labels, true_voltages, electrolyzer_parameters,concentration = batch
        outputs = self.model(volt_data, impe_data, env_params,electrolyzer_parameters,concentration)
        # è§£åŒ…æ¨¡å‹è¾“å‡º
        (prob_output, predicted_voltage, wuxing,cls_attn_mean ,raw_prob_output,influence,param_attn_mean,freq_attn,freq_attn_param)= outputs


        # 1ï¸âƒ£ ç”µå‹æŸå¤±
        # å»æ‰å¤šä½™çš„ç»´åº¦ï¼Œä¿è¯ shape ä¸€è‡´
        # predicted_voltage = predicted_voltage.squeeze(-1)  # [8, 1]
        true_voltages = true_voltages.squeeze(-1)          # [8, 1]

        loss_voltage = self.mse_loss_fn(predicted_voltage, true_voltages)

        # 2ï¸âƒ£ åˆ†ç±»æŸå¤±
        loss_ce = self.ce_loss_fn(raw_prob_output, true_labels)

        # # 3ï¸âƒ£ æ³¨æ„åŠ›é›†ä¸­æŸå¤±
        # loss_attn1 = compute_attention_focus_loss(freq_attn, top_k=3)
        # loss_attn2 = compute_attention_focus_loss(freq_attn_param, top_k=3)
        # loss_attn = (loss_attn1 + loss_attn2) / 2.0

        # æ€»æŸå¤±
        total_loss = self.alpha * loss_voltage + self.beta * loss_ce 
        # total_loss = self.alpha * loss_voltage + self.beta * loss_ce + self.gamma * loss_attn

        return total_loss, {
            "total": total_loss.item(),
            "voltage": loss_voltage.item(),
            "ce": loss_ce.item(),
            # "attn": loss_attn.item()
        }
    

class Trainer_ThreeSystem(BaseTrainer):
    def __init__(self, model, optimizer, device, model_save_folder, alpha=1.0, beta=1.0, gamma=0.1, save_every=100):
        """
        ä¸‰ç³»ç»Ÿè®­ç»ƒå™¨
        Args:
            model: PyTorchæ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            device: torch.device
            model_save_folder: ä¿å­˜è·¯å¾„
            alpha: ç”µå‹é¢„æµ‹æŸå¤±æƒé‡
            beta: åˆ†ç±»æŸå¤±æƒé‡
            gamma: æ³¨æ„åŠ›é›†ä¸­æŸå¤±æƒé‡
            save_every: æ¯éš”å¤šå°‘epochä¿å­˜ä¸€æ¬¡
        """
        super().__init__(model, optimizer, device, model_save_folder, save_every=save_every)
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.ce_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
        self.mse_loss_fn = nn.MSELoss()

    def compute_loss(self, batch):
        """
        è®¡ç®—æ€»æŸå¤± = Î± * ç”µå‹æŸå¤± + Î² * åˆ†ç±»æŸå¤± + Î³ * æ³¨æ„åŠ›é›†ä¸­æŸå¤±
        """
        volt_data, impe_data, env_params, true_labels, true_voltages, concentrations= batch
        outputs = self.model(volt_data, impe_data, env_params,concentrations)
        # è§£åŒ…æ¨¡å‹è¾“å‡º
        (prob_output, predicted_voltage, wuxing,cls_attn_mean ,raw_prob_output,influence,param_attn_mean,freq_attn,freq_attn_param)= outputs


        # 1ï¸âƒ£ ç”µå‹æŸå¤±
        # å»æ‰å¤šä½™çš„ç»´åº¦ï¼Œä¿è¯ shape ä¸€è‡´
        # predicted_voltage = predicted_voltage.squeeze(-1)  # [8, 1]
        true_voltages = true_voltages.squeeze(-1)          # [8, 1]

        loss_voltage = self.mse_loss_fn(predicted_voltage, true_voltages)

        # 2ï¸âƒ£ åˆ†ç±»æŸå¤±
        loss_ce = self.ce_loss_fn(raw_prob_output, true_labels)

        # # 3ï¸âƒ£ æ³¨æ„åŠ›é›†ä¸­æŸå¤±
        # loss_attn1 = compute_attention_focus_loss(freq_attn, top_k=3)
        # loss_attn2 = compute_attention_focus_loss(freq_attn_param, top_k=3)
        # loss_attn = (loss_attn1 + loss_attn2) / 2.0

        # æ€»æŸå¤±
        total_loss = self.alpha * loss_voltage + self.beta * loss_ce 
        # total_loss = self.alpha * loss_voltage + self.beta * loss_ce + self.gamma * loss_attn

        return total_loss, {
            "total": total_loss.item(),
            "voltage": loss_voltage.item(),
            "ce": loss_ce.item(),
            # "attn": loss_attn.item()
        }


class Trainer_ThreeSystem_1004(BaseTrainer):
    def __init__(self, model, optimizer, device, model_save_folder, alpha=1.0, beta=1.0, gamma=0.1, save_every=100):
        """
        ä¸‰ç³»ç»Ÿè®­ç»ƒå™¨
        Args:
            model: PyTorchæ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            device: torch.device
            model_save_folder: ä¿å­˜è·¯å¾„
            alpha: ç”µå‹é¢„æµ‹æŸå¤±æƒé‡
            beta: åˆ†ç±»æŸå¤±æƒé‡
            gamma: æ³¨æ„åŠ›é›†ä¸­æŸå¤±æƒé‡
            save_every: æ¯éš”å¤šå°‘epochä¿å­˜ä¸€æ¬¡
        """
        super().__init__(model, optimizer, device, model_save_folder, save_every=save_every)
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.classify_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
        self.mse_loss_fn = nn.MSELoss()

    def compute_loss(self, batch):
        """
        è®¡ç®—æ€»æŸå¤± = Î± * ç”µå‹æŸå¤± + Î² * åˆ†ç±»æŸå¤± + Î³ * æ³¨æ„åŠ›é›†ä¸­æŸå¤±
        """
        volt_data, impe_data, env_params, true_labels, true_voltages, electrolyzer_parameters,concentration = batch
        outputs = self.model(volt_data, impe_data, env_params,electrolyzer_parameters,concentration)
        # è§£åŒ…æ¨¡å‹è¾“å‡º
        (prob_output, predicted_voltage, conc_pred,wuxing,cls_attn_mean ,raw_prob_output,influence,param_attn_mean,freq_attn,freq_attn_param)= outputs


        # 1ï¸âƒ£ ç”µå‹æŸå¤±
        # å»æ‰å¤šä½™çš„ç»´åº¦ï¼Œä¿è¯ shape ä¸€è‡´
        # predicted_voltage = predicted_voltage.squeeze(-1)  # [8, 1]
        true_voltages = true_voltages.squeeze(-1)          # [8, 1]

        loss_voltage = self.mse_loss_fn(predicted_voltage, true_voltages)

        # 2ï¸âƒ£ åˆ†ç±»æŸå¤±
        loss_classify = self.classify_loss_fn(raw_prob_output, true_labels)

        # # 3ï¸âƒ£ æ³¨æ„åŠ›é›†ä¸­æŸå¤±
        # loss_attn1 = compute_attention_focus_loss(freq_attn, top_k=3)
        # loss_attn2 = compute_attention_focus_loss(freq_attn_param, top_k=3)
        # loss_attn = (loss_attn1 + loss_attn2) / 2.0

        #æµ“åº¦æŸå¤±
        # å»æ‰å¤šä½™çš„ç»´åº¦ï¼Œä¿è¯ shape ä¸€è‡´
        # print("concentration.shape:",concentration.shape)
        # print("conc_pred.shape:",conc_pred.shape)
        loss_conc = self.mse_loss_fn(conc_pred, concentration)

        # 2ï¸âƒ£ åˆ†ç±»æŸå¤±
        loss_classify = self.classify_loss_fn(raw_prob_output, true_labels)

        # æ€»æŸå¤±
        total_loss = self.alpha * loss_voltage + self.beta * loss_classify +self.gamma*loss_conc
        # total_loss = self.alpha * loss_voltage + self.beta * loss_classify + self.gamma * loss_attn

        return total_loss, {
            "total": total_loss.item(),
            "voltage": loss_voltage.item(),
            "classify": loss_classify.item(),
            "conc":loss_conc.item(),
            # "attn": loss_attn.item()
        }
    # åœ¨ Trainer_ThreeSystem_1004 ç±»é‡Œæ–°å¢ï¼š
    # åœ¨ model_train.py çš„ Trainer_ThreeSystem_1004 ç±»ä¸­æ–°å¢ï¼š
    # model_train.py ï¼ˆä»…å±•ç¤º compute_loss_pairs çš„å‡½æ•°ç­¾åä¸ä¸€è‡´æ€§å¤„çš„ä¿®æ”¹ï¼‰
    #volt_data, impe_data, env_param, label, true_voltage, electrolyzer_parameters,concentration
    def compute_loss_pairs(self, batchA, batchB, dummy_mask=None,
                        lambda_consistency=1.0, eps=1e-9, use_log_space=True,
                        lambda_polarity = 0.5,lambda_monodec = 0.2):
        """
        dummy_mask: [B] çš„ bool å¼ é‡ã€‚True è¡¨ç¤ºâ€œè‡ªé…å¯¹â€ï¼Œä¸€è‡´æ€§æŸå¤±åº”å±è”½ã€‚
        """

        # ------- å‰å‘ A / Bï¼šæ³¨æ„ä¼  5 ä¸ªè¾“å…¥ -------
        (probA, predVA, concA, wuxingA, clsA, rawProbA, influenceA, *_ ) = \
            self.model(batchA[0].to(self.device),
                    batchA[1].to(self.device),
                    batchA[2].to(self.device),
                    batchA[5].to(self.device),
                    batchA[6].to(self.device))

        (probB, predVB, concB, wuxingB, clsB, rawProbB, influenceB, *_ ) = \
            self.model(batchB[0].to(self.device),
                    batchB[1].to(self.device),
                    batchB[2].to(self.device),
                    batchB[5].to(self.device),
                    batchB[6].to(self.device))

        # ------- åŸæœ‰ç”µå‹/åˆ†ç±»/æµ“åº¦æŸå¤±ï¼ˆä¸å˜ï¼‰ -------
        loss_voltageA = self.mse_loss_fn(predVA, batchA[4].to(self.device).squeeze(-1))
        loss_classA   = self.classify_loss_fn(rawProbA, batchA[3].to(self.device))
        loss_concA    = self.mse_loss_fn(concA, batchA[6].to(self.device))

        loss_voltageB = self.mse_loss_fn(predVB, batchB[4].to(self.device).squeeze(-1))
        loss_classB   = self.classify_loss_fn(rawProbB, batchB[3].to(self.device))
        loss_concB    = self.mse_loss_fn(concB, batchB[6].to(self.device))

        loss_base = self.alpha*(loss_voltageA+loss_voltageB) + \
                    self.beta *(loss_classA  +loss_classB  ) + \
                    self.gamma*(loss_concA   +loss_concB   )

        # ------- ä¸€è‡´æ€§æŸå¤±ï¼ˆå¸¦ mask çš„é€æ ·æœ¬è®¡ç®—ï¼‰ -------
        def stack_params(wuxing):
            sigma_mem, alpha_ca, alpha_an, i0_ca, i0_an = wuxing
            return torch.cat([sigma_mem, alpha_ca, alpha_an, i0_ca, i0_an], dim=1)  # (B,5)

        def stack_infl(influence):
            psi, theta_ca, theta_an, phi_ca, phi_an = influence
            return torch.cat([psi, theta_ca, theta_an, phi_ca, phi_an], dim=1)     # (B,5)

        pA = stack_params(wuxingA)    # (B,5)
        mA = stack_infl(influenceA)   # (B,5)
        pB = stack_params(wuxingB)    # (B,5)

        psi, theta_ca, theta_an, phi_ca, phi_an = mA.chunk(5, dim=1)
        sigma, a_ca, a_an, i0_ca, i0_an = pA.chunk(5, dim=1)

        if dummy_mask is None:
            dummy_mask = torch.zeros(pA.size(0), dtype=torch.bool, device=pA.device)

        if use_log_space:
            def clamp01(x):  # é™åˆ¶åˆ° (eps, 1-eps) é˜² log(0) å’Œ log(è´Ÿ)
                return torch.clamp(x, min=eps, max=1.0 - eps)
            L = lambda x: torch.log(torch.clamp(x, min=eps))

            lhs = torch.cat([
                L(sigma)  + L(clamp01(1.0 - psi)),
                L(a_ca)   + L(clamp01(theta_ca)),
                L(a_an)   + L(clamp01(theta_an)),
                L(i0_ca)  + L(clamp01(1.0 - phi_ca)),
                L(i0_an)  + L(clamp01(1.0 - phi_an)),
            ], dim=1)
            rhs = torch.log(torch.clamp(pB, min=eps))

            # é€æ ·æœ¬ MSEï¼ˆfeature ç»´åº¦å–å‡å€¼ï¼‰ï¼Œå†ç”¨ mask åšæ ·æœ¬çº§å¹³å‡
            per_sample = ((lhs - rhs) ** 2).mean(dim=1)  # [B]
        else:
            pA_next_pred = torch.cat([
                sigma  * (1.0 - psi),
                a_ca   * theta_ca,
                a_an   * theta_an,
                i0_ca  * (1.0 - phi_ca),
                i0_an  * (1.0 - phi_an),
            ], dim=1)
            per_sample = ((pA_next_pred - pB) ** 2).mean(dim=1)  # [B]

        valid = (~dummy_mask).float()  # Trueâ†’0, Falseâ†’1 ä¹‹å‰å…ˆå–å
        denom = torch.clamp(valid.sum(), min=1.0)  # é˜²å…¨æ˜¯ dummy
        loss_consistency = (per_sample * valid).sum() / denom



        total = loss_base + lambda_consistency * loss_consistency
        
        # === äº¤æ¢ç”µæµâ€œæåˆ«å¿«æ…¢â€å…ˆéªŒï¼šlog(i0_ca) - log(i0_an) â‰¥ m ===
        m_margin = 2.0   # å»ºè®® 1.5~3 è¯•éªŒï¼›è¡¨ç¤ºå¸Œæœ›è‡³å°‘ ~10^m æ•°é‡çº§å·®
        eps = 1e-12

        i0_ca = pA[:, [3]]  # ä½ å·²æœ‰ pA = [Ïƒ, Î±_ca, Î±_an, i0_ca, i0_an]
        i0_an = pA[:, [4]]

        delta = torch.log(torch.clamp(i0_ca, min=eps)) - torch.log(torch.clamp(i0_an, min=eps))
        loss_i0_polarity = torch.relu(m_margin - delta).mean()
        total += lambda_polarity * loss_i0_polarity
        # ====== å•è°ƒä¸‹é™çº¦æŸï¼šè¦æ±‚ B çš„ç‰©æ€§ â‰¤ A ======
        # åªå¯¹â€œæœ‰æ•ˆç›¸é‚»å¯¹â€ï¼ˆé dummyï¼‰æ–½åŠ 
        valid_mask = (~dummy_mask)

        def mono_decrease_penalty(xA, xB, mask, slack=0.0, eps=1e-12, use_log=True):
            """
            ç›®æ ‡ï¼šxB <= xAï¼ˆå…è®¸ slackï¼‰ï¼Œå¦åˆ™æƒ©ç½šã€‚
            use_log=True æ—¶ï¼šæƒ©ç½š ReLU( log(xB) - log(xA) - slack )
            """
            if not mask.any():
                return torch.tensor(0.0, device=xA.device)
            if use_log:
                lA = torch.log(torch.clamp(xA[mask], min=eps))
                lB = torch.log(torch.clamp(xB[mask], min=eps))
                return torch.relu(lB - lA - slack).mean()
            else:
                return torch.relu(xB[mask] - xA[mask] - slack).mean()

        # pA/pB åˆ—é¡ºåºï¼š0=Ïƒ_mem, 1=Î±_ca, 2=Î±_an, 3=i0_ca, 4=i0_an
        # å»ºè®®ï¼šÏƒã€i0 ç”¨ log åŸŸï¼›Î± ä¹Ÿå¯ç”¨ log åŸŸï¼ˆåœ¨(0,1)å†…æ›´ç¨³ï¼‰ï¼Œç»™ä¸€ç‚¹æ¾å¼›
        slack_sigma = 0.0
        slack_alpha = 0.01   # ç»™å¾ˆå°æ¾å¼›é¿å…æµ®ç‚¹å™ªå£°
        slack_i0    = 0.0

        loss_sigma_dec = mono_decrease_penalty(pA[:, [0]], pB[:, [0]], valid_mask, slack=slack_sigma, use_log=True)
        loss_alpha_ca_dec = mono_decrease_penalty(pA[:, [1]], pB[:, [1]], valid_mask, slack=slack_alpha, use_log=True)
        loss_alpha_an_dec = mono_decrease_penalty(pA[:, [2]], pB[:, [2]], valid_mask, slack=slack_alpha, use_log=True)
        loss_i0_ca_dec = mono_decrease_penalty(pA[:, [3]], pB[:, [3]], valid_mask, slack=slack_i0,    use_log=True)
        loss_i0_an_dec = mono_decrease_penalty(pA[:, [4]], pB[:, [4]], valid_mask, slack=slack_i0,    use_log=True)
        loss_wuxing_diff = loss_sigma_dec + loss_alpha_ca_dec + loss_alpha_an_dec + loss_i0_ca_dec + loss_i0_an_dec

          # å…ˆå°ç‚¹ï¼Œè®­ç»ƒç¨³å®šåå†è°ƒ
        total = total + lambda_monodec * loss_wuxing_diff


        return total, {
            "total": total.item(),
            "voltage": (loss_voltageA+loss_voltageB).item(),
            "classify": (loss_classA+loss_classB).item(),
            "conc": (loss_concA+loss_concB).item(),
            "consistency": loss_consistency.item(),
            "consist_valid": denom.item(),  # å‚ä¸ä¸€è‡´æ€§çš„æ ·æœ¬æ•°
            "consist_dummy": dummy_mask.float().sum().item(),
            "loss_i0_polarity":loss_i0_polarity.item(),
            "loss_wuxing_diff":loss_wuxing_diff.item(),
        }

    # === åŠ åˆ° Trainer_ThreeSystem_1004 ç±»ä¸­ï¼ˆä¸ compute_loss/compute_loss_pairs åŒçº§ï¼‰ ===
    def train_pairs(self, train_loader, num_epochs, lambda_consistency=1.0, eps=1e-9, use_log_space=True,lambda_monodec = 0.2,lambda_polarity = 0.5):
        """
        ä½¿ç”¨æˆå¯¹æ•°æ® (A,B) çš„è®­ç»ƒå¾ªç¯ã€‚è°ƒç”¨ compute_loss_pairsã€‚
        ä¸å½±å“åŸæœ¬çš„å•æ ·æœ¬ train()ã€‚
        """
        for epoch in range(num_epochs):
            self.model.train()
            running = {}

            for batchA, batchB, dummy_mask in train_loader:
                batchA = [b.to(self.device) if hasattr(b, "to") and callable(b.to) else b for b in batchA]
                batchB = [b.to(self.device) if hasattr(b, "to") and callable(b.to) else b for b in batchB]
                dummy_mask = dummy_mask.to(self.device)

                loss, items = self.compute_loss_pairs(
                    batchA, batchB, dummy_mask=dummy_mask,
                    lambda_consistency=lambda_consistency, eps=1e-9, use_log_space=True,
                    lambda_polarity = lambda_monodec, lambda_monodec = lambda_monodec,
                )

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                # ç»Ÿè®¡
                for k, v in items.items():
                    running[k] = running.get(k, 0.0) + v

            # Epoch-wise average
            avg = {k: v / len(train_loader) for k, v in running.items()}
            self._record_loss(avg)

            # è®°å½•å­¦ä¹ ç‡
            if self.writer is not None:
                lr = self.optimizer.param_groups[0]["lr"]
                self.writer.add_scalar("lr", lr, self.global_step)

            # Optional scheduler step
            if self.scheduler is not None:
                self.scheduler.step()


            # æ‰“å°
            msg = " | ".join([f"{k}: {v:.4f}" for k, v in avg.items()])
            print(f"[Pairs] Epoch {epoch+1}/{num_epochs} | {msg}")

            # ï¼ˆæŒ‰ä½ çš„ save_every é€»è¾‘ä¿å­˜ï¼‰
            if (epoch + 1) % self.save_every == 0:
                self.save_model(f"{epoch+1}")
                self.save_losses(f"{epoch+1}")

        # ç»“å°¾ä¿å­˜
        self.save_model("final")
        self.save_losses("final")






class Trainer_MLP_ClassifierOnly(BaseTrainer):
    def __init__(self,
                 model,
                 optimizer,
                 device,
                 model_save_folder,
                 beta: float = 1.0,
                 save_every: int = 100,
                 label_smoothing: float = 0.1,
                 class_weights: torch.Tensor | None = None,
                 ):
        """
        çº¯åˆ†ç±»è®­ç»ƒå™¨ï¼ˆåªä¼˜åŒ–åˆ†ç±»äº¤å‰ç†µï¼‰
        Args:
            model: PyTorch æ¨¡å‹ï¼ˆè¿”å› logitsï¼‰
            optimizer: ä¼˜åŒ–å™¨
            device: torch.device
            model_save_folder: ä¿å­˜ç›®å½•
            beta: åˆ†ç±»æŸå¤±ç³»æ•°ï¼ˆé»˜è®¤ 1.0ï¼Œç•™å£å­ä¾¿äºå’Œæ—§è„šæœ¬å¯¹é½ï¼‰
            save_every: æ¯å¤šå°‘ä¸ª epoch ä¿å­˜ä¸€æ¬¡
            label_smoothing: CE çš„æ ‡ç­¾å¹³æ»‘ç³»æ•°
            class_weights: æŒ‰ç±»åˆ«åŠ æƒçš„å¼ é‡ï¼Œå½¢å¦‚ (num_classes,)
        """
        super().__init__(model, optimizer, device, model_save_folder, save_every=save_every)
        self.beta = beta
        self.ce_loss_fn = nn.CrossEntropyLoss(
            weight=class_weights.to(device) if class_weights is not None else None,
            label_smoothing=label_smoothing
        )

    # ---- å¯é€‰ï¼šç¨³å¥åœ°ä»ä¸åŒç‰ˆæœ¬çš„ batch ä¸­æå–éœ€è¦çš„ 6 ä¸ªå­—æ®µ ----
    @staticmethod
    def _unpack_batch_for_classification(batch):
        """
        æœŸæœ›é¡ºåºï¼ˆæ¨èï¼‰:
            (volt, impe, env, labels, ep, conc)
        å…¼å®¹æ—§é¡ºåº:
            (volt, impe, env, labels, true_voltages, ep, conc)
        ä»¥åŠä»»ä½•åŒ…å«ä¸Šè¿°å­—æ®µä¸”é•¿åº¦>=6çš„æƒ…å†µï¼ˆæˆ‘ä»¬åªå–éœ€è¦çš„6ä¸ªï¼‰ã€‚
        """
        if len(batch) >= 7:
            # æ—§ç®¡çº¿: (0:volt,1:impe,2:env,3:labels,4:true_voltages,5:ep,6:conc)
            volt_data, impe_data, env_params, labels, _, ep, conc = batch[:7]
        elif len(batch) == 6:
            # æ–°ç®¡çº¿: (0:volt,1:impe,2:env,3:labels,4:ep,5:conc)
            volt_data, impe_data, env_params, labels, ep, conc = batch
        else:
            raise ValueError(
                f"Batch length must be 6 or 7 for classification-only, but got {len(batch)}."
            )
        return volt_data, impe_data, env_params, labels, ep, conc

    def compute_loss(self, batch):
        """
        åªè®¡ç®—åˆ†ç±»æŸå¤±ï¼š
            total = Î² * CE(logits, labels)
        è¿”å›:
            total_loss, {"total":..., "ce":..., "acc":...}
        """
        volt_data, impe_data, env_params, true_labels, electrolyzer_parameters, concentration = \
            self._unpack_batch_for_classification(batch)

        # ---- æ”¾åˆ° deviceï¼ˆå¦‚æœä¸Šæ¸¸ DataLoader æ²¡åšï¼‰----
        volt_data = volt_data.to(self.device)
        impe_data = impe_data.to(self.device)
        env_params = env_params.to(self.device)
        true_labels = true_labels.to(self.device).long()
        electrolyzer_parameters = electrolyzer_parameters.to(self.device)
        concentration = concentration.to(self.device)

        # ---- å‰å‘ï¼šæ¨¡å‹åªéœ€è¾“å‡º logits ----
        logits = self.model(
            volt_data, impe_data, env_params,
            electrolyzer_parameters, concentration
        )  # (B, num_classes)

        # ---- åˆ†ç±»æŸå¤± ----
        loss_ce = self.ce_loss_fn(logits, true_labels)
        total_loss = self.beta * loss_ce

        # ---- ç®€å•æŒ‡æ ‡ï¼štop-1 å‡†ç¡®ç‡ ----
        with torch.no_grad():
            pred = logits.argmax(dim=1)
            acc = (pred == true_labels).float().mean().item()

        return total_loss, {
            "total": total_loss.item(),
            "ce": loss_ce.item(),
            "acc": acc,
        }


class Trainer_ThreeSystem_1117(BaseTrainer):
    """
    Trainer_ThreeSystem_1117

    Pairwise training for the three-system model with:
    - voltage regression (physics-based voltage),
    - ion-type classification,
    - concentration regression,
    - rule-space prototype regression,
    - hierarchical group / sub-class classification (decision-tree like),
    - consistency constraints between paired samples (A, B),
    - monotonic decay constraints on physical parameters across time,
    - polarity constraints between anode / cathode exchange current densities.

    Total loss (per batch of pairs) is:

        L_total =
            Î± * (voltage loss for A + B)
          + Î² * (classification loss for A + B)
          + Î³ * (concentration loss for A + B)
          + Î»_consistency * consistency_loss(A â†’ B)
          + Î»_rule        * rule_space_loss(A, B)
          + Î»_group       * group_loss(A, B)
          + Î»_tree        * tree_loss(A, B)
          + Î»_polarity    * polarity_loss(A)
          + Î»_monodec     * monotonicity_loss(A â†’ B)
    """

    def __init__(
        self,
        model,
        optimizer,
        device,
        model_save_folder,
        alpha: float = 1.0,
        beta: float = 1.0,
        gamma: float = 0.1,
        lambda_rule: float = 0.01,
        lambda_group: float = 0.01,
        lambda_tree: float = 0.01,
        save_every: int = 100,
        writer=None,       

    ) -> None:
        """
        Args
        ----
        model : nn.Module
            Three-system model (1117 version), must output:
            prob, predV, conc, wuxing, cls_logits, rawProb, influence,
            paramAttn, freqAttn, freqAttnParam, rule, group_logits,
            high3_logits, low3_logits.
        optimizer : torch.optim.Optimizer
            Optimizer for all model parameters.
        device : torch.device or str
            Device used for training.
        model_save_folder : str or Path
            Directory to save model checkpoints and loss curves.
        alpha, beta, gamma : float
            Weights for voltage, classification, and concentration losses.
        lambda_rule : float
            Weight of rule-space regression loss (towards ion prototypes).
        lambda_group : float
            Weight of binary high/low group classification loss.
        lambda_tree : float
            Weight of intra-group 3-way classification loss (high3 + low3).
        save_every : int
            Save model and loss history every `save_every` epochs.
        """
        super().__init__(model, optimizer, device, model_save_folder, save_every=save_every)

        # Loss weights
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.lambda_rule = lambda_rule
        self.lambda_group = lambda_group
        self.lambda_tree = lambda_tree

        # Classification and regression losses
        self.classify_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
        self.mse_loss_fn = nn.MSELoss()

        # ====== Mapping from label index to ion type (example) ======
        # Assume:
        #   0: Ca2+_ion
        #   1: Na+_ion
        #   2: Fe3+_ion
        #   3: Ni2+_ion
        #   4: Cr3+_ion
        #   5: Cu2+_ion
        #   6: no_ion
        #
        # high_group_ids: ions in "high influence" group
        # low_group_ids : ions in "low influence" group
        # no_ion_id     : background / clean condition
        self.high_group_ids = torch.tensor([0, 1, 3], dtype=torch.long)  # example: Ca, Na, Ni
        self.low_group_ids = torch.tensor([2, 4, 5], dtype=torch.long)   # example: Fe, Cr, Cu
        self.no_ion_id = 6
        self.writer = writer        # âœ… è®°å½•ä¸‹æ¥
        self.global_step = 0        # ç”¨æ¥åœ¨ add_scalar é‡Œåš step

    # =========================================================================
    # Pair-wise training with rule & hierarchical tree structure
    # =========================================================================
    def compute_loss_pairs(
        self,
        batchA,
        batchB,
        dummy_mask=None,
        lambda_consistency: float = 1.0,
        eps: float = 1e-9,
        use_log_space: bool = True,
        lambda_monodec: float = 0.2,
        lambda_polarity: float = 0.5,
    ):
        """
        Compute loss for a pair of samples (A, B).

        Each batch is a tuple/list:
            batchX = [
                volt_data,           # [0] voltage time series (B, T, 1)
                impe_data,           # [1] impedance spectra (B, T, F, 2)
                env_params,          # [2] environment parameters (B, 3) e.g. T, flow, current
                label,               # [3] ion type label (B,)
                true_voltage,        # [4] measured physical voltage (B, T)
                freq_values,         # [5] frequency values (B, F) or similar
                conc_label,          # [6] concentration label (B, ?) e.g. ppm or class
            ]

        Loss components:
        - voltage loss (MSE on predicted vs true voltage)
        - classification loss (ion type)
        - concentration loss
        - rule-space regression towards ion prototypes
        - group (high/low) classification
        - 3-way classification within high / low groups
        - consistency loss: physical parameters from A vs B (time ordering)
        - polarity loss: enforce i0_ca > i0_an (in log space)
        - monotonic decay loss: enforce physical parameters in B <= A (with slack)
        """
        # ------------------------------------------------------------------
        # 1) Forward pass for sample A
        # ------------------------------------------------------------------
        (
            probA,         # final probabilities (after softmax)                 (B, 7)
            predVA,        # predicted voltage sequence                          (B, T)
            concA,         # predicted concentration (regression or class logits)
            wuxingA,       # 5 physical parameters: (sigma_mem, alpha_ca, alpha_an, i0_ca, i0_an)
            clsA,          # not used directly here, reserved
            rawProbA,      # pre-softmax logits for ion classification           (B, 7)
            influenceA,    # 5 behavioral influence factors: (psi, theta_ca, theta_an, phi_ca, phi_an)
            paramAttnA,    # attention over physical parameters (unused here)
            freqAttnA,     # attention over frequencies (unused here)
            freqAttnParamA,# cross attention between freq and params (unused here)
            ruleA,         # rule-space embedding output                          (B, D_rule)
            groupA,        # logits for high/low group (2-way)                    (B, 2)
            high3A,        # logits for 3-way class inside high group             (B, 3)
            low3A,         # logits for 3-way class inside low group              (B, 3)
        ) = self.model(
            batchA[0].to(self.device),  # voltage data
            batchA[1].to(self.device),  # impedance data
            batchA[2].to(self.device),  # environment parameters
            batchA[5].to(self.device),  # frequency values or freq-related input
            batchA[6].to(self.device),  # concentration label / auxiliary
        )

        # ------------------------------------------------------------------
        # 2) Forward pass for sample B
        # ------------------------------------------------------------------
        (
            probB,
            predVB,
            concB,
            wuxingB,
            clsB,
            rawProbB,
            influenceB,
            paramAttnB,
            freqAttnB,
            freqAttnParamB,
            ruleB,
            groupB,
            high3B,
            low3B,
        ) = self.model(
            batchB[0].to(self.device),
            batchB[1].to(self.device),
            batchB[2].to(self.device),
            batchB[5].to(self.device),
            batchB[6].to(self.device),
        )

        # ------------------------------------------------------------------
        # 3) Prepare ground-truth labels / targets
        # ------------------------------------------------------------------
        labelA = batchA[3].to(self.device)                     # ion label for A       (B,)
        labelB = batchB[3].to(self.device)                     # ion label for B       (B,)
        trueVA = batchA[4].to(self.device).squeeze(-1)         # true voltage A        (B, T)
        trueVB = batchB[4].to(self.device).squeeze(-1)         # true voltage B        (B, T)
        conc_trueA = batchA[6].to(self.device)                 # true conc A           (B, ...)
        conc_trueB = batchB[6].to(self.device)                 # true conc B           (B, ...)

        # ------------------------------------------------------------------
        # 4) Base losses: voltage / classification / concentration
        # ------------------------------------------------------------------
        loss_voltageA = self.mse_loss_fn(predVA, trueVA)
        loss_classA = self.classify_loss_fn(rawProbA, labelA)
        loss_concA = self.mse_loss_fn(concA, conc_trueA)

        loss_voltageB = self.mse_loss_fn(predVB, trueVB)
        loss_classB = self.classify_loss_fn(rawProbB, labelB)
        loss_concB = self.mse_loss_fn(concB, conc_trueB)

        loss_base = (
            self.alpha * (loss_voltageA + loss_voltageB)
            + self.beta * (loss_classA + loss_classB)
            + self.gamma * (loss_concA + loss_concB)
        )

        # ------------------------------------------------------------------
        # 5) Rule-space regression loss (A / B)
        #     The model contains pre-defined ion rule prototypes:
        #     ion_rule_proto[label] âˆˆ R^D_rule
        # ------------------------------------------------------------------
        ion_rule_proto = self.model.ion_rule_proto.to(self.device)

        # A: rule-space regression towards prototype of true label
        rule_targetA = ion_rule_proto[labelA]          # (B, D_rule)
        loss_ruleA = self.mse_loss_fn(ruleA, rule_targetA)

        # B: rule-space regression
        rule_targetB = ion_rule_proto[labelB]
        loss_ruleB = self.mse_loss_fn(ruleB, rule_targetB)

        loss_rule = loss_ruleA + loss_ruleB

        # ------------------------------------------------------------------
        # 6) Hierarchical group / sub-class losses (A / B)
        # ------------------------------------------------------------------
        high_ids = self.high_group_ids.to(self.device)   # ion indices in "high" group
        low_ids = self.low_group_ids.to(self.device)     # ion indices in "low" group
        no_ion_id = self.no_ion_id                       # index for "no ion" / background

        # ===== A-side: group (high vs low) classification =====
        # Boolean masks for high / low / valid-group samples
        is_high_A = (labelA.unsqueeze(1) == high_ids.unsqueeze(0)).any(dim=1)
        is_low_A = (labelA.unsqueeze(1) == low_ids.unsqueeze(0)).any(dim=1)
        valid_group_A = (is_high_A | is_low_A) & (labelA != no_ion_id)

        if valid_group_A.any():
            # group_targetsA: 0 for low, 1 for high
            group_targetsA = torch.zeros_like(labelA)
            group_targetsA[is_high_A] = 1
            loss_groupA = self.classify_loss_fn(
                groupA[valid_group_A],               # logits for valid ions
                group_targetsA[valid_group_A],       # high(1)/low(0)
            )
        else:
            loss_groupA = torch.tensor(0.0, device=self.device)

        # ===== A-side: 3-way classification inside high group =====
        if is_high_A.any():
            high_ids_expanded = high_ids.unsqueeze(0)                    # (1, 3)
            labels_highA = labelA[is_high_A].unsqueeze(1)               # (B_high, 1)
            # Find index k such that labels_highA == high_ids[k]
            high_targets3A = (labels_highA == high_ids_expanded).nonzero(as_tuple=False)[:, 1]
            loss_high3A = self.classify_loss_fn(
                high3A[is_high_A],  # (B_high, 3)
                high_targets3A,     # (B_high,)
            )
        else:
            loss_high3A = torch.tensor(0.0, device=self.device)

        # ===== A-side: 3-way classification inside low group =====
        if is_low_A.any():
            low_ids_expanded = low_ids.unsqueeze(0)                      # (1, 3)
            labels_lowA = labelA[is_low_A].unsqueeze(1)                 # (B_low, 1)
            low_targets3A = (labels_lowA == low_ids_expanded).nonzero(as_tuple=False)[:, 1]
            loss_low3A = self.classify_loss_fn(
                low3A[is_low_A],   # (B_low, 3)
                low_targets3A,     # (B_low,)
            )
        else:
            loss_low3A = torch.tensor(0.0, device=self.device)

        # ===== B-side: group (high vs low) classification =====
        is_high_B = (labelB.unsqueeze(1) == high_ids.unsqueeze(0)).any(dim=1)
        is_low_B = (labelB.unsqueeze(1) == low_ids.unsqueeze(0)).any(dim=1)
        valid_group_B = (is_high_B | is_low_B) & (labelB != no_ion_id)

        if valid_group_B.any():
            group_targetsB = torch.zeros_like(labelB)
            group_targetsB[is_high_B] = 1
            loss_groupB = self.classify_loss_fn(
                groupB[valid_group_B],
                group_targetsB[valid_group_B],
            )
        else:
            loss_groupB = torch.tensor(0.0, device=self.device)

        # ===== B-side: 3-way classification inside high group =====
        if is_high_B.any():
            high_ids_expanded = high_ids.unsqueeze(0)
            labels_highB = labelB[is_high_B].unsqueeze(1)
            high_targets3B = (labels_highB == high_ids_expanded).nonzero(as_tuple=False)[:, 1]
            loss_high3B = self.classify_loss_fn(
                high3B[is_high_B],
                high_targets3B,
            )
        else:
            loss_high3B = torch.tensor(0.0, device=self.device)

        # ===== B-side: 3-way classification inside low group =====
        if is_low_B.any():
            low_ids_expanded = low_ids.unsqueeze(0)
            labels_lowB = labelB[is_low_B].unsqueeze(1)
            low_targets3B = (labels_lowB == low_ids_expanded).nonzero(as_tuple=False)[:, 1]
            loss_low3B = self.classify_loss_fn(
                low3B[is_low_B],
                low_targets3B,
            )
        else:
            loss_low3B = torch.tensor(0.0, device=self.device)

        loss_group = loss_groupA + loss_groupB
        loss_tree = loss_high3A + loss_low3A + loss_high3B + loss_low3B

        # ------------------------------------------------------------------
        # 7) Consistency loss between (A, B) on physical parameters
        # ------------------------------------------------------------------
        def stack_params(wuxing):
            """Stack 5 physical parameters into a single tensor: (B, 5)."""
            sigma_mem, alpha_ca, alpha_an, i0_ca, i0_an = wuxing
            return torch.cat([sigma_mem, alpha_ca, alpha_an, i0_ca, i0_an], dim=1)

        def stack_infl(influence):
            """Stack 5 behavioral influence factors into a single tensor: (B, 5)."""
            psi, theta_ca, theta_an, phi_ca, phi_an = influence
            return torch.cat([psi, theta_ca, theta_an, phi_ca, phi_an], dim=1)

        pA = stack_params(wuxingA)       # (B, 5) physical parameters at time A
        mA = stack_infl(influenceA)      # (B, 5) influence factors at time A
        pB = stack_params(wuxingB)       # (B, 5) physical parameters at time B

        psi, theta_ca, theta_an, phi_ca, phi_an = mA.chunk(5, dim=1)
        sigma, a_ca, a_an, i0_ca, i0_an = pA.chunk(5, dim=1)

        if dummy_mask is None:
            # dummy_mask identifies padded / invalid pairs that are ignored in consistency loss
            dummy_mask = torch.zeros(pA.size(0), dtype=torch.bool, device=pA.device)

        if use_log_space:
            # Consistency in log-space: L(Ïƒ) + L(1-Ïˆ) etc. vs log(pB)
            def clamp01(x):
                return torch.clamp(x, min=eps, max=1.0 - eps)

            L = lambda x: torch.log(torch.clamp(x, min=eps))

            lhs = torch.cat(
                [
                    L(sigma) + L(clamp01(1.0 - psi)),
                    L(a_ca) + L(clamp01(theta_ca)),
                    L(a_an) + L(clamp01(theta_an)),
                    L(i0_ca) + L(clamp01(1.0 - phi_ca)),
                    L(i0_an) + L(clamp01(1.0 - phi_an)),
                ],
                dim=1,
            )
            rhs = torch.log(torch.clamp(pB, min=eps))
            per_sample = ((lhs - rhs) ** 2).mean(dim=1)  # (B,)
        else:
            # Consistency in original space
            pA_next_pred = torch.cat(
                [
                    sigma * (1.0 - psi),
                    a_ca * theta_ca,
                    a_an * theta_an,
                    i0_ca * (1.0 - phi_ca),
                    i0_an * (1.0 - phi_an),
                ],
                dim=1,
            )
            per_sample = ((pA_next_pred - pB) ** 2).mean(dim=1)

        valid = (~dummy_mask).float()
        denom = torch.clamp(valid.sum(), min=1.0)
        loss_consistency = (per_sample * valid).sum() / denom

        # ------------------------------------------------------------------
        # 8) Combine base + consistency + rule + group + tree losses
        # ------------------------------------------------------------------
        total = (
            loss_base
            + lambda_consistency * loss_consistency
            + self.lambda_rule * loss_rule
            + self.lambda_group * loss_group
            + self.lambda_tree * loss_tree
        )

        # ------------------------------------------------------------------
        # 9) Polarity constraint: log(i0_ca) - log(i0_an) â‰¥ m_margin
        #     Enforces that cathode exchange current density > anode.
        # ------------------------------------------------------------------
        m_margin = 2.0
        eps_small = 1e-12

        i0_ca_scalar = pA[:, [3]]  # i0_ca (B,1)
        i0_an_scalar = pA[:, [4]]  # i0_an (B,1)

        delta = torch.log(torch.clamp(i0_ca_scalar, min=eps_small)) - torch.log(
            torch.clamp(i0_an_scalar, min=eps_small)
        )
        loss_i0_polarity = torch.relu(m_margin - delta).mean()
        total = total + lambda_polarity * loss_i0_polarity

        # ------------------------------------------------------------------
        # 10) Monotonic-decay constraints: parameters at B should not increase vs A
        # ------------------------------------------------------------------
        valid_mask = (~dummy_mask)

        def mono_decrease_penalty(xA, xB, mask, slack: float = 0.0, eps: float = 1e-12, use_log: bool = True):
            """
            Penalty to enforce xB <= xA - slack (monotonic decay).
            If use_log=True, the constraint is applied in log-space.
            """
            if not mask.any():
                return torch.tensor(0.0, device=xA.device)
            if use_log:
                lA = torch.log(torch.clamp(xA[mask], min=eps))
                lB = torch.log(torch.clamp(xB[mask], min=eps))
                return torch.relu(lB - lA - slack).mean()
            else:
                return torch.relu(xB[mask] - xA[mask] - slack).mean()

        # Different slack values per parameter type
        slack_sigma = 0.0
        slack_alpha = 0.01
        slack_i0 = 0.0

        loss_sigma_dec = mono_decrease_penalty(pA[:, [0]], pB[:, [0]], valid_mask, slack=slack_sigma, use_log=True)
        loss_alpha_ca_dec = mono_decrease_penalty(pA[:, [1]], pB[:, [1]], valid_mask, slack=slack_alpha, use_log=True)
        loss_alpha_an_dec = mono_decrease_penalty(pA[:, [2]], pB[:, [2]], valid_mask, slack=slack_alpha, use_log=True)
        loss_i0_ca_dec = mono_decrease_penalty(pA[:, [3]], pB[:, [3]], valid_mask, slack=slack_i0, use_log=True)
        loss_i0_an_dec = mono_decrease_penalty(pA[:, [4]], pB[:, [4]], valid_mask, slack=slack_i0, use_log=True)

        loss_wuxing_diff = (
            loss_sigma_dec
            + loss_alpha_ca_dec
            + loss_alpha_an_dec
            + loss_i0_ca_dec
            + loss_i0_an_dec
        )
        total = total + lambda_monodec * loss_wuxing_diff

        # ------------------------------------------------------------------
        # 11) Collect scalar diagnostics for logging
        # ------------------------------------------------------------------
        log_dict = {
            "total": total.item(),
            "voltage": (loss_voltageA + loss_voltageB).item(),
            "classify": (loss_classA + loss_classB).item(),
            "conc": (loss_concA + loss_concB).item(),
            "consistency": loss_consistency.item(),
            "consist_valid": denom.item(),
            "consist_dummy": dummy_mask.float().sum().item(),
            "loss_i0_polarity": loss_i0_polarity.item(),
            "loss_wuxing_diff": loss_wuxing_diff.item(),
            "rule": loss_rule.item(),
            "group": loss_group.item(),
            "tree": loss_tree.item(),
        }

        return total, log_dict

    # =========================================================================
    # Training loop for pair-wise data (A, B)
    # =========================================================================
    def train_pairs(
        self,
        train_loader,
        num_epochs: int,
        lambda_consistency: float = 1.0,
        eps: float = 1e-9,
        use_log_space: bool = True,
        lambda_monodec: float = 0.2,
        lambda_polarity: float = 0.5,
    ) -> None:
        """
        Training loop using pair-wise data (batchA, batchB, dummy_mask).

        This does not affect the original single-sample `train()`; it is an
        additional routine specialized for consistency and monotonic constraints.

        Args
        ----
        train_loader :
            Iterable of (batchA, batchB, dummy_mask).
        num_epochs : int
            Number of epochs for pair-wise training.
        lambda_consistency, eps, use_log_space, lambda_monodec, lambda_polarity :
            Passed directly to `compute_loss_pairs`.
        """
        for epoch in range(num_epochs):
            self.model.train()
            running = {}

            for batchA, batchB, dummy_mask in train_loader:
                # Move data to device (if tensor-like)
                batchA = [b.to(self.device) if hasattr(b, "to") and callable(b.to) else b for b in batchA]
                batchB = [b.to(self.device) if hasattr(b, "to") and callable(b.to) else b for b in batchB]
                dummy_mask = dummy_mask.to(self.device)

                # Compute loss and per-batch diagnostics
                loss, items = self.compute_loss_pairs(
                    batchA,
                    batchB,
                    dummy_mask=dummy_mask,
                    lambda_consistency=lambda_consistency,
                    eps=eps,
                    use_log_space=use_log_space,
                    lambda_monodec=lambda_monodec,
                    lambda_polarity=lambda_polarity,
                )

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                # Accumulate losses for epoch-level averaging
                for k, v in items.items():
                    running[k] = running.get(k, 0.0) + v

            # Epoch-wise average
            avg = {k: v / len(train_loader) for k, v in running.items()}
            self._record_loss(avg)

            # Optional scheduler step
            if self.scheduler is not None:
                self.scheduler.step()

            # Console print
            msg = " | ".join([f"{k}: {v:.4f}" for k, v in avg.items()])
            print(f"[Pairs] Epoch {epoch + 1}/{num_epochs} | {msg}")

            # Periodic checkpoint saving
            if (epoch + 1) % self.save_every == 0:
                self.save_model(f"{epoch + 1}")
                self.save_losses(f"{epoch + 1}")

        # Final save
        self.save_model("final")
        self.save_losses("final")
