import os
import csv
import json
import re
import argparse
import traceback
import multiprocessing
import random
import shutil
from pathlib import Path
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed

import torch
import torch.optim as optim
from torch.utils.data import DataLoader, WeightedRandomSampler
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter
import pandas as pd

# ==== é¡¹ç›®å†…éƒ¨æ¨¡å—ï¼ˆä¿æŒä¸å˜ï¼‰ ====
from model_datasets import Dataset_2_Stable_plus
from model_models_world_model import Model_three_system_1117
from model_test import test_single_xlsx_and_generate_explanations_three_system_1117
from model_train_world_model import Trainer_ThreeSystem_1117
from paired_dataset import SlidingWindowPairDataset, collate_pairs


# --------------------------
# YAML load
# --------------------------
def load_config(cfg_path: Path) -> dict:
    try:
        import yaml  # PyYAML
    except Exception as e:
        raise RuntimeError(
            "ç¼ºå°‘ PyYAML ä¾èµ–ã€‚è¯·å…ˆå®‰è£…ï¼špip install pyyaml"
        ) from e

    with open(cfg_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    if not isinstance(cfg, dict):
        raise ValueError(f"é…ç½®æ–‡ä»¶å†…å®¹éæ³•ï¼ˆåº”ä¸º dictï¼‰ï¼š{cfg_path}")

    # minimal sanity checks
    for k in ["experiment", "data", "train", "test", "paths", "model", "dataset"]:
        if k not in cfg:
            raise KeyError(f"é…ç½®ç¼ºå°‘å­—æ®µ: {k}")

    return cfg


def build_device() -> tuple[torch.device, str]:
    if torch.cuda.is_available():
        return torch.device("cuda"), "cuda"
    return torch.device("cpu"), "cpu"


def resolve_base_dir() -> Path:
    # ä¸åŸè„šæœ¬ä¸€è‡´ï¼šè„šæœ¬æ‰€åœ¨ç›®å½•çš„ parent.parent.parent
    current_file = Path(__file__).resolve()
    return current_file.parent.parent.parent


def build_paths(cfg: dict) -> dict:
    base_dir = resolve_base_dir()
    exp_name = cfg["experiment"]["name"]

    paths_cfg = cfg["paths"]

    data_folder = base_dir / paths_cfg["data_folder"]
    base_test_folder = base_dir / paths_cfg["test_folder"]

    model_save_folder = base_dir / paths_cfg["model_save_root"] / exp_name
    model_save_folder.mkdir(parents=True, exist_ok=True)

    test_model_path = model_save_folder / "trained_model_epoch_final.pth"

    stats_file = base_dir / paths_cfg["stats_file"]

    # label_mapping.jsonï¼šä¼˜å…ˆä½¿ç”¨ paths_cfg["label_mapping_json"]ï¼ˆç›¸å¯¹ main.py åŒç›®å½• or base_dirï¼‰
    # åŸè„šæœ¬æ˜¯ current_file.parent / "label_mapping.json"ï¼Œè¿™é‡Œæ›´å¥å£®ï¼š
    lm = Path(paths_cfg["label_mapping_json"])
    if lm.is_absolute():
        json_path = lm
    else:
        # å…ˆå°è¯• main.py åŒç›®å½•ï¼Œå†å°è¯• base_dir ä¸‹
        p1 = Path(__file__).resolve().parent / lm
        p2 = base_dir / lm
        json_path = p1 if p1.exists() else p2

    debug_log_dir = base_dir / paths_cfg["debug_log_dir"]
    debug_log_dir.mkdir(parents=True, exist_ok=True)

    tb_log_dir = base_dir / paths_cfg["tensorboard_root"] / exp_name
    tb_log_dir.mkdir(parents=True, exist_ok=True)

    return {
        "base_dir": base_dir,
        "exp_name": exp_name,
        "data_folder": data_folder,
        "test_folder_path": base_test_folder,
        "model_save_folder": model_save_folder,
        "test_model_path": test_model_path,
        "stats_file": stats_file,
        "json_path": json_path,
        "debug_log_dir": debug_log_dir,
        "tb_log_dir": tb_log_dir,
    }


def load_label_mapping(json_path: Path) -> dict:
    if not json_path.exists():
        print(f"âš ï¸ æœªæ‰¾åˆ° label_mapping.json: {json_path}ï¼Œåç»­å¦‚æœä¸éœ€è¦å¯ä»¥å¿½ç•¥ã€‚")
        return {}
    with open(json_path, "r", encoding="utf-8") as f:
        return json.load(f)


def infer_label_from_filename(fname: str, label_mapping: dict) -> str | None:
    if label_mapping:
        for label in label_mapping.keys():
            if isinstance(label, str) and label in fname:
                return label

    keywords = ["é’™ç¦»å­", "é’ ç¦»å­", "é•ç¦»å­", "é“¬ç¦»å­", "é“œç¦»å­", "é“ç¦»å­", "æ— æ±¡æŸ“"]
    for kw in keywords:
        if (kw in fname) and ("ion_column" not in fname):
            return kw

    lower = fname.lower()
    if ("blank" in lower) or ("çº¯æ°´" in lower) or ("ion_column" in fname):
        return "æ— æ±¡æŸ“"

    return None


def is_valid_xlsx_for_model(xlsx_path: Path, num_time_points: int, num_freq_points: int) -> bool:
    try:
        df = pd.read_excel(xlsx_path)
    except Exception as e:
        print(f"[SKIP] æ— æ³•è¯»å–æ–‡ä»¶ {xlsx_path.name}: {e}")
        return False

    required_cols = ["Time(h)", "mean_voltage", "Zreal", "Zimag", "Freq"]
    for col in required_cols:
        if col not in df.columns:
            print(f"[SKIP] æ–‡ä»¶ {xlsx_path.name} ç¼ºå°‘å¿…è¦åˆ—: {col}")
            return False

    time_series = df["Time(h)"].dropna().unique().tolist()
    if len(time_series) < num_time_points:
        print(f"[SKIP] æ–‡ä»¶ {xlsx_path.name}: ä»…æœ‰ {len(time_series)} ä¸ªæ—¶é—´ç‚¹ < è¦æ±‚çš„ {num_time_points}")
        return False

    time_points = sorted(time_series)[:num_time_points]
    for t in time_points:
        time_data = df[df["Time(h)"] == t]
        if time_data.empty:
            print(f"[SKIP] æ–‡ä»¶ {xlsx_path.name}: ç¼ºå°‘æ—¶é—´ç‚¹ {t}h")
            return False

        time_data = time_data.sort_values(by="Freq")
        voltage = time_data["mean_voltage"].values[0]
        if pd.isna(voltage):
            print(f"[SKIP] æ–‡ä»¶ {xlsx_path.name}: æ—¶é—´ç‚¹ {t}h çš„ mean_voltage ä¸º NaN")
            return False

        impedance_np = time_data[["Zreal", "Zimag"]].values
        if impedance_np.shape[0] < num_freq_points:
            print(f"[SKIP] æ–‡ä»¶ {xlsx_path.name}: æ—¶é—´ç‚¹ {t}h çš„é˜»æŠ—ç‚¹æ•° {impedance_np.shape[0]} < è¦æ±‚çš„ {num_freq_points}")
            return False

    print(f"[OK] å¯ä½œä¸ºæµ‹è¯•æ ·æœ¬: {xlsx_path.name}")
    return True


def prepare_test_folder(paths: dict, label_mapping: dict, num_time_points: int, num_freq_points: int, seed: int):
    data_folder: Path = paths["data_folder"]
    base_test_folder: Path = paths["test_folder_path"]
    exp_name: str = paths["exp_name"]

    # å›ºå®šæµ‹è¯•ç›®å½•æ¨¡å¼ï¼štest_folder çš„ç›®å½•åä¸æ˜¯ datasets_for_all_test
    if base_test_folder.name != "datasets_for_all_test":
        base_test_folder.mkdir(parents=True, exist_ok=True)
        existing_xlsx = list(base_test_folder.glob("*.xlsx"))
        if existing_xlsx:
            print(f"ğŸ“‚ ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æµ‹è¯•ç›®å½•ï¼š{base_test_folder}ï¼ˆå‘ç° {len(existing_xlsx)} ä¸ª .xlsx æ–‡ä»¶ï¼‰")
        else:
            print(f"âš ï¸ æŒ‡å®šçš„æµ‹è¯•ç›®å½• {base_test_folder} ä¸­æ²¡æœ‰ä»»ä½• .xlsx æ–‡ä»¶ï¼Œåç»­æµ‹è¯•/éªŒè¯å°†æ²¡æœ‰æ ·æœ¬å¯ç”¨ã€‚")
        paths["test_folder_path"] = base_test_folder
        return

    # è‡ªåŠ¨åˆ’åˆ†æ¨¡å¼ï¼šdatasets_for_all_test/<exp_name>
    current_test_folder = base_test_folder / exp_name
    current_test_folder.mkdir(parents=True, exist_ok=True)

    existing_xlsx = list(current_test_folder.glob("*.xlsx"))
    if existing_xlsx:
        print(f"ğŸ“‚ æ£€æµ‹åˆ°å·²æœ‰æµ‹è¯•æ ·æœ¬ï¼ˆå…± {len(existing_xlsx)} ä¸ªï¼‰ï¼Œç›´æ¥ä½¿ç”¨ï¼š{current_test_folder}")
        paths["test_folder_path"] = current_test_folder
        return

    all_xlsx = sorted(data_folder.glob("*.xlsx"))
    if not all_xlsx:
        print(f"âš ï¸ åœ¨ {data_folder} ä¸‹æœªæ‰¾åˆ°ä»»ä½• .xlsx æ–‡ä»¶ï¼Œæ— æ³•åˆ’åˆ†æµ‹è¯•é›†ã€‚")
        paths["test_folder_path"] = current_test_folder
        return

    label_to_files: dict[str, list[Path]] = defaultdict(list)
    for f in all_xlsx:
        label = infer_label_from_filename(f.name, label_mapping)
        if label is None:
            continue
        if not is_valid_xlsx_for_model(f, num_time_points, num_freq_points):
            continue
        label_to_files[label].append(f)

    if not label_to_files:
        print("âš ï¸ æ²¡æœ‰ä»»ä½•å¯ç”¨äºåˆ’åˆ†æµ‹è¯•é›†çš„åˆæ ¼æ ·æœ¬ã€‚")
        paths["test_folder_path"] = current_test_folder
        return

    random.seed(int(seed))
    total_moved = 0
    for label, files in label_to_files.items():
        files_sorted = sorted(files, key=lambda p: p.name)
        selected = files_sorted[:3]  # æ¯ç±»æœ€å¤š 3 ä¸ª
        print(f"ğŸ§ª ç±»åˆ« [{label}] é€‰ä¸­ {len(selected)} ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†ã€‚")
        for f in selected:
            dest = current_test_folder / f.name
            print(f"  - å¤åˆ¶ {f} -> {dest}")
            shutil.copy2(str(f), str(dest))
            total_moved += 1

    print(f"âœ… æµ‹è¯•é›†åˆ’åˆ†å®Œæˆï¼Œå…±å¤åˆ¶ {total_moved} ä¸ªæ ·æœ¬åˆ° {current_test_folder}")
    paths["test_folder_path"] = current_test_folder



def build_model(device: torch.device, cfg: dict):
    d = cfg["data"]
    m = cfg["model"]

    model = Model_three_system_1117(
        volt_input_dim=m["volt_input_dim"],
        volt_mlp_hidden_dims=m["volt_mlp_hidden_dims"],
        mlp_output_dims=m["mlp_output_dims"],
        volt_mlp_num_layers=m["volt_mlp_num_layers"],

        impe_input_dim=m["impe_input_dim"],
        impe_mlp_hidden_dims=m["impe_mlp_hidden_dims"],
        impe_mlp_num_layers=m["impe_mlp_num_layers"],

        transformer_d_model=m["transformer_d_model"],
        nhead=m["nhead"],
        transformer_num_layers=m["transformer_num_layers"],
        param_transformer_num_layers=m["param_transformer_num_layers"],

        physic_mlp_hidden_dims=m["physic_mlp_hidden_dims"],
        physic_mlp_num_layers=m["physic_mlp_num_layers"],

        ion_attr_embed_hidden_dims=m["ion_attr_embed_hidden_dims"],
        ion_attr_embed_num_layers=m["ion_attr_embed_num_layers"],
        ion_encoder_num_layers=m["ion_encoder_num_layers"],
        ion_post_hidden_dims=m["ion_post_hidden_dims"],
        ion_post_num_layers=m["ion_post_num_layers"],

        probMLP_input_dims=m["probMLP_input_dims"],
        probMLP_hidden_dims=m["probMLP_hidden_dims"],
        probMLP_num_layers=m["probMLP_num_layers"],

        param_mlp_hidden_dims=m["param_mlp_hidden_dims"],
        param_mlp_num_layers=m["param_mlp_num_layers"],

        freq_encoder_hidden_dims=m["freq_encoder_hidden_dims"],
        freq_encoder_num_layers=m["freq_encoder_num_layers"],
        time_encoder_hidden_dims=m["time_encoder_hidden_dims"],
        time_encoder_num_layers=m["time_encoder_num_layers"],

        cross_transformer_num_layers=m["cross_transformer_num_layers"],

        param_embed_hidden_dims=m["param_embed_hidden_dims"],
        param_embed_num_layers=m["param_embed_num_layers"],
        physic_embed_hidden_dims=m["physic_embed_hidden_dims"],
        physic_embed_num_layers=m["physic_embed_num_layers"],

        envMLP_input_dim=m["envMLP_input_dim"],
        env_mlp_hidden_dims=m["env_mlp_hidden_dims"],
        env_mlp_num_layers=m["env_mlp_num_layers"],
        ep_input_dim=m["ep_input_dim"],
        ep_mlp_hidden_dims=m["ep_mlp_hidden_dims"],
        ep_mlp_num_layers=m["ep_mlp_num_layers"],

        Z_encoder_num_layers=m["Z_encoder_num_layers"],

        num_freq_points=d["num_freq_points"],
        num_time_points=d["num_time_points"],
    ).to(device)

    return model


def build_dataloaders(paths: dict, cfg: dict):
    d = cfg["data"]
    t = cfg["train"]
    ds = cfg["dataset"]

    data_folder = paths["data_folder"]
    val_data_folder = paths["test_folder_path"]
    # âœ… æ”¶é›†æµ‹è¯•é›†æ–‡ä»¶åï¼ˆåªå– nameï¼Œç”¨äºä¸è®­ç»ƒç›®å½•çš„ os.listdir åŒ¹é…ï¼‰
    val_fnames = {p.name for p in Path(val_data_folder).glob("*.xlsx")}

    stats_file = paths["stats_file"]

    base_train = Dataset_2_Stable_plus(
        data_folder=data_folder,
        stats_file=str(stats_file),
        save_stats=True,
        num_time_points=d["num_time_points"],
        exclude_fnames=val_fnames,   # âœ… æ–°å¢

    )
    inter = set(base_train.file_names) & val_fnames
    print(f"[LEAK-CHECK] overlap(train, val) = {len(inter)}")
    if len(inter) > 0:
        print("[LEAK-CHECK] examples:", list(sorted(inter))[:10])

    print("[DEBUG] data_folder =", str(data_folder.resolve()))
    print("[DEBUG] base_train.file_names[:10] =")
    for x in base_train.file_names[:10]:
        print("  ", x)
    print("[DEBUG] exist check (first 10):")
    for x in base_train.file_names[:10]:
        p = (Path(data_folder) / x) if not str(x).startswith("/") else Path(x)
        print("  ", p, "exists=", p.exists())


    base_val = Dataset_2_Stable_plus(
        data_folder=val_data_folder,
        stats_file=str(stats_file),
        save_stats=False,
        num_time_points=d["num_time_points"],
    )

    pair_train = SlidingWindowPairDataset(
        base_train,
        keep_unpaired=ds["keep_unpaired"],
        debug=ds["debug"],
        focus_prefix_contains=ds.get("focus_prefix_contains", None),
        max_print=int(ds.get("max_print", 0)),
        num_time_points=d["num_time_points"],
    )

    pair_val = SlidingWindowPairDataset(
        base_val,
        keep_unpaired=ds["keep_unpaired"],
        debug=ds["debug"],
        focus_prefix_contains=None,
        max_print=int(ds.get("max_print", 0)),
        num_time_points=d["num_time_points"],
    )

    print("num base train samples:", len(base_train))
    print("num base val   samples:", len(base_val))
    print("num pair train samples:", len(pair_train))
    print("num pair val   samples:", len(pair_val))

    if len(pair_train) > 0:
        (A, B, dummy_mask) = collate_pairs([pair_train[0]])
        print("dummy_mask[0] =", bool(dummy_mask[0]))

    # WeightedRandomSampler (stage-based)
    base_weight = float(t["sampler"]["base_weight"])
    rapid_weight = float(t["sampler"]["rapid_weight"])

    sample_weights = []
    for i in range(len(pair_train)):
        sampleA, sampleB, *_ = pair_train[i]
        stageB = int(sampleB[-1].item())
        sample_weights.append(rapid_weight if stageB == 1 else base_weight)

    sample_weights_tensor = torch.tensor(sample_weights, dtype=torch.double)
    sampler = WeightedRandomSampler(
        weights=sample_weights_tensor,
        num_samples=len(sample_weights_tensor),
        replacement=True,
    )

    train_loader = DataLoader(
        pair_train,
        batch_size=int(t["batch_size"]),
        sampler=sampler,
        collate_fn=collate_pairs,
    )

    val_loader = DataLoader(
        pair_val,
        batch_size=int(t["batch_size"]),
        shuffle=False,
        collate_fn=collate_pairs,
    )

    return train_loader, val_loader


def process_single_file(
    xlsx_path_str: str,
    model_path: str,
    device_str: str,
    cfg: dict,
    exp_name: str,
    log_dir: Path,
):
    d = cfg["data"]
    xlsx_path = Path(xlsx_path_str)
    log_path = log_dir / f"{xlsx_path.stem}.log"

    with open(log_path, "w", encoding="utf-8") as logf:
        try:
            device = torch.device(device_str)
            model = build_model(device, cfg)
            state = torch.load(model_path, map_location=device, weights_only=True)
            model.load_state_dict(state, strict=False)
            model.eval()

            correct, predict, truth = test_single_xlsx_and_generate_explanations_three_system_1117(
                xlsx_path=xlsx_path_str,
                model=model,
                device=device,
                num_time_points=int(d["num_time_points"]),
                num_freq_points=int(d["num_freq_points"]),
                folder_name=exp_name,
            )
            return correct, predict, truth
        except Exception:
            print("âŒ æ–‡ä»¶å¤„ç†å‡ºé”™:", file=logf)
            traceback.print_exc(file=logf)
            print("=== å¤„ç†å¤±è´¥ ===", file=logf)
            return None, None, None


def run_train(device: torch.device, paths: dict, cfg: dict):
    print("ğŸš€ è¿›å…¥è®­ç»ƒæ¨¡å¼ (--train)")
    train_loader, val_loader = build_dataloaders(paths, cfg)

    model = build_model(device, cfg)

    tcfg = cfg["train"]
    optimizer = optim.Adam(
        model.parameters(),
        lr=float(tcfg["learning_rate"]),
        weight_decay=float(tcfg.get("weight_decay", 0.0)),
    )

    tb_log_dir = paths["tb_log_dir"]
    print(f"ğŸ“ TensorBoard æ—¥å¿—ç›®å½•: {tb_log_dir}")
    writer = SummaryWriter(log_dir=str(tb_log_dir))

    tr = tcfg["trainer"]
    trainer = Trainer_ThreeSystem_1117(
        model=model,
        optimizer=optimizer,
        device=device,
        model_save_folder=paths["model_save_folder"],
        alpha=float(tr["alpha"]),
        beta=float(tr["beta"]),
        gamma=float(tr["gamma"]),
        lambda_rule=float(tr["lambda_rule"]),
        lambda_group=float(tr["lambda_group"]),
        lambda_tree=float(tr["lambda_tree"]),
        lambda_band=float(tr["lambda_band"]),
        save_every=int(tr["save_every"]),
        label_smoothing=float(tr["label_smoothing"]),
    )

    tp = tcfg["train_pairs"]
    trainer.train_pairs(
        train_loader,
        num_epochs=int(tcfg["num_epochs"]),
        eps=float(tp["eps"]),
        use_log_space=bool(tp["use_log_space"]),
        lambda_monodec=float(tp["lambda_monodec"]),
        lambda_polarity=float(tp["lambda_polarity"]),
        weight_ratio=float(tp["weight_ratio"]),
    )

    writer.close()


def _parse_epoch_num(path: Path) -> int:
    m = re.search(r"trained_model_epoch_(\d+)\.pth$", path.name)
    return int(m.group(1)) if m else -1


def _get_max_workers(cfg: dict) -> int:
    cpu_count = multiprocessing.cpu_count()
    factor = float(cfg["test"].get("max_workers_factor", 0.5))
    return max(1, int(cpu_count * factor))


def run_test(paths: dict, cfg: dict):
    print("ğŸ” è¿›å…¥æµ‹è¯•æ¨¡å¼ (--test)")

    test_model_path = paths["test_model_path"]
    test_folder_path = paths["test_folder_path"]
    exp_name = paths["exp_name"]
    log_dir = paths["debug_log_dir"]

    device_str = cfg["test"].get("device_str", "cpu")
    max_workers = _get_max_workers(cfg)

    print(f"ğŸ” æ­£åœ¨åŠ è½½æ¨¡å‹: {test_model_path}")
    xlsx_paths = [str(test_folder_path / f) for f in os.listdir(test_folder_path) if f.endswith(".xlsx")]
    print(f"ğŸ“Š å…±æ£€æµ‹åˆ° {len(xlsx_paths)} ä¸ª .xlsx æ–‡ä»¶ï¼Œå‡†å¤‡å¹¶è¡Œå¤„ç†...")

    correct_count = 0
    total = len(xlsx_paths)
    confusion_counter = defaultdict(lambda: defaultdict(int))

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = [
            executor.submit(
                process_single_file,
                path,
                str(test_model_path),
                device_str,
                cfg,
                exp_name,
                log_dir,
            )
            for path in xlsx_paths
        ]

        for future in as_completed(futures):
            try:
                correct, predict, truth = future.result()
                if correct is None:
                    continue
                if correct:
                    correct_count += 1
                confusion_counter[truth][predict] += 1
            except Exception as e:
                print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

    if total > 0:
        acc = correct_count / total
        print(f"\nâœ… æ€»å…±æµ‹è¯•æ ·æœ¬æ•°: {total}")
        print(f"ğŸ¯ é¢„æµ‹æ­£ç¡®æ ·æœ¬æ•°: {correct_count}")
        print(f"ğŸ“Š å‡†ç¡®ç‡: {acc:.2%}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½• .xlsx æµ‹è¯•æ–‡ä»¶")

    print("\nğŸ“‰ é”™è¯¯åˆ†æï¼ˆçœŸå®æ ‡ç­¾ â†’ é¢„æµ‹æ ‡ç­¾ â†’ ä¸ªæ•°ï¼‰:")
    for truth_label, pred_dict in confusion_counter.items():
        for predicted_label, count in pred_dict.items():
            print(f"  çœŸå®: {truth_label} â†’ é¢„æµ‹: {predicted_label} : {count} ä¸ª")


def run_search(paths: dict, cfg: dict):
    print("ğŸ” è¿›å…¥æœç´¢æ¨¡å¼ (--search)")

    model_save_folder = paths["model_save_folder"]
    test_folder_path = paths["test_folder_path"]
    exp_name = paths["exp_name"]
    log_dir = paths["debug_log_dir"]

    device_str = cfg["test"].get("device_str", "cpu")
    max_workers = _get_max_workers(cfg)

    ckpts = [p for p in model_save_folder.glob("trained_model_epoch_*.pth") if p.name != "trained_model_epoch_final.pth"]
    ckpts = sorted(ckpts, key=_parse_epoch_num)

    if not ckpts:
        print(f"âš ï¸ æœªåœ¨ {model_save_folder} æ‰¾åˆ°ä»»ä½• trained_model_epoch_*.pth")
        return

    xlsx_paths = [str(test_folder_path / f) for f in os.listdir(test_folder_path) if f.endswith(".xlsx")]
    if not xlsx_paths:
        print(f"âš ï¸ æœªåœ¨ {test_folder_path} æ‰¾åˆ°ä»»ä½• .xlsx æµ‹è¯•æ–‡ä»¶")
        return

    print(f"ğŸ” å…±å‘ç° {len(ckpts)} ä¸ª checkpointï¼Œå°†é€ä¸ªè¯„æµ‹ï¼›æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{len(xlsx_paths)}")
    results = []

    for i, ckpt_path in enumerate(ckpts, 1):
        epoch_num = _parse_epoch_num(ckpt_path)
        if epoch_num < 0:
            print(f"è·³è¿‡æ— æ³•è¯†åˆ« epoch çš„æ–‡ä»¶ï¼š{ckpt_path.name}")
            continue

        print(f"\n[{i}/{len(ckpts)}] ğŸ” è¯„æµ‹ checkpoint: {ckpt_path.name}  (epoch={epoch_num})")
        correct_count, total = 0, len(xlsx_paths)
        confusion_counter = defaultdict(lambda: defaultdict(int))

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(
                    process_single_file,
                    path,
                    str(ckpt_path),
                    device_str,
                    cfg,
                    exp_name,
                    log_dir,
                )
                for path in xlsx_paths
            ]

            for future in as_completed(futures):
                try:
                    correct, predict, truth = future.result()
                    if correct is None:
                        continue
                    if correct:
                        correct_count += 1
                    confusion_counter[truth][predict] += 1
                except Exception as e:
                    print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")

        acc = (correct_count / total) if total > 0 else 0.0
        print(f"ğŸ¯ epoch={epoch_num} | æ­£ç¡® {correct_count}/{total} | å‡†ç¡®ç‡={acc:.2%}")
        results.append((epoch_num, acc, correct_count, total, ckpt_path.name))

    results_by_epoch = sorted(results, key=lambda x: x[0])
    csv_path_epoch = model_save_folder / "search_checkpoints_epoch_sorted.csv"
    with open(csv_path_epoch, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["rank_by_epoch", "epoch", "accuracy", "correct", "total", "ckpt_file"])
        for rank, (ep, acc, cor, tot, name) in enumerate(results_by_epoch, 1):
            w.writerow([rank, ep, f"{acc:.6f}", cor, tot, name])
    print(f"\nâœ… å·²ä¿å­˜ CSVï¼ˆæŒ‰ epoch å‡åºï¼‰ï¼š{csv_path_epoch}")

    results_by_acc = sorted(results, key=lambda x: x[1])
    csv_path_acc = model_save_folder / "search_checkpoints_accuracy_sorted.csv"
    with open(csv_path_acc, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["rank_by_acc", "epoch", "accuracy", "correct", "total", "ckpt_file"])
        for rank, (ep, acc, cor, tot, name) in enumerate(results_by_acc, 1):
            w.writerow([rank, ep, f"{acc:.6f}", cor, tot, name])
    print(f"âœ… å·²ä¿å­˜ CSVï¼ˆæŒ‰å‡†ç¡®ç‡å‡åºï¼‰ï¼š{csv_path_acc}")

    try:
        eps = [r[0] for r in results_by_epoch]
        acc_e = [r[1] for r in results_by_epoch]
        plt.figure()
        plt.plot(eps, acc_e, marker="o")
        plt.xlabel("Epoch (ascending)")
        plt.ylabel("Accuracy")
        plt.title("Checkpoint Accuracy vs Epoch (epoch-ascending)")
        plt.grid(True)
        plt.tight_layout()
        png_path_by_epoch = model_save_folder / "search_checkpoints_accuracy_by_epoch.png"
        plt.savefig(png_path_by_epoch, dpi=150)
        plt.close()
        print(f"ğŸ–¼ï¸ å·²ä¿å­˜æ›²çº¿å›¾ï¼ˆæŒ‰ epoch å‡åºï¼‰ï¼š{png_path_by_epoch}")

        ranks = list(range(1, len(results_by_acc) + 1))
        accs = [r[1] for r in results_by_acc]
        epochs_sorted_for_acc = [r[0] for r in results_by_acc]
        plt.figure()
        plt.plot(ranks, accs, marker="o")
        plt.xlabel("Rank by Accuracy (ascending)")
        plt.ylabel("Accuracy")
        plt.title("Checkpoint Accuracy (sorted by accuracy ascending)")
        if len(ranks) <= 20:
            plt.xticks(ranks, [f"ep{e}" for e in epochs_sorted_for_acc], rotation=45, ha="right")
        plt.grid(True)
        plt.tight_layout()
        png_path_sorted = model_save_folder / "search_checkpoints_accuracy_sorted.png"
        plt.savefig(png_path_sorted, dpi=150)
        plt.close()
        print(f"ğŸ–¼ï¸ å·²ä¿å­˜æ›²çº¿å›¾ï¼ˆæŒ‰å‡†ç¡®ç‡å‡åºï¼‰ï¼š{png_path_sorted}")
    except Exception as e:
        print(f"âš ï¸ ç”»å›¾å¤±è´¥ï¼š{e}")


def parse_args():
    parser = argparse.ArgumentParser(description="è¿è¡Œ ion_detect æ¨¡å‹è®­ç»ƒ / æµ‹è¯• / æœç´¢ï¼ˆYAML é…ç½®ç‰ˆï¼‰")
    parser.add_argument("--config", type=str, required=True, help="YAML é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œä¾‹å¦‚ 20251208a.yaml")
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--train", action="store_true", help="è¿è¡Œè®­ç»ƒæ¨¡å¼")
    group.add_argument("--test", action="store_true", help="è¿è¡Œæµ‹è¯•æ¨¡å¼")
    group.add_argument("--search", action="store_true", help="éå†ç›®å½•ä¸­çš„æ‰€æœ‰ epoch æƒé‡å¹¶è¯„æµ‹")
    return parser.parse_args()


def main():
    args = parse_args()
    cfg_path = Path(args.config).resolve()
    cfg = load_config(cfg_path)

    # seed
    seed = int(cfg["experiment"].get("seed", 42))
    random.seed(seed)
    torch.manual_seed(seed)

    device, device_str = build_device()
    paths = build_paths(cfg)

    d = cfg["data"]
    num_time_points = int(d["num_time_points"])
    num_freq_points = int(d["num_freq_points"])

    label_mapping = load_label_mapping(paths["json_path"])
    # è‡ªåŠ¨åˆ’åˆ†æµ‹è¯•æ ·æœ¬/æˆ–ä½¿ç”¨å›ºå®šæµ‹è¯•ç›®å½•
    prepare_test_folder(paths, label_mapping, num_time_points, num_freq_points, seed)

    print(f"âœ… å½“å‰ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ“‚ è®­ç»ƒé›†ç›®å½•: {paths['data_folder']}")
    print(f"ğŸ“‚ æµ‹è¯•/éªŒè¯é›†ç›®å½•: {paths['test_folder_path']}")
    print(f"ğŸ§ª experiment: {paths['exp_name']}")

    if args.train:
        run_train(device, paths, cfg)
    elif args.test:
        run_test(paths, cfg)
    elif args.search:
        run_search(paths, cfg)
    else:
        raise ValueError("å¿…é¡»æŒ‡å®š --train / --test / --search ä¹‹ä¸€")


if __name__ == "__main__":
    main()
